{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(str(3)+(str(5)))\n",
    "print(int('3')+int('5'))\n",
    "#print(int('3')+int('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "#loop\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    print()\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes!!!\n"
     ]
    }
   ],
   "source": [
    "#condition\n",
    "if 3>5:\n",
    "    print('wow')\n",
    "else:\n",
    "    print('yes!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.02857142857142\n"
     ]
    }
   ],
   "source": [
    "#to calculate the mean of the grades\n",
    "sum=0 #initialization\n",
    "num=0\n",
    "file = open('C:/Users/omerc/Documents/python files/random_grades.txt','r') # open for read\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line) #grade in a number, line is a string\n",
    "    sum+=grade \n",
    "    num+=1\n",
    "file.close()\n",
    "mean=sum/num\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the maximum grade is 98\n"
     ]
    }
   ],
   "source": [
    "max_grade=0\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    if max_grade<grade:\n",
    "        max_grade=grade\n",
    "file.close()\n",
    "print('the maximum grade is',max_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of lucky students 2\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    if grade == 98: #\n",
    "        count+=1\n",
    "file.close()\n",
    "print('the number of lucky students',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.187898571880481\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "num=0\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    sum+=(grade-mean)**2 # we take \"mean\" from above\n",
    "    num+=1\n",
    "file.close() \n",
    "std=(sum/(num-1))**0.5\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean= 84.02857142857142 \n",
      "std= 10.04130246044012 \n",
      "max= 98\n",
      "lenght of list= 35\n",
      "[65, 67, 68, 68, 69, 70, 75, 76, 76, 77, 78, 80, 80, 81, 82, 82, 84, 84, 86, 87, 88, 89, 89, 90, 92, 92, 94, 95, 95, 96, 96, 97, 97, 98, 98]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "grades=[] #initialization of empty list\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    grades.append(grade) # add value to list\n",
    "file.close()\n",
    "print('mean=',numpy.mean(grades),'\\n''std=',numpy.std(grades),'\\n''max=', numpy.max(grades))\n",
    "print('lenght of list=',len(grades)) # lenght of list\n",
    "print(sorted(grades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2941\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    sum+=(grade) # we take \"mean\" from above\n",
    "\n",
    "file.close() \n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the maximum grade is 98 the minimum grade is 65\n"
     ]
    }
   ],
   "source": [
    "min_grade=100\n",
    "max_grade=0\n",
    "file=open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    grade=int(line)\n",
    "    if max_grade<grade:\n",
    "        max_grade=grade\n",
    "    if  min_grade>grade:\n",
    "        min_grade=grade\n",
    "file.close()\n",
    "print('the maximum grade is',max_grade,'the minimum grade is',min_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pass students is- 35 the ave. of pass grades is- 84.02857142857142\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "num=0\n",
    "file = open('C:/Users/omerc/Documents/python files/random_grades.txt','r') # open for read\n",
    "for line in file:\n",
    "    line=line.strip() #delite invisible characters fron line\n",
    "    grade=int(line)\n",
    "    if 55<grade:\n",
    "        num+=1\n",
    "        sum+=grade\n",
    "pass_ave=sum/num\n",
    "print('number of pass students is-',num, 'the ave. of pass grades is-',pass_ave)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "# to identify the most common grade\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "grades = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "file = open('C:/Users/omerc/Documents/python files/random_grades.txt','r')\n",
    "\n",
    "for line in file:\n",
    "\n",
    "    line = line.strip()\n",
    "\n",
    "    grade = int(line)\n",
    "\n",
    "    grades.append(grade)\n",
    "\n",
    "    counts = np.bincount(grades)\n",
    "\n",
    "file.close()\n",
    "\n",
    "print (np.argmax(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 380\n",
      "of 199\n",
      "a 144\n",
      "to 128\n",
      "and 122\n",
      "in 108\n",
      "is 96\n",
      "we 77\n",
      "that 73\n",
      "with 61\n",
      "capsule 58\n",
      "for 56\n",
      "on 50\n",
      "each 41\n",
      "layer 41\n",
      "capsules 36\n",
      "routing 34\n",
      "by 34\n",
      "are 34\n",
      "digit 34\n",
      "this 32\n",
      "it 31\n",
      "as 27\n",
      "all 27\n",
      "from 25\n",
      "an 24\n",
      "output 24\n",
      "at 23\n",
      "two 23\n",
      "vector 22\n",
      "convolutional 22\n",
      "et 22\n",
      "al. 22\n",
      "use 21\n",
      "model 21\n",
      "test 21\n",
      "capsnet 20\n",
      "one 19\n",
      "mnist 19\n",
      "neural 18\n",
      "3 18\n",
      "which 17\n",
      "1 16\n",
      "has 16\n",
      "training 16\n",
      "but 15\n",
      "image 14\n",
      "digits 14\n",
      "instantiation 13\n",
      "represent 13\n",
      "have 13\n",
      "can 13\n",
      "not 13\n",
      "parameters 12\n",
      "than 12\n",
      "only 12\n",
      "more 12\n",
      "be 12\n",
      "other 12\n",
      "activity 11\n",
      "entity 11\n",
      "network 11\n",
      "there 11\n",
      "loss 11\n",
      "5 11\n",
      "between 10\n",
      "length 10\n",
      "its 10\n",
      "these 10\n",
      "our 10\n",
      "networks 10\n",
      "learning 10\n",
      "layers 10\n",
      "\u0002 10\n",
      "pixel 10\n",
      "used 10\n",
      "level 9\n",
      "will 9\n",
      "image. 9\n",
      "different 9\n",
      "information 9\n",
      "= 9\n",
      "i 9\n",
      "fig. 9\n",
      "reconstruction 9\n",
      "8) 9\n",
      "achieved 9\n",
      "object 8\n",
      "active 8\n",
      "make 8\n",
      "show 8\n",
      "feature 8\n",
      "2 8\n",
      "j 8\n",
      "fully 8\n",
      "connected 8\n",
      "9 8\n",
      "set 8\n",
      "figure 8\n",
      "classification 8\n",
      "during 8\n",
      "multimnist 8\n",
      "size 8\n",
      "dimensions 8\n",
      "trained 8\n",
      "pages 8\n",
      "dynamic 7\n",
      "using 7\n",
      "how 7\n",
      "parse 7\n",
      "1. 7\n",
      "should 7\n",
      "vj 7\n",
      "+ 7\n",
      "because 7\n",
      "same 7\n",
      "then 7\n",
      "class 7\n",
      "stride 7\n",
      "primary 7\n",
      "dataset 7\n",
      "shows 7\n",
      "average 7\n",
      "error 7\n",
      "affine 7\n",
      "arxiv 7\n",
      "preprint 7\n",
      "change 7\n",
      "type 6\n",
      "capsules. 6\n",
      "when 6\n",
      "multiple 6\n",
      "highly 6\n",
      "overlapping 6\n",
      "vectors 6\n",
      "prediction 6\n",
      "capsule. 6\n",
      "single 6\n",
      "small 6\n",
      "possible 6\n",
      "coupling 6\n",
      "sum 6\n",
      "if 6\n",
      "so 6\n",
      "get 6\n",
      "input 6\n",
      "bij 6\n",
      "they 6\n",
      "convolution 6\n",
      "8 6\n",
      "deep 6\n",
      "final 6\n",
      "no 6\n",
      "reconstructions 6\n",
      "while 6\n",
      "columns 6\n",
      "7 6\n",
      "also 6\n",
      "accuracy 6\n",
      "number 6\n",
      "7) 6\n",
      "recognition 6\n",
      "iteration 6\n",
      "geoffrey 5\n",
      "whose 5\n",
      "represents 5\n",
      "or 5\n",
      "probability 5\n",
      "transformation 5\n",
      "higher 5\n",
      "results 5\n",
      "much 5\n",
      "like 5\n",
      "parent 5\n",
      "very 5\n",
      "conference 5\n",
      "processing 5\n",
      "fact 5\n",
      "coefficients 5\n",
      "allows 5\n",
      "most 5\n",
      "about 5\n",
      "good 5\n",
      "do 5\n",
      "sj 5\n",
      "log 5\n",
      "iterations 5\n",
      "􀀀 5\n",
      "kernels 5\n",
      "primarycapsules 5\n",
      "digitcaps 5\n",
      "decoder 5\n",
      "reconstruct 5\n",
      "training. 5\n",
      "images 5\n",
      "data 5\n",
      "9) 5\n",
      "networks. 5\n",
      "after 5\n",
      "transformations 5\n",
      "representation 5\n",
      "models 5\n",
      "0) 5\n",
      "images. 5\n",
      "distributed 5\n",
      "neurons 4\n",
      "orientation 4\n",
      "higher-level 4\n",
      "achieves 4\n",
      "iterative 4\n",
      "scalar 4\n",
      "product 4\n",
      "ensure 4\n",
      "knowledge 4\n",
      "gives 4\n",
      "visual 4\n",
      "into 4\n",
      "many 4\n",
      "(hinton 4\n",
      "activities 4\n",
      "existence 4\n",
      "down 4\n",
      "makes 4\n",
      "weight 4\n",
      "segmenting 4\n",
      "learned 4\n",
      "detectors 4\n",
      "them 4\n",
      "position 4\n",
      "location 4\n",
      "outputs 4\n",
      "below 4\n",
      "total 4\n",
      "first 4\n",
      "initial 4\n",
      "logits 4\n",
      "does 4\n",
      "were 4\n",
      "grid 4\n",
      "(l 4\n",
      "1): 4\n",
      "eq. 4\n",
      "margin 4\n",
      "digits, 4\n",
      "architecture 4\n",
      "simple 4\n",
      "layer. 4\n",
      "conv1 4\n",
      "what 4\n",
      "units 4\n",
      "overlap 4\n",
      "6 4\n",
      "label 4\n",
      "since 4\n",
      "exponentially 4\n",
      "digit. 4\n",
      "correct 4\n",
      "scale 4\n",
      "example 4\n",
      "(5; 4\n",
      "5) 4\n",
      "standard 4\n",
      "baseline 4\n",
      "- 4\n",
      "network. 4\n",
      "localized 4\n",
      "part 4\n",
      "pixels. 4\n",
      "tested 4\n",
      "any 4\n",
      "set. 4\n",
      "similar 4\n",
      "objects 4\n",
      "segmentation 4\n",
      "right 4\n",
      "able 4\n",
      "4) 4\n",
      "cifar10 4\n",
      "spatial 4\n",
      "advances 4\n",
      "systems, 4\n",
      "2004. 4\n",
      "hinton 3\n",
      "such 3\n",
      "system 3\n",
      "state-of-the-art 3\n",
      "better 3\n",
      "net 3\n",
      "vision 3\n",
      "determined 3\n",
      "fixation, 3\n",
      "paper 3\n",
      "assume 3\n",
      "just 3\n",
      "tree 3\n",
      "ignore 3\n",
      "over 3\n",
      "out 3\n",
      "[2011]) 3\n",
      "above 3\n",
      "parts 3\n",
      "properties 3\n",
      "present 3\n",
      "include 3\n",
      "way 3\n",
      "long 3\n",
      "mechanism 3\n",
      "computes 3\n",
      "increasing 3\n",
      "max-pooling 3\n",
      "values 3\n",
      "last 3\n",
      "larger 3\n",
      "space 3\n",
      "simply 3\n",
      "current 3\n",
      "therefore 3\n",
      "almost 3\n",
      "jjsj 3\n",
      "where 3\n",
      "^ujji 3\n",
      "time 3\n",
      "agreement 3\n",
      "capsule, 3\n",
      "therefore, 3\n",
      "1: 3\n",
      "l 3\n",
      "allow 3\n",
      "\u0015 3\n",
      "4 3\n",
      "shown 3\n",
      "intensities 3\n",
      "channels 3\n",
      "8d 3\n",
      "found 3\n",
      "was 3\n",
      "6) 3\n",
      "their 3\n",
      "see 3\n",
      "tensorflow 3\n",
      "adam 3\n",
      "regularization 3\n",
      "encode 3\n",
      "2. 3\n",
      "robust 3\n",
      "up 3\n",
      "pixels 3\n",
      "direction 3\n",
      "10k 3\n",
      "confuses 3\n",
      "3) 3\n",
      "rate 3\n",
      "three 3\n",
      "10 3\n",
      "learn 3\n",
      "variations 3\n",
      "width 3\n",
      "traditional 3\n",
      "natural 3\n",
      "data. 3\n",
      "translation 3\n",
      "set, 3\n",
      "ba 3\n",
      "need 3\n",
      "generate 3\n",
      "case 3\n",
      "dataset. 3\n",
      "best 3\n",
      "account 3\n",
      "1) 3\n",
      "l:(2; 3\n",
      "l:(4; 3\n",
      "step 3\n",
      "deal 3\n",
      "hidden 3\n",
      "representational 3\n",
      "recurrent 3\n",
      "exponential 3\n",
      "viewpoint 3\n",
      "machine 3\n",
      "2015. 3\n",
      "schmidhuber. 3\n",
      "computer 3\n",
      "pattern 3\n",
      "2011. 3\n",
      "2013. 3\n",
      "unsupervised 3\n",
      "hinton, 3\n",
      "yann 3\n",
      "iteration. 3\n",
      "e. 2\n",
      "active. 2\n",
      "multi-layer 2\n",
      "performance 2\n",
      "recognizing 2\n",
      "digits. 2\n",
      "routing-by-agreement 2\n",
      "lower-level 2\n",
      "details 2\n",
      "sequence 2\n",
      "fixation 2\n",
      "processed 2\n",
      "trees 2\n",
      "carved 2\n",
      "called 2\n",
      "problem 2\n",
      "assigning 2\n",
      "within 2\n",
      "types 2\n",
      "pose 2\n",
      "instantiated 2\n",
      "separate 2\n",
      "logistic 2\n",
      "unit 2\n",
      "explore 2\n",
      "cannot 2\n",
      "applying 2\n",
      "sent 2\n",
      "parent, 2\n",
      "“prediction 2\n",
      "multiplying 2\n",
      "large 2\n",
      "increases 2\n",
      "capsule’s 2\n",
      "output. 2\n",
      "far 2\n",
      "effective 2\n",
      "local 2\n",
      "below. 2\n",
      "implement 2\n",
      "translated 2\n",
      "extremely 2\n",
      "even 2\n",
      "routing-by-agreement, 2\n",
      "would 2\n",
      "image, 2\n",
      "unlike 2\n",
      "low 2\n",
      "capsules, 2\n",
      "ascend 2\n",
      "entities 2\n",
      "suggests 2\n",
      "inputs 2\n",
      "general 2\n",
      "whole 2\n",
      "implementation 2\n",
      "input. 2\n",
      "shrunk 2\n",
      "zero 2\n",
      "non-linearity. 2\n",
      "jj2 2\n",
      "produced 2\n",
      "cij^ujji 2\n",
      "; 2\n",
      "cij 2\n",
      "process. 2\n",
      "prior 2\n",
      "sense 2\n",
      "representations 2\n",
      "probably 2\n",
      "exist. 2\n",
      "k 2\n",
      "priors 2\n",
      "before 2\n",
      "computing 2\n",
      "rather 2\n",
      "procedure 2\n",
      "2: 2\n",
      "0. 2\n",
      "3: 2\n",
      "4: 2\n",
      ". 2\n",
      "softmax 2\n",
      "5: 2\n",
      "tc 2\n",
      "c 2\n",
      "suggest 2\n",
      "losses 2\n",
      "relu 2\n",
      "activating 2\n",
      "second 2\n",
      "32 2\n",
      "6] 2\n",
      "class. 2\n",
      "discussion 2\n",
      "chang 2\n",
      "instance 2\n",
      "loss. 2\n",
      "matrix 2\n",
      "(1; 2\n",
      "sigmoid 2\n",
      "target 2\n",
      "vector) 2\n",
      "16d 2\n",
      "(bij 2\n",
      ") 2\n",
      "optimizer 2\n",
      "default 2\n",
      "including 2\n",
      "rate, 2\n",
      "minimize 2\n",
      "4. 2\n",
      "method 2\n",
      "additional 2\n",
      "illustrated 2\n",
      "28 2\n",
      "(lecun 2\n",
      "shifted 2\n",
      "respectively. 2\n",
      "sample 2\n",
      "iterations. 2\n",
      "(l; 2\n",
      "p; 2\n",
      "r) 2\n",
      "classifications 2\n",
      "(2; 2\n",
      "(8; 2\n",
      "3; 2\n",
      "(%) 2\n",
      "yes 2\n",
      "ensembling 2\n",
      "augmentation 2\n",
      "tab. 2\n",
      "importance 2\n",
      "cnn 2\n",
      "256; 2\n",
      "128 2\n",
      "cross 2\n",
      "entropy 2\n",
      "individual 2\n",
      "encoding 2\n",
      "stroke 2\n",
      "skew 2\n",
      "reconstruction. 2\n",
      "examples 2\n",
      "dimension 2\n",
      "some 2\n",
      "variation 2\n",
      "robustness 2\n",
      "16 2\n",
      "thickness 2\n",
      "40 2\n",
      "random 2\n",
      "early 2\n",
      "expanded 2\n",
      "affnist 2\n",
      "parallel 2\n",
      "attention 2\n",
      "overlap. 2\n",
      "task 2\n",
      "domain 2\n",
      "shape 2\n",
      "top 2\n",
      "another 2\n",
      "boxes 2\n",
      "pairs 2\n",
      "vs 2\n",
      "pick 2\n",
      "chosen 2\n",
      "reconstructed 2\n",
      "lower 2\n",
      "assign 2\n",
      "fit 2\n",
      "0 2\n",
      "already 2\n",
      "r:(2; 2\n",
      "r:(6; 2\n",
      "l:(6; 2\n",
      "l:(7; 2\n",
      "r:(8; 2\n",
      "r:(9; 2\n",
      "l:(8; 2\n",
      "l:(9; 2\n",
      "increased 2\n",
      "decay 2\n",
      "observe 2\n",
      "both 2\n",
      "style 2\n",
      "given 2\n",
      "them. 2\n",
      "pooling 2\n",
      "smaller 2\n",
      "64 2\n",
      "10.6% 2\n",
      "24 2\n",
      "category 2\n",
      "everything 2\n",
      "nets 2\n",
      "uses 2\n",
      "routing. 2\n",
      "smallnorb 2\n",
      "32x32 2\n",
      "speech 2\n",
      "double 2\n",
      "now 2\n",
      "approach 2\n",
      "recognition, 2\n",
      "inefficiencies 2\n",
      "novel 2\n",
      "viewpoints. 2\n",
      "fragments 2\n",
      "matrices 2\n",
      "motivated 2\n",
      "perceptual 2\n",
      "advantage 2\n",
      "methods 2\n",
      "transformer 2\n",
      "vision, 2\n",
      "research 2\n",
      "systems. 2\n",
      "2016. 2\n",
      "dan 2\n",
      "ueli 2\n",
      "meier, 2\n",
      "jürgen 2\n",
      "classification. 2\n",
      "ieee 2\n",
      "on, 2\n",
      "ieee, 2\n",
      "artificial 2\n",
      "2011, 2\n",
      "international 2\n",
      "2, 2\n",
      "e 2\n",
      "zoubin 2\n",
      "ghahramani, 2\n",
      "yee 2\n",
      "whye 2\n",
      "teh. 2\n",
      "463–469, 2\n",
      "andrew 2\n",
      "lecun, 2\n",
      "proceedings 2\n",
      "volume 2\n",
      "matthew 2\n",
      "rob 2\n",
      "fergus. 2\n",
      "experimentally 2\n",
      "a.1 2\n",
      "500 2\n",
      "epochs 2\n",
      "a.2 2\n",
      "capsulenet 2\n",
      "sara 1\n",
      "sabour 1\n",
      "nicholas 1\n",
      "frosst 1\n",
      "google 1\n",
      "brain 1\n",
      "toronto 1\n",
      "{sasabour, 1\n",
      "frosst, 1\n",
      "geoffhinton}@google.com 1\n",
      "abstract 1\n",
      "group 1\n",
      "specific 1\n",
      "part. 1\n",
      "exists 1\n",
      "paramters. 1\n",
      "predictions, 1\n",
      "via 1\n",
      "matrices, 1\n",
      "predictions 1\n",
      "agree, 1\n",
      "becomes 1\n",
      "discrimininatively 1\n",
      "trained, 1\n",
      "considerably 1\n",
      "achieve 1\n",
      "mechanism: 1\n",
      "prefers 1\n",
      "send 1\n",
      "big 1\n",
      "coming 1\n",
      "introduction 1\n",
      "human 1\n",
      "ignores 1\n",
      "irrelevant 1\n",
      "carefully 1\n",
      "points 1\n",
      "tiny 1\n",
      "fraction 1\n",
      "optic 1\n",
      "array 1\n",
      "ever 1\n",
      "highest 1\n",
      "resolution. 1\n",
      "introspection 1\n",
      "poor 1\n",
      "guide 1\n",
      "understanding 1\n",
      "scene 1\n",
      "comes 1\n",
      "fixations 1\n",
      "glean 1\n",
      "us 1\n",
      "identified 1\n",
      "properties. 1\n",
      "creates 1\n",
      "something 1\n",
      "issue 1\n",
      "single-fixation 1\n",
      "coordinated 1\n",
      "fixations. 1\n",
      "generally 1\n",
      "constructed 1\n",
      "fly 1\n",
      "dynamically 1\n",
      "allocating 1\n",
      "memory, 1\n",
      "but, 1\n",
      "following 1\n",
      "[2000a], 1\n",
      "shall 1\n",
      "that, 1\n",
      "fixed 1\n",
      "multilayer 1\n",
      "sculpture 1\n",
      "rock. 1\n",
      "divided 1\n",
      "groups 1\n",
      "“capsules” 1\n",
      "node 1\n",
      "correspond 1\n",
      "process, 1\n",
      "choose 1\n",
      "tree. 1\n",
      "levels 1\n",
      "system, 1\n",
      "process 1\n",
      "solving 1\n",
      "wholes. 1\n",
      "various 1\n",
      "particular 1\n",
      "parameter 1\n",
      "(position, 1\n",
      "size, 1\n",
      "orientation), 1\n",
      "deformation, 1\n",
      "velocity, 1\n",
      "albedo, 1\n",
      "hue, 1\n",
      "texture 1\n",
      "etc. 1\n",
      "special 1\n",
      "property 1\n",
      "obvious 1\n",
      "exists. 1\n",
      "interesting 1\n",
      "alternative 1\n",
      "overall 1\n",
      "force 1\n",
      "31st 1\n",
      "systems 1\n",
      "(nips 1\n",
      "2017), 1\n",
      "beach, 1\n",
      "ca, 1\n",
      "usa. 1\n",
      "arxiv:1710.09829v1 1\n",
      "[cs.cv] 1\n",
      "26 1\n",
      "oct 1\n",
      "2017 1\n",
      "entity1. 1\n",
      "exceed 1\n",
      "non-linearity 1\n",
      "leaves 1\n",
      "unchanged 1\n",
      "scales 1\n",
      "magnitude. 1\n",
      "powerful 1\n",
      "gets 1\n",
      "appropriate 1\n",
      "above. 1\n",
      "initially, 1\n",
      "routed 1\n",
      "parents 1\n",
      "scaled 1\n",
      "vector” 1\n",
      "own 1\n",
      "matrix. 1\n",
      "top-down 1\n",
      "feedback 1\n",
      "effect 1\n",
      "coefficient 1\n",
      "decreasing 1\n",
      "parents. 1\n",
      "contribution 1\n",
      "thus 1\n",
      "further 1\n",
      "parent’s 1\n",
      "“routing-by-agreement” 1\n",
      "primitive 1\n",
      "form 1\n",
      "implemented 1\n",
      "detector 1\n",
      "pool 1\n",
      "demonstrate 1\n",
      "“explaining 1\n",
      "away” 1\n",
      "needed 1\n",
      "objects. 1\n",
      "(cnns) 1\n",
      "replicas 1\n",
      "translate 1\n",
      "acquired 1\n",
      "positions. 1\n",
      "proven 1\n",
      "helpful 1\n",
      "interpretation. 1\n",
      "though 1\n",
      "replacing 1\n",
      "scalar-output 1\n",
      "cnns 1\n",
      "vector-output 1\n",
      "still 1\n",
      "replicate 1\n",
      "across 1\n",
      "space, 1\n",
      "convolutional. 1\n",
      "cnns, 1\n",
      "cover 1\n",
      "regions 1\n",
      "throw 1\n",
      "away 1\n",
      "precise 1\n",
      "region. 1\n",
      "“place-coded” 1\n",
      "hierarchy 1\n",
      "positional 1\n",
      "“rate-coded” 1\n",
      "real-valued 1\n",
      "components 1\n",
      "shift 1\n",
      "place-coding 1\n",
      "rate-coding 1\n",
      "combined 1\n",
      "complex 1\n",
      "degrees 1\n",
      "freedom 1\n",
      "dimensionality 1\n",
      "increase 1\n",
      "hierarchy. 1\n",
      "computed 1\n",
      "ways 1\n",
      "idea 1\n",
      "aim 1\n",
      "fairly 1\n",
      "straightforward 1\n",
      "works 1\n",
      "well 1\n",
      "helps. 1\n",
      "want 1\n",
      "represented 1\n",
      "non-linear 1\n",
      "\"squashing\" 1\n",
      "function 1\n",
      "short 1\n",
      "slightly 1\n",
      "leave 1\n",
      "discriminative 1\n",
      "jj 1\n",
      "(1) 1\n",
      "weighted 1\n",
      "vectors” 1\n",
      "ui 1\n",
      "matrixwij 1\n",
      "x 1\n",
      "=wijui 1\n",
      "(2) 1\n",
      "“routing 1\n",
      "softmax” 1\n",
      "probabilities 1\n",
      "1this 1\n",
      "biological 1\n",
      "accurate 1\n",
      "things 1\n",
      "don’t 1\n",
      "coupled 1\n",
      "j. 1\n",
      "pexp(bij) 1\n",
      "exp(bik) 1\n",
      "(3) 1\n",
      "discriminatively 1\n",
      "weights. 1\n",
      "depend 1\n",
      "image2. 1\n",
      "iteratively 1\n",
      "refined 1\n",
      "measuring 1\n",
      "j, 1\n",
      "made 1\n",
      "i. 1\n",
      "aij 1\n",
      ":^ujji. 1\n",
      "treated 1\n",
      "likelihood 1\n",
      "added 1\n",
      "logit, 1\n",
      "new 1\n",
      "linking 1\n",
      "unit. 1\n",
      "algorithm. 1\n",
      "routing(^ujji, 1\n",
      "r, 1\n",
      "l) 1\n",
      "r 1\n",
      "l: 1\n",
      "ci 1\n",
      "softmax(bi) 1\n",
      "p 1\n",
      "6: 1\n",
      "squash(sj) 1\n",
      "squash 1\n",
      "7: 1\n",
      "^ujji:vj 1\n",
      "return 1\n",
      "exists, 1\n",
      "top-level 1\n",
      "loss, 1\n",
      "lk 1\n",
      "k: 1\n",
      "lc 1\n",
      "max(0;m+ 1\n",
      "jjvcjj)2 1\n",
      "(1 1\n",
      "tc) 1\n",
      "max(0; 1\n",
      "jjvcjj 1\n",
      "m􀀀)2 1\n",
      "(4) 1\n",
      "iff 1\n",
      "present3 1\n",
      "m+ 1\n",
      "0:9 1\n",
      "m􀀀 1\n",
      "0:1. 1\n",
      "down-weighting 1\n",
      "absent 1\n",
      "classes 1\n",
      "stops 1\n",
      "shrinking 1\n",
      "lengths 1\n",
      "0:5. 1\n",
      "shallow 1\n",
      "256, 1\n",
      "activation. 1\n",
      "converts 1\n",
      "lowest 1\n",
      "multi-dimensional 1\n",
      "and, 1\n",
      "inverse 1\n",
      "graphics 1\n",
      "perspective, 1\n",
      "corresponds 1\n",
      "inverting 1\n",
      "rendering 1\n",
      "computation 1\n",
      "piecing 1\n",
      "together 1\n",
      "familiar 1\n",
      "wholes, 1\n",
      "designed 1\n",
      "at. 1\n",
      "(primarycapsules) 1\n",
      "(i.e. 1\n",
      "contains 1\n",
      "kernel 1\n",
      "2). 1\n",
      "sees 1\n",
      "256\u000281 1\n",
      "receptive 1\n",
      "fields 1\n",
      "center 1\n",
      "[32; 1\n",
      "6; 1\n",
      "2for 1\n",
      "sufficient 1\n",
      "equal. 1\n",
      "3we 1\n",
      "contain 1\n",
      "instances 1\n",
      "address 1\n",
      "weakness 1\n",
      "section. 1\n",
      "layers. 1\n",
      "comparable 1\n",
      "(such 1\n",
      "chen 1\n",
      "[2015]). 1\n",
      "indicates 1\n",
      "presence 1\n",
      "calculate 1\n",
      "wij 1\n",
      "ui; 1\n",
      "10). 1\n",
      "structure 1\n",
      "representation. 1\n",
      "euclidean 1\n",
      "distance 1\n",
      "minimized 1\n",
      "true 1\n",
      "(each 1\n",
      "[6; 1\n",
      "sharing 1\n",
      "weights 1\n",
      "other. 1\n",
      "block 1\n",
      "(digitcaps) 1\n",
      "per 1\n",
      "receives 1\n",
      "consecutive 1\n",
      "(e.g. 1\n",
      "digitcaps). 1\n",
      "1d 1\n",
      "agree 1\n",
      "on. 1\n",
      "primarycapsules. 1\n",
      "initialized 1\n",
      "zero. 1\n",
      "initially 1\n",
      "(ui) 1\n",
      "(v0:::v10) 1\n",
      "equal 1\n",
      "(cij 1\n",
      "). 1\n",
      "(abadi 1\n",
      "[2016]) 1\n",
      "parameters, 1\n",
      "decaying 1\n",
      "4.1 1\n",
      "encourage 1\n",
      "training, 1\n",
      "mask 1\n",
      "reconstruct. 1\n",
      "fed 1\n",
      "consisting 1\n",
      "described 1\n",
      "squared 1\n",
      "differences 1\n",
      "intensities. 1\n",
      "0:0005 1\n",
      "dominate 1\n",
      "keeping 1\n",
      "important 1\n",
      "details. 1\n",
      "performed 1\n",
      "[1998]) 1\n",
      "been 1\n",
      "padding. 1\n",
      "augmentation/deformation 1\n",
      "used. 1\n",
      "60k 1\n",
      "testing 1\n",
      "label, 1\n",
      "rightmost 1\n",
      "failure 1\n",
      "explains 1\n",
      "picks 1\n",
      "smoothing 1\n",
      "noise. 1\n",
      "2; 1\n",
      "2) 1\n",
      "5; 1\n",
      "8; 1\n",
      "(9; 1\n",
      "9; 1\n",
      "table 1\n",
      "accuracy. 1\n",
      "deviation 1\n",
      "reported 1\n",
      "trials. 1\n",
      "0:39 1\n",
      "0:34\u00060:032 1\n",
      "0:29\u00060:011 1\n",
      "0:35\u00060:036 1\n",
      "0:25\u00060:005 1\n",
      "(ciregan 1\n",
      "[2012]) 1\n",
      "drastic 1\n",
      "(sato 1\n",
      "[2015]) 1\n",
      "(wan 1\n",
      "[2013] 1\n",
      "0:21% 1\n",
      "0:57% 1\n",
      "without 1\n",
      "them). 1\n",
      "(0:25%) 1\n",
      "previously 1\n",
      "deeper 1\n",
      "reports 1\n",
      "setups 1\n",
      "regularizer. 1\n",
      "channels. 1\n",
      "5x5 1\n",
      "followed 1\n",
      "328; 1\n",
      "192. 1\n",
      "dropout 1\n",
      "5.1 1\n",
      "passing 1\n",
      "zeroing 1\n",
      "span 1\n",
      "instantiated. 1\n",
      "thickness, 1\n",
      "width. 1\n",
      "digit-specific 1\n",
      "tail 1\n",
      "making 1\n",
      "feed 1\n",
      "perturbed 1\n",
      "version 1\n",
      "perturbation 1\n",
      "affects 1\n",
      "perturbations 1\n",
      "(out 1\n",
      "16) 1\n",
      "always 1\n",
      "combinations 1\n",
      "global 1\n",
      "variations, 1\n",
      "example, 1\n",
      "ascender 1\n",
      "loop. 1\n",
      "5.2 1\n",
      "experiments 1\n",
      "learns 1\n",
      "variance 1\n",
      "skew, 1\n",
      "rotation, 1\n",
      "style, 1\n",
      "etc 1\n",
      "hand 1\n",
      "written 1\n",
      "moderately 1\n",
      "(with 1\n",
      "maxpooling 1\n",
      "dropout) 1\n",
      "padded 1\n",
      "perturbations. 1\n",
      "row 1\n",
      "tweaked 1\n",
      "intervals 1\n",
      "0:05 1\n",
      "range 1\n",
      "[􀀀0:25; 1\n",
      "0:25]. 1\n",
      "placed 1\n",
      "randomly 1\n",
      "black 1\n",
      "background 1\n",
      "affnist4 1\n",
      "transformation. 1\n",
      "never 1\n",
      "seen 1\n",
      "mnist. 1\n",
      "under-trained 1\n",
      "stopping 1\n",
      "99.23% 1\n",
      "79% 1\n",
      "(99.22%) 1\n",
      "66% 1\n",
      "viewed 1\n",
      "attend 1\n",
      "others. 1\n",
      "recognize 1\n",
      "prupose 1\n",
      "[2000b] 1\n",
      "others 1\n",
      "(goodfellow 1\n",
      "[2013], 1\n",
      "[2014], 1\n",
      "greff 1\n",
      "[2016]). 1\n",
      "help 1\n",
      "obviate 1\n",
      "decisions 1\n",
      "6.1 1\n",
      "overlaying 1\n",
      "(training 1\n",
      "test) 1\n",
      "resulting 1\n",
      "36\u000236 1\n",
      "considering 1\n",
      "28\u000228 1\n",
      "bounded 1\n",
      "20\u000220 1\n",
      "box, 1\n",
      "bounding 1\n",
      "80% 1\n",
      "1k 1\n",
      "examples. 1\n",
      "60m 1\n",
      "10m. 1\n",
      "6.2 1\n",
      "scratch 1\n",
      "model. 1\n",
      "achieving 1\n",
      "5:0% 1\n",
      "sequential 1\n",
      "[2014] 1\n",
      "easier 1\n",
      "less 1\n",
      "(80% 1\n",
      "around 1\n",
      "< 1\n",
      "4% 1\n",
      "[2014]). 1\n",
      "images, 1\n",
      "composed 1\n",
      "treat 1\n",
      "(we 1\n",
      "know 1\n",
      "4available 1\n",
      "http://www.cs.toronto.edu/~tijmen/affnist/. 1\n",
      "overlayed 1\n",
      "green 1\n",
      "red 1\n",
      "upper 1\n",
      "l:(l1; 1\n",
      "l2) 1\n",
      "r:(r1; 1\n",
      "r2) 1\n",
      "wrong 1\n",
      "(p). 1\n",
      "(4; 1\n",
      "accounts 1\n",
      "being 1\n",
      "difficult 1\n",
      "scenarios 1\n",
      "(column 1\n",
      "4). 1\n",
      "note 1\n",
      "generation 1\n",
      "clipped 1\n",
      "(*) 1\n",
      "mark 1\n",
      "neither 1\n",
      "nor 1\n",
      "prediction. 1\n",
      "finding 1\n",
      "ones 1\n",
      "knows 1\n",
      "also, 1\n",
      "loop 1\n",
      "triggered 1\n",
      "accounted 1\n",
      "8. 1\n",
      "support. 1\n",
      "r:(7; 1\n",
      "*r:(5; 1\n",
      "*r:(2; 1\n",
      "r:p:(2; 1\n",
      "l:(5; 1\n",
      "*r:(0; 1\n",
      "*r:(1; 1\n",
      "r:(4; 1\n",
      "r:p:(4; 1\n",
      "l:(1; 1\n",
      "composite 1\n",
      "image). 1\n",
      "difference 1\n",
      "10\u0002 1\n",
      "larger. 1\n",
      "segment 1\n",
      "original 1\n",
      "correctly 1\n",
      "overlaps 1\n",
      "(a 1\n",
      "digits) 1\n",
      "accounting 1\n",
      "encoded 1\n",
      "digitcaps. 1\n",
      "encoding. 1\n",
      "regardless 1\n",
      "votes 1\n",
      "receiving 1\n",
      "emphasizes 1\n",
      "task. 1\n",
      "512 1\n",
      "256 1\n",
      "5\u00025 1\n",
      "third 1\n",
      "1024d 1\n",
      "non-linearities. 1\n",
      "connected. 1\n",
      "train 1\n",
      "24:56m 1\n",
      "times 1\n",
      "11:36m 1\n",
      "parameters. 1\n",
      "started 1\n",
      "(32 1\n",
      "512d 1\n",
      "layer) 1\n",
      "incrementally 1\n",
      "until 1\n",
      "reached 1\n",
      "subset 1\n",
      "searched 1\n",
      "validation 1\n",
      "decode 1\n",
      "non-zero 1\n",
      "intensity 1\n",
      "datasets 1\n",
      "few 1\n",
      "settings 1\n",
      "hyper-parameters 1\n",
      "ensemble 1\n",
      "patches 1\n",
      "except 1\n",
      "color 1\n",
      "helped 1\n",
      "introduce 1\n",
      "\"none-of-the-above\" 1\n",
      "softmaxes, 1\n",
      "expect 1\n",
      "ten 1\n",
      "explain 1\n",
      "applied 1\n",
      "(zeiler 1\n",
      "fergus 1\n",
      "[2013]). 1\n",
      "drawback 1\n",
      "shares 1\n",
      "generative 1\n",
      "likes 1\n",
      "clutter 1\n",
      "“orphan” 1\n",
      "cifar-10, 1\n",
      "backgrounds 1\n",
      "too 1\n",
      "varied 1\n",
      "reasonable 1\n",
      "sized 1\n",
      "helps 1\n",
      "poorer 1\n",
      "performance. 1\n",
      "exact 1\n",
      "[2004]) 1\n",
      "2:7% 1\n",
      "in-par 1\n",
      "(cire¸san 1\n",
      "[2011]). 1\n",
      "consists 1\n",
      "96x96 1\n",
      "stereo 1\n",
      "grey-scale 1\n",
      "resized 1\n",
      "48x48 1\n",
      "crops 1\n",
      "passed 1\n",
      "central 1\n",
      "patch 1\n",
      "test. 1\n",
      "svhn 1\n",
      "(netzer 1\n",
      "73257 1\n",
      "reduced 1\n",
      "64, 1\n",
      "6d-capsules 1\n",
      "end 1\n",
      "4:3% 1\n",
      "previous 1\n",
      "work 1\n",
      "thirty 1\n",
      "years, 1\n",
      "markov 1\n",
      "gaussian 1\n",
      "mixtures 1\n",
      "distributions. 1\n",
      "easy 1\n",
      "computers, 1\n",
      "had 1\n",
      "limitation 1\n",
      "ultimately 1\n",
      "fatal: 1\n",
      "one-of-n 1\n",
      "inefficient 1\n",
      "compared 1\n",
      "with, 1\n",
      "say, 1\n",
      "representations. 1\n",
      "amount 1\n",
      "hmm 1\n",
      "remember 1\n",
      "string 1\n",
      "generated 1\n",
      "far, 1\n",
      "square 1\n",
      "nodes. 1\n",
      "neurons. 1\n",
      "become 1\n",
      "dominant 1\n",
      "ask 1\n",
      "whether 1\n",
      "may 1\n",
      "lead 1\n",
      "demise. 1\n",
      "candidate 1\n",
      "difficulty 1\n",
      "generalizing 1\n",
      "ability 1\n",
      "built 1\n",
      "in, 1\n",
      "chose 1\n",
      "replicating 1\n",
      "grows 1\n",
      "dimensions, 1\n",
      "labelled 1\n",
      "similarly 1\n",
      "way. 1\n",
      "avoid 1\n",
      "converting 1\n",
      "recognized 1\n",
      "predict 1\n",
      "fragments. 1\n",
      "intrinsic 1\n",
      "relationship 1\n",
      "constitute 1\n",
      "invariant 1\n",
      "automatically 1\n",
      "generalizes 1\n",
      "strong 1\n",
      "assumption: 1\n",
      "represents. 1\n",
      "assumption, 1\n",
      "phenomenon 1\n",
      "\"crowding\" 1\n",
      "(pelli 1\n",
      "[2004]), 1\n",
      "eliminates 1\n",
      "binding 1\n",
      "[1981]) 1\n",
      "(its 1\n",
      "location. 1\n",
      "efficient 1\n",
      "point 1\n",
      "high-dimensional 1\n",
      "representation, 1\n",
      "take 1\n",
      "full 1\n",
      "relationships 1\n",
      "modelled 1\n",
      "multiplies. 1\n",
      "vary 1\n",
      "varies 1\n",
      "trying 1\n",
      "eliminate 1\n",
      "activities. 1\n",
      "\"normalization\" 1\n",
      "(jaderberg 1\n",
      "[2015]): 1\n",
      "time. 1\n",
      "dealing 1\n",
      "segmentation, 1\n",
      "toughest 1\n",
      "problems 1\n",
      "demonstrated 1\n",
      "paper. 1\n",
      "stage 1\n",
      "beginning 1\n",
      "century. 1\n",
      "fundamental 1\n",
      "reasons 1\n",
      "believing 1\n",
      "requires 1\n",
      "lot 1\n",
      "insights 1\n",
      "out-perform 1\n",
      "developed 1\n",
      "technology. 1\n",
      "unparalleled 1\n",
      "indication 1\n",
      "worth 1\n",
      "exploring. 1\n",
      "references 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "martín 1\n",
      "abadi, 1\n",
      "ashish 1\n",
      "agarwal, 1\n",
      "paul 1\n",
      "barham, 1\n",
      "eugene 1\n",
      "brevdo, 1\n",
      "zhifeng 1\n",
      "chen, 1\n",
      "craig 1\n",
      "citro, 1\n",
      "greg 1\n",
      "s 1\n",
      "corrado, 1\n",
      "andy 1\n",
      "davis, 1\n",
      "jeffrey 1\n",
      "dean, 1\n",
      "matthieu 1\n",
      "devin, 1\n",
      "tensorflow: 1\n",
      "large-scale 1\n",
      "heterogeneous 1\n",
      "arxiv:1603.04467, 1\n",
      "jimmy 1\n",
      "ba, 1\n",
      "volodymyr 1\n",
      "mnih, 1\n",
      "koray 1\n",
      "kavukcuoglu. 1\n",
      "attention. 1\n",
      "arxiv:1412.7755, 1\n",
      "2014. 1\n",
      "jia-ren 1\n",
      "yong-sheng 1\n",
      "chen. 1\n",
      "batch-normalized 1\n",
      "maxout 1\n",
      "arxiv:1511.02583, 1\n",
      "ciregan, 1\n",
      "multi-column 1\n",
      "(cvpr), 1\n",
      "2012 1\n",
      "3642–3649. 1\n",
      "2012. 1\n",
      "cire¸san, 1\n",
      "jonathan 1\n",
      "masci, 1\n",
      "luca 1\n",
      "m 1\n",
      "gambardella, 1\n",
      "highperformance 1\n",
      "arxiv:1102.0183, 1\n",
      "ian 1\n",
      "goodfellow, 1\n",
      "yaroslav 1\n",
      "bulatov, 1\n",
      "julian 1\n",
      "ibarz, 1\n",
      "sacha 1\n",
      "arnoud, 1\n",
      "vinay 1\n",
      "shet. 1\n",
      "multi-digit 1\n",
      "street 1\n",
      "view 1\n",
      "imagery 1\n",
      "arxiv:1312.6082, 1\n",
      "klaus 1\n",
      "greff, 1\n",
      "antti 1\n",
      "rasmus, 1\n",
      "mathias 1\n",
      "berglund, 1\n",
      "tele 1\n",
      "hao, 1\n",
      "harri 1\n",
      "valpola, 1\n",
      "juergen 1\n",
      "tagger: 1\n",
      "grouping. 1\n",
      "4484–4492, 1\n",
      "alex 1\n",
      "krizhevsky, 1\n",
      "sida 1\n",
      "wang. 1\n",
      "transforming 1\n",
      "auto-encoders. 1\n",
      "learning–icann 1\n",
      "44–51, 1\n",
      "hinton. 1\n",
      "joint 1\n",
      "intelligence 1\n",
      "vol 1\n",
      "1981. 1\n",
      "2000a. 1\n",
      "2000b. 1\n",
      "max 1\n",
      "jaderberg, 1\n",
      "karen 1\n",
      "simonyan, 1\n",
      "zisserman, 1\n",
      "2017–2025, 1\n",
      "corinna 1\n",
      "cortes, 1\n",
      "christopher 1\n",
      "jc 1\n",
      "burges. 1\n",
      "database 1\n",
      "handwritten 1\n",
      "1998. 1\n",
      "fu 1\n",
      "jie 1\n",
      "huang, 1\n",
      "leon 1\n",
      "bottou. 1\n",
      "generic 1\n",
      "invariance 1\n",
      "lighting. 1\n",
      "cvpr 1\n",
      "2004 1\n",
      "society 1\n",
      "ii–104. 1\n",
      "yuval 1\n",
      "netzer, 1\n",
      "tao 1\n",
      "wang, 1\n",
      "coates, 1\n",
      "alessandro 1\n",
      "bissacco, 1\n",
      "bo 1\n",
      "wu, 1\n",
      "y 1\n",
      "ng. 1\n",
      "reading 1\n",
      "learning. 1\n",
      "nips 1\n",
      "workshop 1\n",
      "learning, 1\n",
      "page 1\n",
      "5, 1\n",
      "denis 1\n",
      "g 1\n",
      "pelli, 1\n",
      "melanie 1\n",
      "palomares, 1\n",
      "najib 1\n",
      "majaj. 1\n",
      "crowding 1\n",
      "ordinary 1\n",
      "masking: 1\n",
      "distinguishing 1\n",
      "integration 1\n",
      "detection. 1\n",
      "journal 1\n",
      "4(12):12–12, 1\n",
      "ikuro 1\n",
      "sato, 1\n",
      "hiroki 1\n",
      "nishimura, 1\n",
      "kensuke 1\n",
      "yokoi. 1\n",
      "apac: 1\n",
      "augmented 1\n",
      "arxiv:1505.03229, 1\n",
      "li 1\n",
      "wan, 1\n",
      "zeiler, 1\n",
      "sixin 1\n",
      "zhang, 1\n",
      "cun, 1\n",
      "dropconnect. 1\n",
      "30th 1\n",
      "(icml-13), 1\n",
      "1058–1066, 1\n",
      "d 1\n",
      "zeiler 1\n",
      "stochastic 1\n",
      "arxiv:1301.3557, 1\n",
      "use? 1\n",
      "order 1\n",
      "verify 1\n",
      "convergence 1\n",
      "algorithm 1\n",
      "plot 1\n",
      "negligible 1\n",
      "start 1\n",
      "2nd 1\n",
      "pass 1\n",
      "settles 1\n",
      "0.007 1\n",
      "1e 1\n",
      "average. 1\n",
      "a.1: 1\n",
      "logit 1\n",
      "stabilized 1\n",
      "decreases 1\n",
      "linearly 1\n",
      "(a) 1\n",
      "(b) 1\n",
      "differences. 1\n",
      "observed 1\n",
      "capacity 1\n",
      "tends 1\n",
      "overfit 1\n",
      "comparison 1\n",
      "experiments. 1\n",
      "a.2: 1\n",
      "traning 1\n",
      "batches 1\n",
      "optimizes 1\n",
      "faster 1\n",
      "converges 1\n",
      "end. 1\n"
     ]
    }
   ],
   "source": [
    "counts={}\n",
    "file=open('C:/Users/omerc/Documents/python files/Hinton.txt','r', encoding='utf8')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    line=line.lower() #lower case all characters\n",
    "    \n",
    "    words=line.split()\n",
    "    for word in words:\n",
    "        counts[word]=counts.get(word,0)+1  #dictionary.get(keyname, value) value- if dont find the keyname\n",
    "file.close()\n",
    "for word in sorted(counts,key=counts.get, reverse=True):\n",
    "    print(word, counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that 73\n",
      "with 61\n",
      "capsule 58\n",
      "each 41\n",
      "layer 41\n",
      "capsules 36\n",
      "routing 34\n",
      "digit 34\n",
      "this 32\n",
      "from 25\n",
      "output 24\n",
      "vector 22\n",
      "convolutional 22\n",
      "model 21\n",
      "test 21\n",
      "capsnet 20\n",
      "mnist 19\n",
      "neural 18\n",
      "which 17\n",
      "training 16\n",
      "image 14\n",
      "digits 14\n",
      "instantiation 13\n",
      "represent 13\n",
      "have 13\n",
      "parameters 12\n",
      "than 12\n",
      "only 12\n",
      "more 12\n",
      "other 12\n",
      "activity 11\n",
      "entity 11\n",
      "network 11\n",
      "there 11\n",
      "loss 11\n",
      "between 10\n",
      "length 10\n",
      "these 10\n",
      "networks 10\n",
      "learning 10\n",
      "layers 10\n",
      "pixel 10\n",
      "used 10\n",
      "level 9\n",
      "will 9\n",
      "image. 9\n",
      "different 9\n",
      "information 9\n",
      "fig. 9\n",
      "reconstruction 9\n",
      "achieved 9\n",
      "object 8\n",
      "active 8\n",
      "make 8\n",
      "show 8\n",
      "feature 8\n",
      "fully 8\n",
      "connected 8\n",
      "figure 8\n",
      "classification 8\n",
      "during 8\n",
      "multimnist 8\n",
      "size 8\n",
      "dimensions 8\n",
      "trained 8\n",
      "pages 8\n",
      "dynamic 7\n",
      "using 7\n",
      "parse 7\n",
      "should 7\n",
      "because 7\n",
      "same 7\n",
      "then 7\n",
      "class 7\n",
      "stride 7\n",
      "primary 7\n",
      "dataset 7\n",
      "shows 7\n",
      "average 7\n",
      "error 7\n",
      "affine 7\n",
      "arxiv 7\n",
      "preprint 7\n",
      "change 7\n",
      "type 6\n",
      "capsules. 6\n",
      "when 6\n",
      "multiple 6\n",
      "highly 6\n",
      "overlapping 6\n",
      "vectors 6\n",
      "prediction 6\n",
      "capsule. 6\n",
      "single 6\n",
      "small 6\n",
      "possible 6\n",
      "coupling 6\n",
      "input 6\n",
      "they 6\n",
      "convolution 6\n"
     ]
    }
   ],
   "source": [
    "counts = {} #create dictionary\n",
    "file = open ('python files/Hinton.txt','r', encoding='utf8')\n",
    "for line in file:\n",
    "    line = line.strip() #remove spaces\n",
    "    line = line.lower() # convert all to lower case\n",
    "    words = line.split() #make a list from the line\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0) + 1 #get used to take data from dict  dict.get(key[, value]) \n",
    "        #in this case we save the number of occurence, example: counts[routing]=counts.get(routing,0) + 1 when routing=2\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    if len(word) <= 3: # select a lot of a stop-words\n",
    "        continue\n",
    "    count += 1\n",
    "    if count > 100: # print only 100 first words\n",
    "        break\n",
    "    print(word, counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "freq={}\n",
    "file=open('python files/first_20k_words.txt','r')\n",
    "for line in file:\n",
    "    line=line.strip()\n",
    "    (word,f)=line.split()\n",
    "    freq[word]=float(f) #enter the line to the dict\n",
    "file.close()\n",
    "print(len(freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capsules 12247397.428046549\n",
      "capsule 8588648.176393058\n",
      "digit 4296455.424274971\n",
      "routing 3601771.223966609\n",
      "dataset 2347181.705395165\n",
      "digits 2100430.588270596\n",
      "stride 1832460.732984293\n",
      "iterations 1743739.9734951523\n",
      "pixel 1654314.4520910534\n",
      "sj 1426737.05236125\n",
      "iteration 1141617.6722415662\n",
      "iterative 1023253.4342943387\n",
      "reconstruct 1013007.0100085093\n",
      "neural 998186.6276264782\n",
      "scalar 917599.5595522114\n",
      "achieves 874278.7200559538\n",
      "detectors 752105.8965102286\n",
      "representational 734088.6289671373\n",
      "segmentation 725294.6509519493\n",
      "geoffrey 711662.7287995673\n",
      "vector 699340.7124374567\n",
      "ascend 697714.9834292692\n",
      "overlapping 679778.8452823346\n",
      "logistic 648403.306856865\n",
      "intensities 630636.3120388472\n",
      "activating 591226.2031453234\n",
      "layer 573233.3577074021\n",
      "vectors 502727.2955785134\n",
      "pixels 488432.29514335486\n",
      "transformer 483430.4222764739\n",
      "coupling 443033.3013364838\n",
      "transformations 422083.4036805673\n",
      "exponential 405575.30857521395\n",
      "outputs 399281.2936713915\n",
      "multiplying 387349.17591462826\n",
      "localized 377618.54862310836\n",
      "entity 358502.3726338845\n",
      "reconstruction 353022.8562687053\n",
      "routed 350189.1021151422\n",
      "reconstructed 340611.05623488536\n",
      "baseline 336453.94366078713\n",
      "30th 328105.51873482513\n",
      "ui 327107.38935592555\n",
      "classifications 326610.5985139218\n",
      "allocating 322258.3867745158\n",
      "nets 320924.2618741977\n",
      "assigning 316115.5718530695\n",
      "melanie 314524.7530980688\n",
      "klaus 313489.45107997116\n",
      "overlap 309064.07670970383\n",
      "prediction 307922.85506071214\n",
      "experimentally 306983.8833461243\n",
      "generative 302233.5056064315\n",
      "tao 300336.376741951\n",
      "squash 300210.1471029721\n",
      "drawback 296815.17319165356\n",
      "layers 295970.0715063693\n",
      "robust 295796.7284881829\n",
      "entropy 289544.546428468\n",
      "dynamically 287331.55187771167\n",
      "ba 286519.26842080127\n",
      "crowding 285420.71012672683\n",
      "replicate 273007.7261186491\n",
      "chang 267065.4844567888\n",
      "shrinking 261602.05096007956\n",
      "matrices 258792.4743148469\n",
      "smoothing 256891.1038610733\n",
      "tc 253971.479002908\n",
      "linearly 252959.62764342813\n",
      "encoding 252140.0385774259\n",
      "jj 251540.6867060747\n",
      "perturbation 251275.22175038318\n",
      "modelled 241289.45082520993\n",
      "recurrent 235225.85602609438\n",
      "coefficients 234860.8684215471\n",
      "rob 234038.56955626287\n",
      "viewpoint 233710.38608955784\n",
      "stereo 226147.13132363916\n",
      "output 223631.51489851784\n",
      "eliminates 223104.7253580831\n",
      "jc 222915.7378510923\n",
      "neurons 221236.49074678376\n",
      "512 219852.69869187646\n",
      "trained 213255.42401842534\n",
      "denis 213160.53119604374\n",
      "stabilized 210335.9064425888\n",
      "scaled 206130.3155855132\n",
      "grid 205482.26688036823\n",
      "vs 205300.8684226734\n",
      "networks 203131.47481575978\n",
      "columns 196684.5540341641\n",
      "parameters 196514.48803063013\n",
      "minimized 195545.47410000194\n",
      "fixation 195093.40096571232\n",
      "picks 187497.65627929653\n",
      "perceptual 186110.56828862027\n",
      "invariant 185932.35780822937\n",
      "squared 185642.41557911152\n",
      "augmented 184525.67674791947\n",
      "fu 183546.85951323374\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "file = open ('python files/first_20k_words.txt','r')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    (word, f) = line.split()\n",
    "    freq[word] = float(f)\n",
    "file.close()\n",
    "\n",
    "counts = {}\n",
    "file = open ('python files/Hinton.txt','r', encoding='utf8')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word in freq: # <--- \n",
    "            counts[word] = counts.get(word,0) + 1 / freq[word]\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    count += 1\n",
    "    if count > 100: # print only 100 first words\n",
    "        break\n",
    "    print(word, counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional 22\n",
      "al. 22\n",
      "capsnet 20\n",
      "mnist 19\n",
      "instantiation 13\n",
      "\u0002 10\n",
      "image. 9\n",
      "= 9\n",
      "fig. 9\n",
      "8) 9\n",
      "multimnist 8\n",
      "parse 7\n",
      "1. 7\n",
      "vj 7\n",
      "+ 7\n",
      "affine 7\n",
      "arxiv 7\n",
      "preprint 7\n",
      "capsules. 6\n",
      "capsule. 6\n",
      "bij 6\n",
      "convolution 6\n",
      "reconstructions 6\n",
      "7) 6\n",
      "􀀀 5\n",
      "kernels 5\n",
      "primarycapsules 5\n",
      "digitcaps 5\n",
      "decoder 5\n",
      "training. 5\n",
      "9) 5\n",
      "networks. 5\n",
      "0) 5\n",
      "images. 5\n",
      "higher-level 4\n",
      "(hinton 4\n",
      "segmenting 4\n",
      "logits 4\n",
      "(l 4\n",
      "1): 4\n",
      "eq. 4\n",
      "digits, 4\n",
      "layer. 4\n",
      "conv1 4\n",
      "exponentially 4\n",
      "digit. 4\n",
      "(5; 4\n",
      "5) 4\n",
      "- 4\n",
      "network. 4\n",
      "pixels. 4\n",
      "set. 4\n",
      "4) 4\n",
      "cifar10 4\n",
      "systems, 4\n",
      "2004. 4\n",
      "hinton 3\n",
      "state-of-the-art 3\n",
      "fixation, 3\n",
      "[2011]) 3\n",
      "computes 3\n",
      "max-pooling 3\n",
      "jjsj 3\n",
      "^ujji 3\n",
      "capsule, 3\n",
      "therefore, 3\n",
      "1: 3\n",
      "\u0015 3\n",
      "8d 3\n",
      "6) 3\n",
      "tensorflow 3\n",
      "regularization 3\n",
      "encode 3\n",
      "2. 3\n",
      "10k 3\n",
      "confuses 3\n",
      "3) 3\n",
      "data. 3\n",
      "set, 3\n",
      "dataset. 3\n",
      "1) 3\n",
      "l:(2; 3\n",
      "l:(4; 3\n",
      "2015. 3\n",
      "schmidhuber. 3\n",
      "2011. 3\n",
      "2013. 3\n",
      "unsupervised 3\n",
      "hinton, 3\n",
      "yann 3\n",
      "iteration. 3\n",
      "e. 2\n",
      "active. 2\n",
      "multi-layer 2\n",
      "digits. 2\n",
      "routing-by-agreement 2\n",
      "lower-level 2\n",
      "instantiated 2\n",
      "cannot 2\n",
      "parent, 2\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "file = open('python files/Hinton.txt', 'r', encoding='utf8')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.lower() # lower case all the characters\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if word not in freq: # <--- freq is the dict contains: words, frequency \n",
    "            counts[word] = counts.get(word,0) + 1\n",
    "file.close()\n",
    "\n",
    "n = 0\n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    n += 1\n",
    "    if n > 100:\n",
    "        break\n",
    "    print(word, counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic\n",
      "routing\n",
      "between\n",
      "capsules\n",
      "sara\n",
      "sabour\n",
      "nicholas\n",
      "frosst\n",
      "geoffrey\n",
      "e.\n",
      "hinton\n",
      "google\n",
      "brain\n",
      "toronto\n",
      "{sasabour,\n",
      "frosst,\n",
      "geoffhinton}@google.com\n",
      "abstract\n",
      "a\n",
      "capsule\n",
      "is\n",
      "a\n",
      "group\n",
      "of\n",
      "neurons\n",
      "whose\n",
      "activity\n",
      "vector\n",
      "represents\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "a\n",
      "specific\n",
      "type\n",
      "of\n",
      "entity\n",
      "such\n",
      "as\n",
      "an\n",
      "object\n",
      "or\n",
      "object\n",
      "part.\n",
      "we\n",
      "use\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "activity\n",
      "vector\n",
      "to\n",
      "represent\n",
      "the\n",
      "probability\n",
      "that\n",
      "the\n",
      "entity\n",
      "exists\n",
      "and\n",
      "its\n",
      "orientation\n",
      "to\n",
      "represent\n",
      "the\n",
      "instantiation\n",
      "paramters.\n",
      "active\n",
      "capsules\n",
      "at\n",
      "one\n",
      "level\n",
      "make\n",
      "predictions,\n",
      "via\n",
      "transformation\n",
      "matrices,\n",
      "for\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "higher-level\n",
      "capsules.\n",
      "when\n",
      "multiple\n",
      "predictions\n",
      "agree,\n",
      "a\n",
      "higher\n",
      "level\n",
      "capsule\n",
      "becomes\n",
      "active.\n",
      "we\n",
      "show\n",
      "that\n",
      "a\n",
      "discrimininatively\n",
      "trained,\n",
      "multi-layer\n",
      "capsule\n",
      "system\n",
      "achieves\n",
      "state-of-the-art\n",
      "performance\n",
      "on\n",
      "mnist\n",
      "and\n",
      "is\n",
      "considerably\n",
      "better\n",
      "than\n",
      "a\n",
      "convolutional\n",
      "net\n",
      "at\n",
      "recognizing\n",
      "highly\n",
      "overlapping\n",
      "digits.\n",
      "to\n",
      "achieve\n",
      "these\n",
      "results\n",
      "we\n",
      "use\n",
      "an\n",
      "iterative\n",
      "routing-by-agreement\n",
      "mechanism:\n",
      "a\n",
      "lower-level\n",
      "capsule\n",
      "prefers\n",
      "to\n",
      "send\n",
      "its\n",
      "output\n",
      "to\n",
      "higher\n",
      "level\n",
      "capsules\n",
      "whose\n",
      "activity\n",
      "vectors\n",
      "have\n",
      "a\n",
      "big\n",
      "scalar\n",
      "product\n",
      "with\n",
      "the\n",
      "prediction\n",
      "coming\n",
      "from\n",
      "the\n",
      "lower-level\n",
      "capsule.\n",
      "1\n",
      "introduction\n",
      "human\n",
      "vision\n",
      "ignores\n",
      "irrelevant\n",
      "details\n",
      "by\n",
      "using\n",
      "a\n",
      "carefully\n",
      "determined\n",
      "sequence\n",
      "of\n",
      "fixation\n",
      "points\n",
      "to\n",
      "ensure\n",
      "that\n",
      "only\n",
      "a\n",
      "tiny\n",
      "fraction\n",
      "of\n",
      "the\n",
      "optic\n",
      "array\n",
      "is\n",
      "ever\n",
      "processed\n",
      "at\n",
      "the\n",
      "highest\n",
      "resolution.\n",
      "introspection\n",
      "is\n",
      "a\n",
      "poor\n",
      "guide\n",
      "to\n",
      "understanding\n",
      "how\n",
      "much\n",
      "of\n",
      "our\n",
      "knowledge\n",
      "of\n",
      "a\n",
      "scene\n",
      "comes\n",
      "from\n",
      "the\n",
      "sequence\n",
      "of\n",
      "fixations\n",
      "and\n",
      "how\n",
      "much\n",
      "we\n",
      "glean\n",
      "from\n",
      "a\n",
      "single\n",
      "fixation,\n",
      "but\n",
      "in\n",
      "this\n",
      "paper\n",
      "we\n",
      "will\n",
      "assume\n",
      "that\n",
      "a\n",
      "single\n",
      "fixation\n",
      "gives\n",
      "us\n",
      "much\n",
      "more\n",
      "than\n",
      "just\n",
      "a\n",
      "single\n",
      "identified\n",
      "object\n",
      "and\n",
      "its\n",
      "properties.\n",
      "we\n",
      "will\n",
      "assume\n",
      "that\n",
      "our\n",
      "multi-layer\n",
      "visual\n",
      "system\n",
      "creates\n",
      "something\n",
      "like\n",
      "a\n",
      "parse\n",
      "tree\n",
      "on\n",
      "each\n",
      "fixation,\n",
      "and\n",
      "we\n",
      "will\n",
      "ignore\n",
      "the\n",
      "issue\n",
      "of\n",
      "how\n",
      "these\n",
      "single-fixation\n",
      "parse\n",
      "trees\n",
      "are\n",
      "coordinated\n",
      "over\n",
      "multiple\n",
      "fixations.\n",
      "parse\n",
      "trees\n",
      "are\n",
      "generally\n",
      "constructed\n",
      "on\n",
      "the\n",
      "fly\n",
      "by\n",
      "dynamically\n",
      "allocating\n",
      "memory,\n",
      "but,\n",
      "following\n",
      "hinton\n",
      "et\n",
      "al.\n",
      "[2000a],\n",
      "we\n",
      "shall\n",
      "assume\n",
      "that,\n",
      "for\n",
      "a\n",
      "single\n",
      "fixation,\n",
      "a\n",
      "parse\n",
      "tree\n",
      "is\n",
      "carved\n",
      "out\n",
      "of\n",
      "a\n",
      "fixed\n",
      "multilayer\n",
      "neural\n",
      "network\n",
      "like\n",
      "a\n",
      "sculpture\n",
      "is\n",
      "carved\n",
      "from\n",
      "a\n",
      "rock.\n",
      "each\n",
      "layer\n",
      "will\n",
      "be\n",
      "divided\n",
      "into\n",
      "many\n",
      "small\n",
      "groups\n",
      "of\n",
      "neurons\n",
      "called\n",
      "“capsules”\n",
      "(hinton\n",
      "et\n",
      "al.\n",
      "[2011])\n",
      "and\n",
      "each\n",
      "node\n",
      "in\n",
      "the\n",
      "parse\n",
      "tree\n",
      "will\n",
      "correspond\n",
      "to\n",
      "an\n",
      "active\n",
      "capsule.\n",
      "using\n",
      "an\n",
      "iterative\n",
      "routing\n",
      "process,\n",
      "each\n",
      "active\n",
      "capsule\n",
      "will\n",
      "choose\n",
      "a\n",
      "capsule\n",
      "in\n",
      "the\n",
      "layer\n",
      "above\n",
      "to\n",
      "be\n",
      "its\n",
      "parent\n",
      "in\n",
      "the\n",
      "tree.\n",
      "for\n",
      "the\n",
      "higher\n",
      "levels\n",
      "of\n",
      "a\n",
      "visual\n",
      "system,\n",
      "this\n",
      "iterative\n",
      "process\n",
      "will\n",
      "be\n",
      "solving\n",
      "the\n",
      "problem\n",
      "of\n",
      "assigning\n",
      "parts\n",
      "to\n",
      "wholes.\n",
      "the\n",
      "activities\n",
      "of\n",
      "the\n",
      "neurons\n",
      "within\n",
      "an\n",
      "active\n",
      "capsule\n",
      "represent\n",
      "the\n",
      "various\n",
      "properties\n",
      "of\n",
      "a\n",
      "particular\n",
      "entity\n",
      "that\n",
      "is\n",
      "present\n",
      "in\n",
      "the\n",
      "image.\n",
      "these\n",
      "properties\n",
      "can\n",
      "include\n",
      "many\n",
      "different\n",
      "types\n",
      "of\n",
      "instantiation\n",
      "parameter\n",
      "such\n",
      "as\n",
      "pose\n",
      "(position,\n",
      "size,\n",
      "orientation),\n",
      "deformation,\n",
      "velocity,\n",
      "albedo,\n",
      "hue,\n",
      "texture\n",
      "etc.\n",
      "one\n",
      "very\n",
      "special\n",
      "property\n",
      "is\n",
      "the\n",
      "existence\n",
      "of\n",
      "the\n",
      "instantiated\n",
      "entity\n",
      "in\n",
      "the\n",
      "image.\n",
      "an\n",
      "obvious\n",
      "way\n",
      "to\n",
      "represent\n",
      "existence\n",
      "is\n",
      "by\n",
      "using\n",
      "a\n",
      "separate\n",
      "logistic\n",
      "unit\n",
      "whose\n",
      "output\n",
      "is\n",
      "the\n",
      "probability\n",
      "that\n",
      "the\n",
      "entity\n",
      "exists.\n",
      "in\n",
      "this\n",
      "paper\n",
      "we\n",
      "explore\n",
      "an\n",
      "interesting\n",
      "alternative\n",
      "which\n",
      "is\n",
      "to\n",
      "use\n",
      "the\n",
      "overall\n",
      "length\n",
      "of\n",
      "the\n",
      "vector\n",
      "of\n",
      "instantiation\n",
      "parameters\n",
      "to\n",
      "represent\n",
      "the\n",
      "existence\n",
      "of\n",
      "the\n",
      "entity\n",
      "and\n",
      "to\n",
      "force\n",
      "the\n",
      "orientation\n",
      "31st\n",
      "conference\n",
      "on\n",
      "neural\n",
      "information\n",
      "processing\n",
      "systems\n",
      "(nips\n",
      "2017),\n",
      "long\n",
      "beach,\n",
      "ca,\n",
      "usa.\n",
      "arxiv:1710.09829v1\n",
      "[cs.cv]\n",
      "26\n",
      "oct\n",
      "2017\n",
      "of\n",
      "the\n",
      "vector\n",
      "to\n",
      "represent\n",
      "the\n",
      "properties\n",
      "of\n",
      "the\n",
      "entity1.\n",
      "we\n",
      "ensure\n",
      "that\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "vector\n",
      "output\n",
      "of\n",
      "a\n",
      "capsule\n",
      "cannot\n",
      "exceed\n",
      "1\n",
      "by\n",
      "applying\n",
      "a\n",
      "non-linearity\n",
      "that\n",
      "leaves\n",
      "the\n",
      "orientation\n",
      "of\n",
      "the\n",
      "vector\n",
      "unchanged\n",
      "but\n",
      "scales\n",
      "down\n",
      "its\n",
      "magnitude.\n",
      "the\n",
      "fact\n",
      "that\n",
      "the\n",
      "output\n",
      "of\n",
      "a\n",
      "capsule\n",
      "is\n",
      "a\n",
      "vector\n",
      "makes\n",
      "it\n",
      "possible\n",
      "to\n",
      "use\n",
      "a\n",
      "powerful\n",
      "dynamic\n",
      "routing\n",
      "mechanism\n",
      "to\n",
      "ensure\n",
      "that\n",
      "the\n",
      "output\n",
      "of\n",
      "the\n",
      "capsule\n",
      "gets\n",
      "sent\n",
      "to\n",
      "an\n",
      "appropriate\n",
      "parent\n",
      "in\n",
      "the\n",
      "layer\n",
      "above.\n",
      "initially,\n",
      "the\n",
      "output\n",
      "is\n",
      "routed\n",
      "to\n",
      "all\n",
      "possible\n",
      "parents\n",
      "but\n",
      "is\n",
      "scaled\n",
      "down\n",
      "by\n",
      "coupling\n",
      "coefficients\n",
      "that\n",
      "sum\n",
      "to\n",
      "1.\n",
      "for\n",
      "each\n",
      "possible\n",
      "parent,\n",
      "the\n",
      "capsule\n",
      "computes\n",
      "a\n",
      "“prediction\n",
      "vector”\n",
      "by\n",
      "multiplying\n",
      "its\n",
      "own\n",
      "output\n",
      "by\n",
      "a\n",
      "weight\n",
      "matrix.\n",
      "if\n",
      "this\n",
      "prediction\n",
      "vector\n",
      "has\n",
      "a\n",
      "large\n",
      "scalar\n",
      "product\n",
      "with\n",
      "the\n",
      "output\n",
      "of\n",
      "a\n",
      "possible\n",
      "parent,\n",
      "there\n",
      "is\n",
      "top-down\n",
      "feedback\n",
      "which\n",
      "has\n",
      "the\n",
      "effect\n",
      "of\n",
      "increasing\n",
      "the\n",
      "coupling\n",
      "coefficient\n",
      "for\n",
      "that\n",
      "parent\n",
      "and\n",
      "decreasing\n",
      "it\n",
      "for\n",
      "other\n",
      "parents.\n",
      "this\n",
      "increases\n",
      "the\n",
      "contribution\n",
      "that\n",
      "the\n",
      "capsule\n",
      "makes\n",
      "to\n",
      "that\n",
      "parent\n",
      "thus\n",
      "further\n",
      "increasing\n",
      "the\n",
      "scalar\n",
      "product\n",
      "of\n",
      "the\n",
      "capsule’s\n",
      "prediction\n",
      "with\n",
      "the\n",
      "parent’s\n",
      "output.\n",
      "this\n",
      "type\n",
      "of\n",
      "“routing-by-agreement”\n",
      "should\n",
      "be\n",
      "far\n",
      "more\n",
      "effective\n",
      "than\n",
      "the\n",
      "very\n",
      "primitive\n",
      "form\n",
      "of\n",
      "routing\n",
      "implemented\n",
      "by\n",
      "max-pooling\n",
      "which\n",
      "allows\n",
      "neurons\n",
      "in\n",
      "one\n",
      "layer\n",
      "to\n",
      "ignore\n",
      "all\n",
      "but\n",
      "the\n",
      "most\n",
      "active\n",
      "feature\n",
      "detector\n",
      "in\n",
      "a\n",
      "local\n",
      "pool\n",
      "in\n",
      "the\n",
      "layer\n",
      "below.\n",
      "we\n",
      "demonstrate\n",
      "that\n",
      "our\n",
      "dynamic\n",
      "routing\n",
      "mechanism\n",
      "is\n",
      "an\n",
      "effective\n",
      "way\n",
      "to\n",
      "implement\n",
      "the\n",
      "“explaining\n",
      "away”\n",
      "that\n",
      "is\n",
      "needed\n",
      "for\n",
      "segmenting\n",
      "highly\n",
      "overlapping\n",
      "objects.\n",
      "convolutional\n",
      "neural\n",
      "networks\n",
      "(cnns)\n",
      "use\n",
      "translated\n",
      "replicas\n",
      "of\n",
      "learned\n",
      "feature\n",
      "detectors\n",
      "and\n",
      "this\n",
      "allows\n",
      "them\n",
      "to\n",
      "translate\n",
      "knowledge\n",
      "about\n",
      "good\n",
      "weight\n",
      "values\n",
      "acquired\n",
      "at\n",
      "one\n",
      "position\n",
      "in\n",
      "an\n",
      "image\n",
      "to\n",
      "other\n",
      "positions.\n",
      "this\n",
      "has\n",
      "proven\n",
      "extremely\n",
      "helpful\n",
      "in\n",
      "image\n",
      "interpretation.\n",
      "even\n",
      "though\n",
      "we\n",
      "are\n",
      "replacing\n",
      "the\n",
      "scalar-output\n",
      "feature\n",
      "detectors\n",
      "of\n",
      "cnns\n",
      "with\n",
      "vector-output\n",
      "capsules\n",
      "and\n",
      "max-pooling\n",
      "with\n",
      "routing-by-agreement,\n",
      "we\n",
      "would\n",
      "still\n",
      "like\n",
      "to\n",
      "replicate\n",
      "learned\n",
      "knowledge\n",
      "across\n",
      "space,\n",
      "so\n",
      "we\n",
      "make\n",
      "all\n",
      "but\n",
      "the\n",
      "last\n",
      "layer\n",
      "of\n",
      "capsules\n",
      "be\n",
      "convolutional.\n",
      "as\n",
      "with\n",
      "cnns,\n",
      "we\n",
      "make\n",
      "higher-level\n",
      "capsules\n",
      "cover\n",
      "larger\n",
      "regions\n",
      "of\n",
      "the\n",
      "image,\n",
      "but\n",
      "unlike\n",
      "max-pooling\n",
      "we\n",
      "do\n",
      "not\n",
      "throw\n",
      "away\n",
      "information\n",
      "about\n",
      "the\n",
      "precise\n",
      "position\n",
      "of\n",
      "the\n",
      "entity\n",
      "within\n",
      "the\n",
      "region.\n",
      "for\n",
      "low\n",
      "level\n",
      "capsules,\n",
      "location\n",
      "information\n",
      "is\n",
      "“place-coded”\n",
      "by\n",
      "which\n",
      "capsule\n",
      "is\n",
      "active.\n",
      "as\n",
      "we\n",
      "ascend\n",
      "the\n",
      "hierarchy\n",
      "more\n",
      "and\n",
      "more\n",
      "of\n",
      "the\n",
      "positional\n",
      "information\n",
      "is\n",
      "“rate-coded”\n",
      "in\n",
      "the\n",
      "real-valued\n",
      "components\n",
      "of\n",
      "the\n",
      "output\n",
      "vector\n",
      "of\n",
      "a\n",
      "capsule.\n",
      "this\n",
      "shift\n",
      "from\n",
      "place-coding\n",
      "to\n",
      "rate-coding\n",
      "combined\n",
      "with\n",
      "the\n",
      "fact\n",
      "that\n",
      "higher-level\n",
      "capsules\n",
      "represent\n",
      "more\n",
      "complex\n",
      "entities\n",
      "with\n",
      "more\n",
      "degrees\n",
      "of\n",
      "freedom\n",
      "suggests\n",
      "that\n",
      "the\n",
      "dimensionality\n",
      "of\n",
      "capsules\n",
      "should\n",
      "increase\n",
      "as\n",
      "we\n",
      "ascend\n",
      "the\n",
      "hierarchy.\n",
      "2\n",
      "how\n",
      "the\n",
      "vector\n",
      "inputs\n",
      "and\n",
      "outputs\n",
      "of\n",
      "a\n",
      "capsule\n",
      "are\n",
      "computed\n",
      "there\n",
      "are\n",
      "many\n",
      "possible\n",
      "ways\n",
      "to\n",
      "implement\n",
      "the\n",
      "general\n",
      "idea\n",
      "of\n",
      "capsules.\n",
      "the\n",
      "aim\n",
      "of\n",
      "this\n",
      "paper\n",
      "is\n",
      "not\n",
      "to\n",
      "explore\n",
      "this\n",
      "whole\n",
      "space\n",
      "but\n",
      "to\n",
      "simply\n",
      "show\n",
      "that\n",
      "one\n",
      "fairly\n",
      "straightforward\n",
      "implementation\n",
      "works\n",
      "well\n",
      "and\n",
      "that\n",
      "dynamic\n",
      "routing\n",
      "helps.\n",
      "we\n",
      "want\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "output\n",
      "vector\n",
      "of\n",
      "a\n",
      "capsule\n",
      "to\n",
      "represent\n",
      "the\n",
      "probability\n",
      "that\n",
      "the\n",
      "entity\n",
      "represented\n",
      "by\n",
      "the\n",
      "capsule\n",
      "is\n",
      "present\n",
      "in\n",
      "the\n",
      "current\n",
      "input.\n",
      "we\n",
      "therefore\n",
      "use\n",
      "a\n",
      "non-linear\n",
      "\"squashing\"\n",
      "function\n",
      "to\n",
      "ensure\n",
      "that\n",
      "short\n",
      "vectors\n",
      "get\n",
      "shrunk\n",
      "to\n",
      "almost\n",
      "zero\n",
      "length\n",
      "and\n",
      "long\n",
      "vectors\n",
      "get\n",
      "shrunk\n",
      "to\n",
      "a\n",
      "length\n",
      "slightly\n",
      "below\n",
      "1.\n",
      "we\n",
      "leave\n",
      "it\n",
      "to\n",
      "discriminative\n",
      "learning\n",
      "to\n",
      "make\n",
      "good\n",
      "use\n",
      "of\n",
      "this\n",
      "non-linearity.\n",
      "vj\n",
      "=\n",
      "jjsj\n",
      "jj2\n",
      "1\n",
      "+\n",
      "jjsj\n",
      "jj2\n",
      "sj\n",
      "jjsj\n",
      "jj\n",
      "(1)\n",
      "where\n",
      "vj\n",
      "is\n",
      "the\n",
      "vector\n",
      "output\n",
      "of\n",
      "capsule\n",
      "j\n",
      "and\n",
      "sj\n",
      "is\n",
      "its\n",
      "total\n",
      "input.\n",
      "for\n",
      "all\n",
      "but\n",
      "the\n",
      "first\n",
      "layer\n",
      "of\n",
      "capsules,\n",
      "the\n",
      "total\n",
      "input\n",
      "to\n",
      "a\n",
      "capsule\n",
      "sj\n",
      "is\n",
      "a\n",
      "weighted\n",
      "sum\n",
      "over\n",
      "all\n",
      "“prediction\n",
      "vectors”\n",
      "^ujji\n",
      "from\n",
      "the\n",
      "capsules\n",
      "in\n",
      "the\n",
      "layer\n",
      "below\n",
      "and\n",
      "is\n",
      "produced\n",
      "by\n",
      "multiplying\n",
      "the\n",
      "output\n",
      "ui\n",
      "of\n",
      "a\n",
      "capsule\n",
      "in\n",
      "the\n",
      "layer\n",
      "below\n",
      "by\n",
      "a\n",
      "weight\n",
      "matrixwij\n",
      "sj\n",
      "=\n",
      "x\n",
      "i\n",
      "cij^ujji\n",
      ";\n",
      "^ujji\n",
      "=wijui\n",
      "(2)\n",
      "where\n",
      "the\n",
      "cij\n",
      "are\n",
      "coupling\n",
      "coefficients\n",
      "that\n",
      "are\n",
      "determined\n",
      "by\n",
      "the\n",
      "iterative\n",
      "dynamic\n",
      "routing\n",
      "process.\n",
      "the\n",
      "coupling\n",
      "coefficients\n",
      "between\n",
      "capsule\n",
      "i\n",
      "and\n",
      "all\n",
      "the\n",
      "capsules\n",
      "in\n",
      "the\n",
      "layer\n",
      "above\n",
      "sum\n",
      "to\n",
      "1\n",
      "and\n",
      "are\n",
      "determined\n",
      "by\n",
      "a\n",
      "“routing\n",
      "softmax”\n",
      "whose\n",
      "initial\n",
      "logits\n",
      "bij\n",
      "are\n",
      "the\n",
      "log\n",
      "prior\n",
      "probabilities\n",
      "that\n",
      "capsule\n",
      "i\n",
      "1this\n",
      "makes\n",
      "biological\n",
      "sense\n",
      "because\n",
      "it\n",
      "does\n",
      "not\n",
      "use\n",
      "large\n",
      "activities\n",
      "to\n",
      "get\n",
      "accurate\n",
      "representations\n",
      "of\n",
      "things\n",
      "that\n",
      "probably\n",
      "don’t\n",
      "exist.\n",
      "2\n",
      "should\n",
      "be\n",
      "coupled\n",
      "to\n",
      "capsule\n",
      "j.\n",
      "cij\n",
      "=\n",
      "pexp(bij)\n",
      "k\n",
      "exp(bik)\n",
      "(3)\n",
      "the\n",
      "log\n",
      "priors\n",
      "can\n",
      "be\n",
      "learned\n",
      "discriminatively\n",
      "at\n",
      "the\n",
      "same\n",
      "time\n",
      "as\n",
      "all\n",
      "the\n",
      "other\n",
      "weights.\n",
      "they\n",
      "depend\n",
      "on\n",
      "the\n",
      "location\n",
      "and\n",
      "type\n",
      "of\n",
      "the\n",
      "two\n",
      "capsules\n",
      "but\n",
      "not\n",
      "on\n",
      "the\n",
      "current\n",
      "input\n",
      "image2.\n",
      "the\n",
      "initial\n",
      "coupling\n",
      "coefficients\n",
      "are\n",
      "then\n",
      "iteratively\n",
      "refined\n",
      "by\n",
      "measuring\n",
      "the\n",
      "agreement\n",
      "between\n",
      "the\n",
      "current\n",
      "output\n",
      "vj\n",
      "of\n",
      "each\n",
      "capsule,\n",
      "j,\n",
      "in\n",
      "the\n",
      "layer\n",
      "above\n",
      "and\n",
      "the\n",
      "prediction\n",
      "^ujji\n",
      "made\n",
      "by\n",
      "capsule\n",
      "i.\n",
      "the\n",
      "agreement\n",
      "is\n",
      "simply\n",
      "the\n",
      "scalar\n",
      "product\n",
      "aij\n",
      "=\n",
      "vj\n",
      ":^ujji.\n",
      "this\n",
      "agreement\n",
      "is\n",
      "treated\n",
      "as\n",
      "if\n",
      "it\n",
      "were\n",
      "a\n",
      "log\n",
      "likelihood\n",
      "and\n",
      "is\n",
      "added\n",
      "to\n",
      "the\n",
      "initial\n",
      "logit,\n",
      "bij\n",
      "before\n",
      "computing\n",
      "the\n",
      "new\n",
      "values\n",
      "for\n",
      "all\n",
      "the\n",
      "coupling\n",
      "coefficients\n",
      "linking\n",
      "capsule\n",
      "i\n",
      "to\n",
      "higher\n",
      "level\n",
      "capsules.\n",
      "in\n",
      "convolutional\n",
      "capsule\n",
      "layers\n",
      "each\n",
      "unit\n",
      "in\n",
      "a\n",
      "capsule\n",
      "is\n",
      "a\n",
      "convolutional\n",
      "unit.\n",
      "therefore,\n",
      "each\n",
      "capsule\n",
      "will\n",
      "output\n",
      "a\n",
      "grid\n",
      "of\n",
      "vectors\n",
      "rather\n",
      "than\n",
      "a\n",
      "single\n",
      "vector\n",
      "output.\n",
      "procedure\n",
      "1\n",
      "routing\n",
      "algorithm.\n",
      "1:\n",
      "procedure\n",
      "routing(^ujji,\n",
      "r,\n",
      "l)\n",
      "2:\n",
      "for\n",
      "all\n",
      "capsule\n",
      "i\n",
      "in\n",
      "layer\n",
      "l\n",
      "and\n",
      "capsule\n",
      "j\n",
      "in\n",
      "layer\n",
      "(l\n",
      "+\n",
      "1):\n",
      "bij\n",
      "0.\n",
      "3:\n",
      "for\n",
      "r\n",
      "iterations\n",
      "do\n",
      "4:\n",
      "for\n",
      "all\n",
      "capsule\n",
      "i\n",
      "in\n",
      "layer\n",
      "l:\n",
      "ci\n",
      "softmax(bi)\n",
      ".\n",
      "softmax\n",
      "computes\n",
      "eq.\n",
      "3\n",
      "5:\n",
      "for\n",
      "all\n",
      "capsule\n",
      "j\n",
      "in\n",
      "layer\n",
      "(l\n",
      "+\n",
      "1):\n",
      "sj\n",
      "p\n",
      "i\n",
      "cij^ujji\n",
      "6:\n",
      "for\n",
      "all\n",
      "capsule\n",
      "j\n",
      "in\n",
      "layer\n",
      "(l\n",
      "+\n",
      "1):\n",
      "vj\n",
      "squash(sj)\n",
      ".\n",
      "squash\n",
      "computes\n",
      "eq.\n",
      "1\n",
      "7:\n",
      "for\n",
      "all\n",
      "capsule\n",
      "i\n",
      "in\n",
      "layer\n",
      "l\n",
      "and\n",
      "capsule\n",
      "j\n",
      "in\n",
      "layer\n",
      "(l\n",
      "+\n",
      "1):\n",
      "bij\n",
      "bij\n",
      "+\n",
      "^ujji:vj\n",
      "return\n",
      "vj\n",
      "3\n",
      "margin\n",
      "loss\n",
      "for\n",
      "digit\n",
      "existence\n",
      "we\n",
      "are\n",
      "using\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "instantiation\n",
      "vector\n",
      "to\n",
      "represent\n",
      "the\n",
      "probability\n",
      "that\n",
      "a\n",
      "capsule’s\n",
      "entity\n",
      "exists,\n",
      "so\n",
      "we\n",
      "would\n",
      "like\n",
      "the\n",
      "top-level\n",
      "capsule\n",
      "for\n",
      "digit\n",
      "class\n",
      "k\n",
      "to\n",
      "have\n",
      "a\n",
      "long\n",
      "instantiation\n",
      "vector\n",
      "if\n",
      "and\n",
      "only\n",
      "if\n",
      "that\n",
      "digit\n",
      "is\n",
      "present\n",
      "in\n",
      "the\n",
      "image.\n",
      "to\n",
      "allow\n",
      "for\n",
      "multiple\n",
      "digits,\n",
      "we\n",
      "use\n",
      "a\n",
      "separate\n",
      "margin\n",
      "loss,\n",
      "lk\n",
      "for\n",
      "each\n",
      "digit\n",
      "capsule,\n",
      "k:\n",
      "lc\n",
      "=\n",
      "tc\n",
      "max(0;m+\n",
      "􀀀\n",
      "jjvcjj)2\n",
      "+\n",
      "\u0015\n",
      "(1\n",
      "􀀀\n",
      "tc)\n",
      "max(0;\n",
      "jjvcjj\n",
      "􀀀\n",
      "m􀀀)2\n",
      "(4)\n",
      "where\n",
      "tc\n",
      "=\n",
      "1\n",
      "iff\n",
      "a\n",
      "digit\n",
      "of\n",
      "class\n",
      "c\n",
      "is\n",
      "present3\n",
      "and\n",
      "m+\n",
      "=\n",
      "0:9\n",
      "and\n",
      "m􀀀\n",
      "=\n",
      "0:1.\n",
      "the\n",
      "\u0015\n",
      "down-weighting\n",
      "of\n",
      "the\n",
      "loss\n",
      "for\n",
      "absent\n",
      "digit\n",
      "classes\n",
      "stops\n",
      "the\n",
      "initial\n",
      "learning\n",
      "from\n",
      "shrinking\n",
      "the\n",
      "lengths\n",
      "of\n",
      "the\n",
      "activity\n",
      "vectors\n",
      "of\n",
      "all\n",
      "the\n",
      "digit\n",
      "capsules.\n",
      "we\n",
      "suggest\n",
      "\u0015\n",
      "=\n",
      "0:5.\n",
      "the\n",
      "total\n",
      "loss\n",
      "is\n",
      "simply\n",
      "the\n",
      "sum\n",
      "of\n",
      "the\n",
      "losses\n",
      "of\n",
      "all\n",
      "digit\n",
      "capsules.\n",
      "4\n",
      "capsnet\n",
      "architecture\n",
      "a\n",
      "simple\n",
      "capsnet\n",
      "architecture\n",
      "is\n",
      "shown\n",
      "in\n",
      "fig.\n",
      "1.\n",
      "the\n",
      "architecture\n",
      "is\n",
      "shallow\n",
      "with\n",
      "only\n",
      "two\n",
      "convolutional\n",
      "layers\n",
      "and\n",
      "one\n",
      "fully\n",
      "connected\n",
      "layer.\n",
      "conv1\n",
      "has\n",
      "256,\n",
      "9\n",
      "\u0002\n",
      "9\n",
      "convolution\n",
      "kernels\n",
      "with\n",
      "a\n",
      "stride\n",
      "of\n",
      "1\n",
      "and\n",
      "relu\n",
      "activation.\n",
      "this\n",
      "layer\n",
      "converts\n",
      "pixel\n",
      "intensities\n",
      "to\n",
      "the\n",
      "activities\n",
      "of\n",
      "local\n",
      "feature\n",
      "detectors\n",
      "that\n",
      "are\n",
      "then\n",
      "used\n",
      "as\n",
      "inputs\n",
      "to\n",
      "the\n",
      "primary\n",
      "capsules.\n",
      "the\n",
      "primary\n",
      "capsules\n",
      "are\n",
      "the\n",
      "lowest\n",
      "level\n",
      "of\n",
      "multi-dimensional\n",
      "entities\n",
      "and,\n",
      "from\n",
      "an\n",
      "inverse\n",
      "graphics\n",
      "perspective,\n",
      "activating\n",
      "the\n",
      "primary\n",
      "capsules\n",
      "corresponds\n",
      "to\n",
      "inverting\n",
      "the\n",
      "rendering\n",
      "process.\n",
      "this\n",
      "is\n",
      "a\n",
      "very\n",
      "different\n",
      "type\n",
      "of\n",
      "computation\n",
      "than\n",
      "piecing\n",
      "instantiated\n",
      "parts\n",
      "together\n",
      "to\n",
      "make\n",
      "familiar\n",
      "wholes,\n",
      "which\n",
      "is\n",
      "what\n",
      "capsules\n",
      "are\n",
      "designed\n",
      "to\n",
      "be\n",
      "good\n",
      "at.\n",
      "the\n",
      "second\n",
      "layer\n",
      "(primarycapsules)\n",
      "is\n",
      "a\n",
      "convolutional\n",
      "capsule\n",
      "layer\n",
      "with\n",
      "32\n",
      "channels\n",
      "of\n",
      "convolutional\n",
      "8d\n",
      "capsules\n",
      "(i.e.\n",
      "each\n",
      "primary\n",
      "capsule\n",
      "contains\n",
      "8\n",
      "convolutional\n",
      "units\n",
      "with\n",
      "a\n",
      "9\n",
      "\u0002\n",
      "9\n",
      "kernel\n",
      "and\n",
      "a\n",
      "stride\n",
      "of\n",
      "2).\n",
      "each\n",
      "primary\n",
      "capsule\n",
      "output\n",
      "sees\n",
      "the\n",
      "outputs\n",
      "of\n",
      "all\n",
      "256\u000281\n",
      "conv1\n",
      "units\n",
      "whose\n",
      "receptive\n",
      "fields\n",
      "overlap\n",
      "with\n",
      "the\n",
      "location\n",
      "of\n",
      "the\n",
      "center\n",
      "of\n",
      "the\n",
      "capsule.\n",
      "in\n",
      "total\n",
      "primarycapsules\n",
      "has\n",
      "[32;\n",
      "6;\n",
      "6]\n",
      "capsule\n",
      "2for\n",
      "mnist\n",
      "we\n",
      "found\n",
      "that\n",
      "it\n",
      "was\n",
      "sufficient\n",
      "to\n",
      "set\n",
      "all\n",
      "of\n",
      "these\n",
      "priors\n",
      "to\n",
      "be\n",
      "equal.\n",
      "3we\n",
      "do\n",
      "not\n",
      "allow\n",
      "an\n",
      "image\n",
      "to\n",
      "contain\n",
      "two\n",
      "instances\n",
      "of\n",
      "the\n",
      "same\n",
      "digit\n",
      "class.\n",
      "we\n",
      "address\n",
      "this\n",
      "weakness\n",
      "of\n",
      "capsules\n",
      "in\n",
      "the\n",
      "discussion\n",
      "section.\n",
      "3\n",
      "figure\n",
      "1:\n",
      "a\n",
      "simple\n",
      "capsnet\n",
      "with\n",
      "3\n",
      "layers.\n",
      "this\n",
      "model\n",
      "gives\n",
      "comparable\n",
      "results\n",
      "to\n",
      "deep\n",
      "convolutional\n",
      "networks\n",
      "(such\n",
      "as\n",
      "chang\n",
      "and\n",
      "chen\n",
      "[2015]).\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "activity\n",
      "vector\n",
      "of\n",
      "each\n",
      "capsule\n",
      "in\n",
      "digitcaps\n",
      "layer\n",
      "indicates\n",
      "presence\n",
      "of\n",
      "an\n",
      "instance\n",
      "of\n",
      "each\n",
      "class\n",
      "and\n",
      "is\n",
      "used\n",
      "to\n",
      "calculate\n",
      "the\n",
      "classification\n",
      "loss.\n",
      "wij\n",
      "is\n",
      "a\n",
      "weight\n",
      "matrix\n",
      "between\n",
      "each\n",
      "ui;\n",
      "i\n",
      "2\n",
      "(1;\n",
      "32\n",
      "\u0002\n",
      "6\n",
      "\u0002\n",
      "6)\n",
      "in\n",
      "primarycapsules\n",
      "and\n",
      "vj\n",
      ";\n",
      "j\n",
      "2\n",
      "(1;\n",
      "10).\n",
      "figure\n",
      "2:\n",
      "decoder\n",
      "structure\n",
      "to\n",
      "reconstruct\n",
      "a\n",
      "digit\n",
      "from\n",
      "the\n",
      "digitcaps\n",
      "layer\n",
      "representation.\n",
      "the\n",
      "euclidean\n",
      "distance\n",
      "between\n",
      "the\n",
      "image\n",
      "and\n",
      "the\n",
      "output\n",
      "of\n",
      "the\n",
      "sigmoid\n",
      "layer\n",
      "is\n",
      "minimized\n",
      "during\n",
      "training.\n",
      "we\n",
      "use\n",
      "the\n",
      "true\n",
      "label\n",
      "as\n",
      "reconstruction\n",
      "target\n",
      "during\n",
      "training.\n",
      "outputs\n",
      "(each\n",
      "output\n",
      "is\n",
      "an\n",
      "8d\n",
      "vector)\n",
      "and\n",
      "each\n",
      "capsule\n",
      "in\n",
      "the\n",
      "[6;\n",
      "6]\n",
      "grid\n",
      "is\n",
      "sharing\n",
      "their\n",
      "weights\n",
      "with\n",
      "each\n",
      "other.\n",
      "one\n",
      "can\n",
      "see\n",
      "primarycapsules\n",
      "as\n",
      "a\n",
      "convolution\n",
      "layer\n",
      "with\n",
      "eq.\n",
      "1\n",
      "as\n",
      "its\n",
      "block\n",
      "non-linearity.\n",
      "the\n",
      "final\n",
      "layer\n",
      "(digitcaps)\n",
      "has\n",
      "one\n",
      "16d\n",
      "capsule\n",
      "per\n",
      "digit\n",
      "class\n",
      "and\n",
      "each\n",
      "of\n",
      "these\n",
      "capsules\n",
      "receives\n",
      "input\n",
      "from\n",
      "all\n",
      "the\n",
      "capsules\n",
      "in\n",
      "the\n",
      "layer\n",
      "below.\n",
      "we\n",
      "have\n",
      "routing\n",
      "only\n",
      "between\n",
      "two\n",
      "consecutive\n",
      "capsule\n",
      "layers\n",
      "(e.g.\n",
      "primarycapsules\n",
      "and\n",
      "digitcaps).\n",
      "since\n",
      "conv1\n",
      "output\n",
      "is\n",
      "1d\n",
      "there\n",
      "is\n",
      "no\n",
      "orientation\n",
      "in\n",
      "its\n",
      "space\n",
      "to\n",
      "agree\n",
      "on.\n",
      "therefore,\n",
      "no\n",
      "routing\n",
      "is\n",
      "used\n",
      "between\n",
      "conv1\n",
      "and\n",
      "primarycapsules.\n",
      "all\n",
      "the\n",
      "routing\n",
      "logits\n",
      "(bij\n",
      ")\n",
      "are\n",
      "initialized\n",
      "to\n",
      "zero.\n",
      "therefore,\n",
      "initially\n",
      "a\n",
      "capsule\n",
      "output\n",
      "(ui)\n",
      "is\n",
      "sent\n",
      "to\n",
      "all\n",
      "parent\n",
      "capsules\n",
      "(v0:::v10)\n",
      "with\n",
      "equal\n",
      "probability\n",
      "(cij\n",
      ").\n",
      "our\n",
      "implementation\n",
      "is\n",
      "in\n",
      "tensorflow\n",
      "(abadi\n",
      "et\n",
      "al.\n",
      "[2016])\n",
      "and\n",
      "we\n",
      "use\n",
      "the\n",
      "adam\n",
      "optimizer\n",
      "with\n",
      "its\n",
      "tensorflow\n",
      "default\n",
      "parameters,\n",
      "including\n",
      "the\n",
      "exponentially\n",
      "decaying\n",
      "learning\n",
      "rate,\n",
      "to\n",
      "minimize\n",
      "the\n",
      "sum\n",
      "of\n",
      "the\n",
      "margin\n",
      "losses\n",
      "in\n",
      "eq.\n",
      "4.\n",
      "4.1\n",
      "reconstruction\n",
      "as\n",
      "a\n",
      "regularization\n",
      "method\n",
      "we\n",
      "use\n",
      "an\n",
      "additional\n",
      "reconstruction\n",
      "loss\n",
      "to\n",
      "encourage\n",
      "the\n",
      "digit\n",
      "capsules\n",
      "to\n",
      "encode\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "the\n",
      "input\n",
      "digit.\n",
      "during\n",
      "training,\n",
      "we\n",
      "mask\n",
      "out\n",
      "all\n",
      "but\n",
      "the\n",
      "activity\n",
      "vector\n",
      "of\n",
      "the\n",
      "correct\n",
      "digit\n",
      "capsule.\n",
      "then\n",
      "we\n",
      "use\n",
      "this\n",
      "activity\n",
      "vector\n",
      "to\n",
      "reconstruct.\n",
      "the\n",
      "output\n",
      "of\n",
      "the\n",
      "digit\n",
      "capsule\n",
      "is\n",
      "fed\n",
      "into\n",
      "a\n",
      "decoder\n",
      "consisting\n",
      "of\n",
      "3\n",
      "fully\n",
      "connected\n",
      "layers\n",
      "that\n",
      "model\n",
      "the\n",
      "pixel\n",
      "intensities\n",
      "as\n",
      "described\n",
      "in\n",
      "fig.\n",
      "2.\n",
      "we\n",
      "minimize\n",
      "the\n",
      "sum\n",
      "of\n",
      "squared\n",
      "differences\n",
      "between\n",
      "the\n",
      "outputs\n",
      "of\n",
      "the\n",
      "logistic\n",
      "units\n",
      "and\n",
      "the\n",
      "pixel\n",
      "intensities.\n",
      "we\n",
      "scale\n",
      "down\n",
      "this\n",
      "reconstruction\n",
      "loss\n",
      "by\n",
      "0:0005\n",
      "so\n",
      "that\n",
      "it\n",
      "does\n",
      "not\n",
      "dominate\n",
      "the\n",
      "margin\n",
      "loss\n",
      "during\n",
      "training.\n",
      "as\n",
      "illustrated\n",
      "in\n",
      "fig.\n",
      "3\n",
      "the\n",
      "reconstructions\n",
      "from\n",
      "the\n",
      "16d\n",
      "output\n",
      "of\n",
      "the\n",
      "capsnet\n",
      "are\n",
      "robust\n",
      "while\n",
      "keeping\n",
      "only\n",
      "important\n",
      "details.\n",
      "5\n",
      "capsules\n",
      "on\n",
      "mnist\n",
      "training\n",
      "is\n",
      "performed\n",
      "on\n",
      "28\n",
      "\u0002\n",
      "28\n",
      "mnist\n",
      "(lecun\n",
      "et\n",
      "al.\n",
      "[1998])\n",
      "images\n",
      "that\n",
      "have\n",
      "been\n",
      "shifted\n",
      "by\n",
      "up\n",
      "to\n",
      "2\n",
      "pixels\n",
      "in\n",
      "each\n",
      "direction\n",
      "with\n",
      "zero\n",
      "padding.\n",
      "no\n",
      "other\n",
      "data\n",
      "augmentation/deformation\n",
      "is\n",
      "used.\n",
      "the\n",
      "dataset\n",
      "has\n",
      "60k\n",
      "and\n",
      "10k\n",
      "images\n",
      "for\n",
      "training\n",
      "and\n",
      "testing\n",
      "respectively.\n",
      "4\n",
      "figure\n",
      "3:\n",
      "sample\n",
      "mnist\n",
      "test\n",
      "reconstructions\n",
      "of\n",
      "a\n",
      "capsnet\n",
      "with\n",
      "3\n",
      "routing\n",
      "iterations.\n",
      "(l;\n",
      "p;\n",
      "r)\n",
      "represents\n",
      "the\n",
      "label,\n",
      "the\n",
      "prediction\n",
      "and\n",
      "the\n",
      "reconstruction\n",
      "target\n",
      "respectively.\n",
      "the\n",
      "two\n",
      "rightmost\n",
      "columns\n",
      "show\n",
      "two\n",
      "reconstructions\n",
      "of\n",
      "a\n",
      "failure\n",
      "example\n",
      "and\n",
      "it\n",
      "explains\n",
      "how\n",
      "the\n",
      "model\n",
      "confuses\n",
      "a\n",
      "5\n",
      "and\n",
      "a\n",
      "3\n",
      "in\n",
      "this\n",
      "image.\n",
      "the\n",
      "other\n",
      "columns\n",
      "are\n",
      "from\n",
      "correct\n",
      "classifications\n",
      "and\n",
      "shows\n",
      "that\n",
      "model\n",
      "picks\n",
      "on\n",
      "the\n",
      "details\n",
      "while\n",
      "smoothing\n",
      "the\n",
      "noise.\n",
      "(l;\n",
      "p;\n",
      "r)\n",
      "(2;\n",
      "2;\n",
      "2)\n",
      "(5;\n",
      "5;\n",
      "5)\n",
      "(8;\n",
      "8;\n",
      "8)\n",
      "(9;\n",
      "9;\n",
      "9)\n",
      "(5;\n",
      "3;\n",
      "5)\n",
      "(5;\n",
      "3;\n",
      "3)\n",
      "input\n",
      "output\n",
      "table\n",
      "1:\n",
      "capsnet\n",
      "classification\n",
      "test\n",
      "accuracy.\n",
      "the\n",
      "mnist\n",
      "average\n",
      "and\n",
      "standard\n",
      "deviation\n",
      "results\n",
      "are\n",
      "reported\n",
      "from\n",
      "3\n",
      "trials.\n",
      "method\n",
      "routing\n",
      "reconstruction\n",
      "mnist\n",
      "(%)\n",
      "multimnist\n",
      "(%)\n",
      "baseline\n",
      "-\n",
      "-\n",
      "0:39\n",
      "8\n",
      "capsnet\n",
      "1\n",
      "no\n",
      "0:34\u00060:032\n",
      "-\n",
      "capsnet\n",
      "1\n",
      "yes\n",
      "0:29\u00060:011\n",
      "7\n",
      "capsnet\n",
      "3\n",
      "no\n",
      "0:35\u00060:036\n",
      "-\n",
      "capsnet\n",
      "3\n",
      "yes\n",
      "0:25\u00060:005\n",
      "5\n",
      "we\n",
      "test\n",
      "using\n",
      "a\n",
      "single\n",
      "model\n",
      "with\n",
      "no\n",
      "ensembling\n",
      "(ciregan\n",
      "et\n",
      "al.\n",
      "[2012])\n",
      "or\n",
      "drastic\n",
      "data\n",
      "augmentation\n",
      "(sato\n",
      "et\n",
      "al.\n",
      "[2015])\n",
      "(wan\n",
      "et\n",
      "al.\n",
      "[2013]\n",
      "achieves\n",
      "0:21%\n",
      "test\n",
      "error\n",
      "with\n",
      "ensembling\n",
      "and\n",
      "data\n",
      "augmentation\n",
      "and\n",
      "0:57%\n",
      "without\n",
      "them).\n",
      "we\n",
      "get\n",
      "a\n",
      "low\n",
      "test\n",
      "error\n",
      "(0:25%)\n",
      "on\n",
      "a\n",
      "3\n",
      "layer\n",
      "network\n",
      "previously\n",
      "only\n",
      "achieved\n",
      "by\n",
      "deeper\n",
      "networks.\n",
      "tab.\n",
      "1\n",
      "reports\n",
      "the\n",
      "test\n",
      "error\n",
      "rate\n",
      "on\n",
      "mnist\n",
      "for\n",
      "different\n",
      "capsnet\n",
      "setups\n",
      "and\n",
      "shows\n",
      "the\n",
      "importance\n",
      "of\n",
      "routing\n",
      "and\n",
      "reconstruction\n",
      "regularizer.\n",
      "the\n",
      "baseline\n",
      "is\n",
      "a\n",
      "standard\n",
      "cnn\n",
      "with\n",
      "three\n",
      "convolutional\n",
      "layers\n",
      "of\n",
      "256;\n",
      "256;\n",
      "128\n",
      "channels.\n",
      "each\n",
      "has\n",
      "5x5\n",
      "kernels\n",
      "and\n",
      "stride\n",
      "of\n",
      "1.\n",
      "the\n",
      "last\n",
      "convolutional\n",
      "layers\n",
      "is\n",
      "followed\n",
      "by\n",
      "two\n",
      "fully\n",
      "connected\n",
      "layers\n",
      "of\n",
      "size\n",
      "328;\n",
      "192.\n",
      "the\n",
      "last\n",
      "fully\n",
      "connected\n",
      "layer\n",
      "is\n",
      "connected\n",
      "with\n",
      "dropout\n",
      "to\n",
      "a\n",
      "10\n",
      "class\n",
      "softmax\n",
      "layer\n",
      "with\n",
      "cross\n",
      "entropy\n",
      "loss.\n",
      "5.1\n",
      "what\n",
      "the\n",
      "individual\n",
      "dimensions\n",
      "of\n",
      "a\n",
      "capsule\n",
      "represent\n",
      "since\n",
      "we\n",
      "are\n",
      "passing\n",
      "the\n",
      "encoding\n",
      "of\n",
      "only\n",
      "one\n",
      "digit\n",
      "and\n",
      "zeroing\n",
      "out\n",
      "other\n",
      "digits,\n",
      "the\n",
      "dimensions\n",
      "of\n",
      "a\n",
      "digit\n",
      "capsule\n",
      "should\n",
      "learn\n",
      "to\n",
      "span\n",
      "the\n",
      "space\n",
      "of\n",
      "variations\n",
      "in\n",
      "the\n",
      "way\n",
      "digits\n",
      "of\n",
      "that\n",
      "class\n",
      "are\n",
      "instantiated.\n",
      "these\n",
      "variations\n",
      "include\n",
      "stroke\n",
      "thickness,\n",
      "skew\n",
      "and\n",
      "width.\n",
      "they\n",
      "also\n",
      "include\n",
      "digit-specific\n",
      "variations\n",
      "such\n",
      "as\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "tail\n",
      "of\n",
      "a\n",
      "2.\n",
      "we\n",
      "can\n",
      "see\n",
      "what\n",
      "the\n",
      "individual\n",
      "dimensions\n",
      "represent\n",
      "by\n",
      "making\n",
      "use\n",
      "of\n",
      "the\n",
      "decoder\n",
      "network.\n",
      "after\n",
      "computing\n",
      "the\n",
      "activity\n",
      "vector\n",
      "for\n",
      "the\n",
      "correct\n",
      "digit\n",
      "capsule,\n",
      "we\n",
      "can\n",
      "feed\n",
      "a\n",
      "perturbed\n",
      "version\n",
      "of\n",
      "this\n",
      "activity\n",
      "vector\n",
      "to\n",
      "the\n",
      "decoder\n",
      "network\n",
      "and\n",
      "see\n",
      "how\n",
      "the\n",
      "perturbation\n",
      "affects\n",
      "the\n",
      "reconstruction.\n",
      "examples\n",
      "of\n",
      "these\n",
      "perturbations\n",
      "are\n",
      "shown\n",
      "in\n",
      "fig.\n",
      "4.\n",
      "we\n",
      "found\n",
      "that\n",
      "one\n",
      "dimension\n",
      "(out\n",
      "of\n",
      "16)\n",
      "of\n",
      "the\n",
      "capsule\n",
      "almost\n",
      "always\n",
      "represents\n",
      "the\n",
      "width\n",
      "of\n",
      "the\n",
      "digit.\n",
      "while\n",
      "some\n",
      "dimensions\n",
      "represent\n",
      "combinations\n",
      "of\n",
      "global\n",
      "variations,\n",
      "there\n",
      "are\n",
      "other\n",
      "dimensions\n",
      "that\n",
      "represent\n",
      "variation\n",
      "in\n",
      "a\n",
      "localized\n",
      "part\n",
      "of\n",
      "the\n",
      "digit.\n",
      "for\n",
      "example,\n",
      "different\n",
      "dimensions\n",
      "are\n",
      "used\n",
      "for\n",
      "the\n",
      "length\n",
      "of\n",
      "the\n",
      "ascender\n",
      "of\n",
      "a\n",
      "6\n",
      "and\n",
      "the\n",
      "size\n",
      "of\n",
      "the\n",
      "loop.\n",
      "5.2\n",
      "robustness\n",
      "to\n",
      "affine\n",
      "transformations\n",
      "experiments\n",
      "show\n",
      "that\n",
      "each\n",
      "digitcaps\n",
      "capsule\n",
      "learns\n",
      "a\n",
      "more\n",
      "robust\n",
      "representation\n",
      "for\n",
      "each\n",
      "class\n",
      "than\n",
      "a\n",
      "traditional\n",
      "convolutional\n",
      "network.\n",
      "because\n",
      "there\n",
      "is\n",
      "natural\n",
      "variance\n",
      "in\n",
      "skew,\n",
      "rotation,\n",
      "style,\n",
      "etc\n",
      "in\n",
      "hand\n",
      "written\n",
      "digits,\n",
      "the\n",
      "trained\n",
      "capsnet\n",
      "is\n",
      "moderately\n",
      "robust\n",
      "to\n",
      "small\n",
      "affine\n",
      "transformations\n",
      "of\n",
      "the\n",
      "training\n",
      "data.\n",
      "to\n",
      "test\n",
      "the\n",
      "robustness\n",
      "of\n",
      "capsnet\n",
      "to\n",
      "affine\n",
      "transformations\n",
      "we\n",
      "trained\n",
      "a\n",
      "capsnet\n",
      "and\n",
      "a\n",
      "traditional\n",
      "convolutional\n",
      "network\n",
      "(with\n",
      "maxpooling\n",
      "and\n",
      "dropout)\n",
      "on\n",
      "a\n",
      "padded\n",
      "and\n",
      "translated\n",
      "mnist\n",
      "training\n",
      "5\n",
      "figure\n",
      "4:\n",
      "dimension\n",
      "perturbations.\n",
      "each\n",
      "row\n",
      "shows\n",
      "the\n",
      "reconstruction\n",
      "when\n",
      "one\n",
      "of\n",
      "the\n",
      "16\n",
      "dimensions\n",
      "in\n",
      "the\n",
      "digitcaps\n",
      "representation\n",
      "is\n",
      "tweaked\n",
      "by\n",
      "intervals\n",
      "of\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05\n",
      "in\n",
      "the\n",
      "range\n",
      "[􀀀0:25;\n",
      "0:25].\n",
      "scale\n",
      "and\n",
      "thickness\n",
      "localized\n",
      "part\n",
      "stroke\n",
      "thickness\n",
      "localized\n",
      "skew\n",
      "width\n",
      "and\n",
      "translation\n",
      "localized\n",
      "part\n",
      "set,\n",
      "in\n",
      "which\n",
      "each\n",
      "example\n",
      "is\n",
      "an\n",
      "mnist\n",
      "digit\n",
      "placed\n",
      "randomly\n",
      "on\n",
      "a\n",
      "black\n",
      "background\n",
      "of\n",
      "40\n",
      "\u0002\n",
      "40\n",
      "pixels.\n",
      "we\n",
      "then\n",
      "tested\n",
      "this\n",
      "network\n",
      "on\n",
      "the\n",
      "affnist4\n",
      "data\n",
      "set,\n",
      "in\n",
      "which\n",
      "each\n",
      "example\n",
      "is\n",
      "an\n",
      "mnist\n",
      "digit\n",
      "with\n",
      "a\n",
      "random\n",
      "small\n",
      "affine\n",
      "transformation.\n",
      "our\n",
      "models\n",
      "were\n",
      "never\n",
      "trained\n",
      "with\n",
      "affine\n",
      "transformations\n",
      "other\n",
      "than\n",
      "translation\n",
      "and\n",
      "any\n",
      "natural\n",
      "transformation\n",
      "seen\n",
      "in\n",
      "the\n",
      "standard\n",
      "mnist.\n",
      "an\n",
      "under-trained\n",
      "capsnet\n",
      "with\n",
      "early\n",
      "stopping\n",
      "which\n",
      "achieved\n",
      "99.23%\n",
      "accuracy\n",
      "on\n",
      "the\n",
      "expanded\n",
      "mnist\n",
      "test\n",
      "set\n",
      "achieved\n",
      "79%\n",
      "accuracy\n",
      "on\n",
      "the\n",
      "affnist\n",
      "test\n",
      "set.\n",
      "a\n",
      "traditional\n",
      "convolutional\n",
      "model\n",
      "with\n",
      "a\n",
      "similar\n",
      "number\n",
      "of\n",
      "parameters\n",
      "which\n",
      "achieved\n",
      "similar\n",
      "accuracy\n",
      "(99.22%)\n",
      "on\n",
      "the\n",
      "expanded\n",
      "mnist\n",
      "test\n",
      "set\n",
      "but\n",
      "only\n",
      "achieved\n",
      "66%\n",
      "on\n",
      "the\n",
      "affnist\n",
      "test\n",
      "set.\n",
      "6\n",
      "segmenting\n",
      "highly\n",
      "overlapping\n",
      "digits\n",
      "dynamic\n",
      "routing\n",
      "can\n",
      "be\n",
      "viewed\n",
      "as\n",
      "a\n",
      "parallel\n",
      "attention\n",
      "mechanism\n",
      "that\n",
      "allows\n",
      "each\n",
      "capsule\n",
      "at\n",
      "one\n",
      "level\n",
      "to\n",
      "attend\n",
      "to\n",
      "some\n",
      "active\n",
      "capsules\n",
      "at\n",
      "the\n",
      "level\n",
      "below\n",
      "and\n",
      "to\n",
      "ignore\n",
      "others.\n",
      "this\n",
      "should\n",
      "allow\n",
      "the\n",
      "model\n",
      "to\n",
      "recognize\n",
      "multiple\n",
      "objects\n",
      "in\n",
      "the\n",
      "image\n",
      "even\n",
      "if\n",
      "objects\n",
      "overlap.\n",
      "hinton\n",
      "et\n",
      "al.\n",
      "prupose\n",
      "the\n",
      "task\n",
      "of\n",
      "segmenting\n",
      "and\n",
      "recognizing\n",
      "highly\n",
      "overlapping\n",
      "digits\n",
      "(hinton\n",
      "et\n",
      "al.\n",
      "[2000b]\n",
      "and\n",
      "others\n",
      "have\n",
      "tested\n",
      "their\n",
      "networks\n",
      "in\n",
      "a\n",
      "similar\n",
      "domain\n",
      "(goodfellow\n",
      "et\n",
      "al.\n",
      "[2013],\n",
      "ba\n",
      "et\n",
      "al.\n",
      "[2014],\n",
      "greff\n",
      "et\n",
      "al.\n",
      "[2016]).\n",
      "the\n",
      "routing-by-agreement\n",
      "should\n",
      "make\n",
      "it\n",
      "possible\n",
      "to\n",
      "use\n",
      "a\n",
      "prior\n",
      "about\n",
      "shape\n",
      "of\n",
      "objects\n",
      "to\n",
      "help\n",
      "segmentation\n",
      "and\n",
      "it\n",
      "should\n",
      "obviate\n",
      "the\n",
      "need\n",
      "to\n",
      "make\n",
      "higher-level\n",
      "segmentation\n",
      "decisions\n",
      "in\n",
      "the\n",
      "domain\n",
      "of\n",
      "pixels.\n",
      "6.1\n",
      "multimnist\n",
      "dataset\n",
      "we\n",
      "generate\n",
      "the\n",
      "multimnist\n",
      "training\n",
      "and\n",
      "test\n",
      "dataset\n",
      "by\n",
      "overlaying\n",
      "a\n",
      "digit\n",
      "on\n",
      "top\n",
      "of\n",
      "another\n",
      "digit\n",
      "from\n",
      "the\n",
      "same\n",
      "set\n",
      "(training\n",
      "or\n",
      "test)\n",
      "but\n",
      "different\n",
      "class.\n",
      "each\n",
      "digit\n",
      "is\n",
      "shifted\n",
      "up\n",
      "to\n",
      "4\n",
      "pixels\n",
      "in\n",
      "each\n",
      "direction\n",
      "resulting\n",
      "in\n",
      "a\n",
      "36\u000236\n",
      "image.\n",
      "considering\n",
      "a\n",
      "digit\n",
      "in\n",
      "a\n",
      "28\u000228\n",
      "image\n",
      "is\n",
      "bounded\n",
      "in\n",
      "a\n",
      "20\u000220\n",
      "box,\n",
      "two\n",
      "digits\n",
      "bounding\n",
      "boxes\n",
      "on\n",
      "average\n",
      "have\n",
      "80%\n",
      "overlap.\n",
      "for\n",
      "each\n",
      "digit\n",
      "in\n",
      "the\n",
      "mnist\n",
      "dataset\n",
      "we\n",
      "generate\n",
      "1k\n",
      "multimnist\n",
      "examples.\n",
      "so\n",
      "the\n",
      "training\n",
      "set\n",
      "size\n",
      "is\n",
      "60m\n",
      "and\n",
      "the\n",
      "test\n",
      "set\n",
      "size\n",
      "is\n",
      "10m.\n",
      "6.2\n",
      "multimnist\n",
      "results\n",
      "our\n",
      "3\n",
      "layer\n",
      "capsnet\n",
      "model\n",
      "trained\n",
      "from\n",
      "scratch\n",
      "on\n",
      "multimnist\n",
      "training\n",
      "data\n",
      "achieves\n",
      "higher\n",
      "test\n",
      "classification\n",
      "accuracy\n",
      "than\n",
      "our\n",
      "baseline\n",
      "convolutional\n",
      "model.\n",
      "we\n",
      "are\n",
      "achieving\n",
      "the\n",
      "same\n",
      "classification\n",
      "error\n",
      "rate\n",
      "of\n",
      "5:0%\n",
      "on\n",
      "highly\n",
      "overlapping\n",
      "digit\n",
      "pairs\n",
      "as\n",
      "the\n",
      "sequential\n",
      "attention\n",
      "model\n",
      "of\n",
      "ba\n",
      "et\n",
      "al.\n",
      "[2014]\n",
      "achieves\n",
      "on\n",
      "a\n",
      "much\n",
      "easier\n",
      "task\n",
      "that\n",
      "has\n",
      "far\n",
      "less\n",
      "overlap\n",
      "(80%\n",
      "overlap\n",
      "of\n",
      "the\n",
      "boxes\n",
      "around\n",
      "the\n",
      "two\n",
      "digits\n",
      "in\n",
      "our\n",
      "case\n",
      "vs\n",
      "<\n",
      "4%\n",
      "for\n",
      "ba\n",
      "et\n",
      "al.\n",
      "[2014]).\n",
      "on\n",
      "test\n",
      "images,\n",
      "which\n",
      "are\n",
      "composed\n",
      "of\n",
      "pairs\n",
      "of\n",
      "images\n",
      "from\n",
      "the\n",
      "test\n",
      "set,\n",
      "we\n",
      "treat\n",
      "the\n",
      "two\n",
      "most\n",
      "active\n",
      "digit\n",
      "capsules\n",
      "as\n",
      "the\n",
      "classification\n",
      "produced\n",
      "by\n",
      "the\n",
      "capsules\n",
      "network.\n",
      "during\n",
      "reconstruction\n",
      "we\n",
      "pick\n",
      "one\n",
      "digit\n",
      "at\n",
      "a\n",
      "time\n",
      "and\n",
      "use\n",
      "the\n",
      "activity\n",
      "vector\n",
      "of\n",
      "the\n",
      "chosen\n",
      "digit\n",
      "capsule\n",
      "to\n",
      "reconstruct\n",
      "the\n",
      "image\n",
      "of\n",
      "the\n",
      "chosen\n",
      "digit\n",
      "(we\n",
      "know\n",
      "this\n",
      "4available\n",
      "at\n",
      "http://www.cs.toronto.edu/~tijmen/affnist/.\n",
      "6\n",
      "figure\n",
      "5:\n",
      "sample\n",
      "reconstructions\n",
      "of\n",
      "a\n",
      "capsnet\n",
      "with\n",
      "3\n",
      "routing\n",
      "iterations\n",
      "on\n",
      "multimnist\n",
      "test\n",
      "dataset.\n",
      "the\n",
      "two\n",
      "reconstructed\n",
      "digits\n",
      "are\n",
      "overlayed\n",
      "in\n",
      "green\n",
      "and\n",
      "red\n",
      "as\n",
      "the\n",
      "lower\n",
      "image.\n",
      "the\n",
      "upper\n",
      "image\n",
      "shows\n",
      "the\n",
      "input\n",
      "image.\n",
      "l:(l1;\n",
      "l2)\n",
      "represents\n",
      "the\n",
      "label\n",
      "for\n",
      "the\n",
      "two\n",
      "digits\n",
      "in\n",
      "the\n",
      "image\n",
      "and\n",
      "r:(r1;\n",
      "r2)\n",
      "represents\n",
      "the\n",
      "two\n",
      "digits\n",
      "used\n",
      "for\n",
      "reconstruction.\n",
      "the\n",
      "two\n",
      "right\n",
      "most\n",
      "columns\n",
      "show\n",
      "two\n",
      "examples\n",
      "with\n",
      "wrong\n",
      "classification\n",
      "reconstructed\n",
      "from\n",
      "the\n",
      "label\n",
      "and\n",
      "from\n",
      "the\n",
      "prediction\n",
      "(p).\n",
      "in\n",
      "(2;\n",
      "8)\n",
      "example\n",
      "the\n",
      "model\n",
      "confuses\n",
      "8\n",
      "with\n",
      "a\n",
      "7\n",
      "and\n",
      "in\n",
      "(4;\n",
      "9)\n",
      "it\n",
      "confuses\n",
      "9\n",
      "with\n",
      "0.\n",
      "the\n",
      "other\n",
      "columns\n",
      "have\n",
      "correct\n",
      "classifications\n",
      "and\n",
      "show\n",
      "that\n",
      "model\n",
      "accounts\n",
      "for\n",
      "all\n",
      "the\n",
      "pixels\n",
      "while\n",
      "being\n",
      "able\n",
      "to\n",
      "assign\n",
      "one\n",
      "pixel\n",
      "to\n",
      "two\n",
      "digits\n",
      "in\n",
      "extremely\n",
      "difficult\n",
      "scenarios\n",
      "(column\n",
      "1\n",
      "􀀀\n",
      "4).\n",
      "note\n",
      "that\n",
      "in\n",
      "dataset\n",
      "generation\n",
      "the\n",
      "pixel\n",
      "values\n",
      "are\n",
      "clipped\n",
      "at\n",
      "1.\n",
      "the\n",
      "two\n",
      "columns\n",
      "with\n",
      "the\n",
      "(*)\n",
      "mark\n",
      "show\n",
      "reconstructions\n",
      "from\n",
      "a\n",
      "digit\n",
      "that\n",
      "is\n",
      "neither\n",
      "the\n",
      "label\n",
      "nor\n",
      "the\n",
      "prediction.\n",
      "these\n",
      "columns\n",
      "suggests\n",
      "that\n",
      "the\n",
      "model\n",
      "is\n",
      "not\n",
      "just\n",
      "finding\n",
      "the\n",
      "best\n",
      "fit\n",
      "for\n",
      "all\n",
      "the\n",
      "digits\n",
      "in\n",
      "the\n",
      "image\n",
      "including\n",
      "the\n",
      "ones\n",
      "that\n",
      "do\n",
      "not\n",
      "exist.\n",
      "therefore\n",
      "in\n",
      "case\n",
      "of\n",
      "(5;\n",
      "0)\n",
      "it\n",
      "cannot\n",
      "reconstruct\n",
      "a\n",
      "7\n",
      "because\n",
      "it\n",
      "knows\n",
      "that\n",
      "there\n",
      "is\n",
      "a\n",
      "5\n",
      "and\n",
      "0\n",
      "that\n",
      "fit\n",
      "best\n",
      "and\n",
      "account\n",
      "for\n",
      "all\n",
      "the\n",
      "pixels.\n",
      "also,\n",
      "in\n",
      "case\n",
      "of\n",
      "(8;\n",
      "1)\n",
      "the\n",
      "loop\n",
      "of\n",
      "8\n",
      "has\n",
      "not\n",
      "triggered\n",
      "0\n",
      "because\n",
      "it\n",
      "is\n",
      "already\n",
      "accounted\n",
      "for\n",
      "by\n",
      "8.\n",
      "therefore\n",
      "it\n",
      "will\n",
      "not\n",
      "assign\n",
      "one\n",
      "pixel\n",
      "to\n",
      "two\n",
      "digits\n",
      "if\n",
      "one\n",
      "of\n",
      "them\n",
      "does\n",
      "not\n",
      "have\n",
      "any\n",
      "other\n",
      "support.\n",
      "r:(2;\n",
      "7)\n",
      "r:(6;\n",
      "0)\n",
      "r:(6;\n",
      "8)\n",
      "r:(7;\n",
      "1)\n",
      "*r:(5;\n",
      "7)\n",
      "*r:(2;\n",
      "3)\n",
      "r:(2;\n",
      "8)\n",
      "r:p:(2;\n",
      "7)\n",
      "l:(2;\n",
      "7)\n",
      "l:(6;\n",
      "0)\n",
      "l:(6;\n",
      "8)\n",
      "l:(7;\n",
      "1)\n",
      "l:(5;\n",
      "0)\n",
      "l:(4;\n",
      "3)\n",
      "l:(2;\n",
      "8)\n",
      "l:(2;\n",
      "8)\n",
      "r:(8;\n",
      "7)\n",
      "r:(9;\n",
      "4)\n",
      "r:(9;\n",
      "5)\n",
      "r:(8;\n",
      "4)\n",
      "*r:(0;\n",
      "8)\n",
      "*r:(1;\n",
      "6)\n",
      "r:(4;\n",
      "9)\n",
      "r:p:(4;\n",
      "0)\n",
      "l:(8;\n",
      "7)\n",
      "l:(9;\n",
      "4)\n",
      "l:(9;\n",
      "5)\n",
      "l:(8;\n",
      "4)\n",
      "l:(1;\n",
      "8)\n",
      "l:(7;\n",
      "6)\n",
      "l:(4;\n",
      "9)\n",
      "l:(4;\n",
      "9)\n",
      "image\n",
      "because\n",
      "we\n",
      "used\n",
      "it\n",
      "to\n",
      "generate\n",
      "the\n",
      "composite\n",
      "image).\n",
      "the\n",
      "only\n",
      "difference\n",
      "with\n",
      "our\n",
      "mnist\n",
      "model\n",
      "is\n",
      "that\n",
      "we\n",
      "increased\n",
      "the\n",
      "decay\n",
      "step\n",
      "for\n",
      "the\n",
      "learning\n",
      "rate\n",
      "to\n",
      "be\n",
      "10\u0002\n",
      "larger\n",
      "because\n",
      "the\n",
      "training\n",
      "dataset\n",
      "is\n",
      "larger.\n",
      "the\n",
      "reconstructions\n",
      "illustrated\n",
      "in\n",
      "fig.\n",
      "5\n",
      "show\n",
      "that\n",
      "capsnet\n",
      "is\n",
      "able\n",
      "to\n",
      "segment\n",
      "the\n",
      "image\n",
      "into\n",
      "the\n",
      "two\n",
      "original\n",
      "digits.\n",
      "since\n",
      "this\n",
      "segmentation\n",
      "is\n",
      "not\n",
      "at\n",
      "pixel\n",
      "level\n",
      "we\n",
      "observe\n",
      "that\n",
      "the\n",
      "model\n",
      "is\n",
      "able\n",
      "to\n",
      "deal\n",
      "correctly\n",
      "with\n",
      "the\n",
      "overlaps\n",
      "(a\n",
      "pixel\n",
      "is\n",
      "on\n",
      "in\n",
      "both\n",
      "digits)\n",
      "while\n",
      "accounting\n",
      "for\n",
      "all\n",
      "the\n",
      "pixels.\n",
      "the\n",
      "position\n",
      "and\n",
      "the\n",
      "style\n",
      "of\n",
      "each\n",
      "digit\n",
      "is\n",
      "encoded\n",
      "in\n",
      "digitcaps.\n",
      "the\n",
      "decoder\n",
      "has\n",
      "learned\n",
      "to\n",
      "reconstruct\n",
      "a\n",
      "digit\n",
      "given\n",
      "the\n",
      "encoding.\n",
      "the\n",
      "fact\n",
      "that\n",
      "it\n",
      "is\n",
      "able\n",
      "to\n",
      "reconstruct\n",
      "digits\n",
      "regardless\n",
      "of\n",
      "the\n",
      "overlap\n",
      "shows\n",
      "that\n",
      "each\n",
      "digit\n",
      "capsule\n",
      "can\n",
      "pick\n",
      "up\n",
      "the\n",
      "style\n",
      "and\n",
      "position\n",
      "from\n",
      "the\n",
      "votes\n",
      "it\n",
      "is\n",
      "receiving\n",
      "from\n",
      "primarycapsules\n",
      "layer.\n",
      "tab.\n",
      "1\n",
      "emphasizes\n",
      "the\n",
      "importance\n",
      "of\n",
      "capsules\n",
      "with\n",
      "routing\n",
      "on\n",
      "this\n",
      "task.\n",
      "as\n",
      "a\n",
      "baseline\n",
      "for\n",
      "the\n",
      "classification\n",
      "of\n",
      "capsnet\n",
      "accuracy\n",
      "we\n",
      "trained\n",
      "a\n",
      "convolution\n",
      "network\n",
      "with\n",
      "two\n",
      "convolution\n",
      "layers\n",
      "and\n",
      "two\n",
      "fully\n",
      "connected\n",
      "layers\n",
      "on\n",
      "top\n",
      "of\n",
      "them.\n",
      "the\n",
      "first\n",
      "layer\n",
      "has\n",
      "512\n",
      "convolution\n",
      "kernels\n",
      "of\n",
      "size\n",
      "9\n",
      "\u0002\n",
      "9\n",
      "and\n",
      "stride\n",
      "1.\n",
      "the\n",
      "second\n",
      "layer\n",
      "has\n",
      "256\n",
      "kernels\n",
      "of\n",
      "size\n",
      "5\u00025\n",
      "and\n",
      "stride\n",
      "1.\n",
      "after\n",
      "each\n",
      "convolution\n",
      "layer\n",
      "the\n",
      "model\n",
      "has\n",
      "a\n",
      "pooling\n",
      "layer\n",
      "of\n",
      "size\n",
      "2\n",
      "\u0002\n",
      "2\n",
      "and\n",
      "stride\n",
      "2.\n",
      "the\n",
      "third\n",
      "layer\n",
      "is\n",
      "a\n",
      "1024d\n",
      "fully\n",
      "connected\n",
      "layer.\n",
      "all\n",
      "three\n",
      "layers\n",
      "have\n",
      "relu\n",
      "non-linearities.\n",
      "the\n",
      "final\n",
      "layer\n",
      "of\n",
      "10\n",
      "units\n",
      "is\n",
      "fully\n",
      "connected.\n",
      "we\n",
      "use\n",
      "the\n",
      "tensorflow\n",
      "default\n",
      "adam\n",
      "optimizer\n",
      "to\n",
      "train\n",
      "a\n",
      "sigmoid\n",
      "cross\n",
      "entropy\n",
      "loss\n",
      "on\n",
      "the\n",
      "output\n",
      "of\n",
      "final\n",
      "layer.\n",
      "this\n",
      "model\n",
      "has\n",
      "24:56m\n",
      "parameters\n",
      "which\n",
      "is\n",
      "2\n",
      "times\n",
      "more\n",
      "parameters\n",
      "than\n",
      "capsnet\n",
      "with\n",
      "11:36m\n",
      "parameters.\n",
      "we\n",
      "started\n",
      "with\n",
      "a\n",
      "smaller\n",
      "cnn\n",
      "(32\n",
      "and\n",
      "64\n",
      "convolutional\n",
      "kernels\n",
      "of\n",
      "5\n",
      "\u0002\n",
      "5\n",
      "and\n",
      "stride\n",
      "of\n",
      "1\n",
      "and\n",
      "a\n",
      "512d\n",
      "fully\n",
      "connected\n",
      "layer)\n",
      "and\n",
      "incrementally\n",
      "increased\n",
      "the\n",
      "width\n",
      "of\n",
      "the\n",
      "network\n",
      "until\n",
      "we\n",
      "reached\n",
      "the\n",
      "best\n",
      "test\n",
      "accuracy\n",
      "on\n",
      "a\n",
      "10k\n",
      "subset\n",
      "of\n",
      "the\n",
      "multimnist\n",
      "data.\n",
      "we\n",
      "also\n",
      "searched\n",
      "for\n",
      "the\n",
      "right\n",
      "decay\n",
      "step\n",
      "on\n",
      "the\n",
      "10k\n",
      "validation\n",
      "set.\n",
      "7\n",
      "we\n",
      "decode\n",
      "the\n",
      "two\n",
      "most\n",
      "active\n",
      "digitcaps\n",
      "capsules\n",
      "one\n",
      "at\n",
      "a\n",
      "time\n",
      "and\n",
      "get\n",
      "two\n",
      "images.\n",
      "then\n",
      "by\n",
      "assigning\n",
      "any\n",
      "pixel\n",
      "with\n",
      "non-zero\n",
      "intensity\n",
      "to\n",
      "each\n",
      "digit\n",
      "we\n",
      "get\n",
      "the\n",
      "segmentation\n",
      "results\n",
      "for\n",
      "each\n",
      "digit.\n",
      "7\n",
      "other\n",
      "datasets\n",
      "we\n",
      "tested\n",
      "our\n",
      "capsule\n",
      "model\n",
      "on\n",
      "cifar10\n",
      "with\n",
      "a\n",
      "few\n",
      "different\n",
      "settings\n",
      "of\n",
      "the\n",
      "hyper-parameters\n",
      "and\n",
      "achieved\n",
      "10.6%\n",
      "error\n",
      "with\n",
      "an\n",
      "ensemble\n",
      "of\n",
      "7\n",
      "models\n",
      "each\n",
      "of\n",
      "which\n",
      "is\n",
      "trained\n",
      "with\n",
      "3\n",
      "routing\n",
      "iterations\n",
      "on\n",
      "24\n",
      "\u0002\n",
      "24\n",
      "patches\n",
      "of\n",
      "the\n",
      "image.\n",
      "each\n",
      "model\n",
      "is\n",
      "the\n",
      "same\n",
      "as\n",
      "the\n",
      "simple\n",
      "model\n",
      "we\n",
      "used\n",
      "for\n",
      "mnist\n",
      "except\n",
      "that\n",
      "there\n",
      "are\n",
      "three\n",
      "color\n",
      "channels\n",
      "and\n",
      "we\n",
      "used\n",
      "64\n",
      "different\n",
      "types\n",
      "of\n",
      "primary\n",
      "capsule.\n",
      "we\n",
      "also\n",
      "found\n",
      "that\n",
      "it\n",
      "helped\n",
      "to\n",
      "introduce\n",
      "a\n",
      "\"none-of-the-above\"\n",
      "category\n",
      "for\n",
      "the\n",
      "routing\n",
      "softmaxes,\n",
      "since\n",
      "we\n",
      "do\n",
      "not\n",
      "expect\n",
      "the\n",
      "final\n",
      "layer\n",
      "of\n",
      "ten\n",
      "capsules\n",
      "to\n",
      "explain\n",
      "everything\n",
      "in\n",
      "the\n",
      "image.\n",
      "10.6%\n",
      "test\n",
      "error\n",
      "is\n",
      "about\n",
      "what\n",
      "standard\n",
      "convolutional\n",
      "nets\n",
      "achieved\n",
      "when\n",
      "they\n",
      "were\n",
      "first\n",
      "applied\n",
      "to\n",
      "cifar10\n",
      "(zeiler\n",
      "and\n",
      "fergus\n",
      "[2013]).\n",
      "one\n",
      "drawback\n",
      "of\n",
      "capsules\n",
      "which\n",
      "it\n",
      "shares\n",
      "with\n",
      "generative\n",
      "models\n",
      "is\n",
      "that\n",
      "it\n",
      "likes\n",
      "to\n",
      "account\n",
      "for\n",
      "everything\n",
      "in\n",
      "the\n",
      "image\n",
      "so\n",
      "it\n",
      "does\n",
      "better\n",
      "when\n",
      "it\n",
      "can\n",
      "model\n",
      "the\n",
      "clutter\n",
      "than\n",
      "when\n",
      "it\n",
      "just\n",
      "uses\n",
      "an\n",
      "additional\n",
      "“orphan”\n",
      "category\n",
      "in\n",
      "the\n",
      "dynamic\n",
      "routing.\n",
      "in\n",
      "cifar-10,\n",
      "the\n",
      "backgrounds\n",
      "are\n",
      "much\n",
      "too\n",
      "varied\n",
      "to\n",
      "model\n",
      "in\n",
      "a\n",
      "reasonable\n",
      "sized\n",
      "net\n",
      "which\n",
      "helps\n",
      "to\n",
      "account\n",
      "for\n",
      "the\n",
      "poorer\n",
      "performance.\n",
      "we\n",
      "also\n",
      "tested\n",
      "the\n",
      "exact\n",
      "same\n",
      "architecture\n",
      "as\n",
      "we\n",
      "used\n",
      "for\n",
      "mnist\n",
      "on\n",
      "smallnorb\n",
      "(lecun\n",
      "et\n",
      "al.\n",
      "[2004])\n",
      "and\n",
      "achieved\n",
      "2:7%\n",
      "test\n",
      "error\n",
      "rate,\n",
      "which\n",
      "is\n",
      "in-par\n",
      "with\n",
      "the\n",
      "state-of-the-art\n",
      "(cire¸san\n",
      "et\n",
      "al.\n",
      "[2011]).\n",
      "the\n",
      "smallnorb\n",
      "dataset\n",
      "consists\n",
      "of\n",
      "96x96\n",
      "stereo\n",
      "grey-scale\n",
      "images.\n",
      "we\n",
      "resized\n",
      "the\n",
      "images\n",
      "to\n",
      "48x48\n",
      "and\n",
      "during\n",
      "training\n",
      "processed\n",
      "random\n",
      "32x32\n",
      "crops\n",
      "of\n",
      "them.\n",
      "we\n",
      "passed\n",
      "the\n",
      "central\n",
      "32x32\n",
      "patch\n",
      "during\n",
      "test.\n",
      "we\n",
      "also\n",
      "trained\n",
      "a\n",
      "smaller\n",
      "network\n",
      "on\n",
      "the\n",
      "small\n",
      "training\n",
      "set\n",
      "of\n",
      "svhn\n",
      "(netzer\n",
      "et\n",
      "al.\n",
      "[2011])\n",
      "with\n",
      "only\n",
      "73257\n",
      "images.\n",
      "we\n",
      "reduced\n",
      "the\n",
      "number\n",
      "of\n",
      "first\n",
      "convolutional\n",
      "layer\n",
      "channels\n",
      "to\n",
      "64,\n",
      "the\n",
      "primary\n",
      "capsule\n",
      "layer\n",
      "to\n",
      "16\n",
      "6d-capsules\n",
      "with\n",
      "8d\n",
      "final\n",
      "capsule\n",
      "layer\n",
      "at\n",
      "the\n",
      "end\n",
      "and\n",
      "achieved\n",
      "4:3%\n",
      "on\n",
      "the\n",
      "test\n",
      "set.\n",
      "8\n",
      "discussion\n",
      "and\n",
      "previous\n",
      "work\n",
      "for\n",
      "thirty\n",
      "years,\n",
      "the\n",
      "state-of-the-art\n",
      "in\n",
      "speech\n",
      "recognition\n",
      "used\n",
      "hidden\n",
      "markov\n",
      "models\n",
      "with\n",
      "gaussian\n",
      "mixtures\n",
      "as\n",
      "output\n",
      "distributions.\n",
      "these\n",
      "models\n",
      "were\n",
      "easy\n",
      "to\n",
      "learn\n",
      "on\n",
      "small\n",
      "computers,\n",
      "but\n",
      "they\n",
      "had\n",
      "a\n",
      "representational\n",
      "limitation\n",
      "that\n",
      "was\n",
      "ultimately\n",
      "fatal:\n",
      "the\n",
      "one-of-n\n",
      "representations\n",
      "they\n",
      "use\n",
      "are\n",
      "exponentially\n",
      "inefficient\n",
      "compared\n",
      "with,\n",
      "say,\n",
      "a\n",
      "recurrent\n",
      "neural\n",
      "network\n",
      "that\n",
      "uses\n",
      "distributed\n",
      "representations.\n",
      "to\n",
      "double\n",
      "the\n",
      "amount\n",
      "of\n",
      "information\n",
      "that\n",
      "an\n",
      "hmm\n",
      "can\n",
      "remember\n",
      "about\n",
      "the\n",
      "string\n",
      "it\n",
      "has\n",
      "generated\n",
      "so\n",
      "far,\n",
      "we\n",
      "need\n",
      "to\n",
      "square\n",
      "the\n",
      "number\n",
      "of\n",
      "hidden\n",
      "nodes.\n",
      "for\n",
      "a\n",
      "recurrent\n",
      "net\n",
      "we\n",
      "only\n",
      "need\n",
      "to\n",
      "double\n",
      "the\n",
      "number\n",
      "of\n",
      "hidden\n",
      "neurons.\n",
      "now\n",
      "that\n",
      "convolutional\n",
      "neural\n",
      "networks\n",
      "have\n",
      "become\n",
      "the\n",
      "dominant\n",
      "approach\n",
      "to\n",
      "object\n",
      "recognition,\n",
      "it\n",
      "makes\n",
      "sense\n",
      "to\n",
      "ask\n",
      "whether\n",
      "there\n",
      "are\n",
      "any\n",
      "exponential\n",
      "inefficiencies\n",
      "that\n",
      "may\n",
      "lead\n",
      "to\n",
      "their\n",
      "demise.\n",
      "a\n",
      "good\n",
      "candidate\n",
      "is\n",
      "the\n",
      "difficulty\n",
      "that\n",
      "convolutional\n",
      "nets\n",
      "have\n",
      "in\n",
      "generalizing\n",
      "to\n",
      "novel\n",
      "viewpoints.\n",
      "the\n",
      "ability\n",
      "to\n",
      "deal\n",
      "with\n",
      "translation\n",
      "is\n",
      "built\n",
      "in,\n",
      "but\n",
      "for\n",
      "the\n",
      "other\n",
      "dimensions\n",
      "of\n",
      "an\n",
      "affine\n",
      "transformation\n",
      "we\n",
      "have\n",
      "to\n",
      "chose\n",
      "between\n",
      "replicating\n",
      "feature\n",
      "detectors\n",
      "on\n",
      "a\n",
      "grid\n",
      "that\n",
      "grows\n",
      "exponentially\n",
      "with\n",
      "the\n",
      "number\n",
      "of\n",
      "dimensions,\n",
      "or\n",
      "increasing\n",
      "the\n",
      "size\n",
      "of\n",
      "the\n",
      "labelled\n",
      "training\n",
      "set\n",
      "in\n",
      "a\n",
      "similarly\n",
      "exponential\n",
      "way.\n",
      "capsules\n",
      "(hinton\n",
      "et\n",
      "al.\n",
      "[2011])\n",
      "avoid\n",
      "these\n",
      "exponential\n",
      "inefficiencies\n",
      "by\n",
      "converting\n",
      "pixel\n",
      "intensities\n",
      "into\n",
      "vectors\n",
      "of\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "recognized\n",
      "fragments\n",
      "and\n",
      "then\n",
      "applying\n",
      "transformation\n",
      "matrices\n",
      "to\n",
      "the\n",
      "fragments\n",
      "to\n",
      "predict\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "larger\n",
      "fragments.\n",
      "transformation\n",
      "matrices\n",
      "that\n",
      "learn\n",
      "to\n",
      "encode\n",
      "the\n",
      "intrinsic\n",
      "spatial\n",
      "relationship\n",
      "between\n",
      "a\n",
      "part\n",
      "and\n",
      "a\n",
      "whole\n",
      "constitute\n",
      "viewpoint\n",
      "invariant\n",
      "knowledge\n",
      "that\n",
      "automatically\n",
      "generalizes\n",
      "to\n",
      "novel\n",
      "viewpoints.\n",
      "capsules\n",
      "make\n",
      "a\n",
      "very\n",
      "strong\n",
      "representational\n",
      "assumption:\n",
      "at\n",
      "each\n",
      "location\n",
      "in\n",
      "the\n",
      "image,\n",
      "there\n",
      "is\n",
      "at\n",
      "most\n",
      "one\n",
      "instance\n",
      "of\n",
      "the\n",
      "type\n",
      "of\n",
      "entity\n",
      "that\n",
      "a\n",
      "capsule\n",
      "represents.\n",
      "this\n",
      "assumption,\n",
      "which\n",
      "was\n",
      "motivated\n",
      "by\n",
      "the\n",
      "perceptual\n",
      "phenomenon\n",
      "called\n",
      "\"crowding\"\n",
      "(pelli\n",
      "et\n",
      "al.\n",
      "[2004]),\n",
      "eliminates\n",
      "the\n",
      "binding\n",
      "problem\n",
      "(hinton\n",
      "[1981])\n",
      "and\n",
      "allows\n",
      "a\n",
      "capsule\n",
      "to\n",
      "use\n",
      "a\n",
      "distributed\n",
      "representation\n",
      "(its\n",
      "activity\n",
      "vector)\n",
      "to\n",
      "encode\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "of\n",
      "the\n",
      "entity\n",
      "of\n",
      "that\n",
      "type\n",
      "at\n",
      "a\n",
      "given\n",
      "location.\n",
      "this\n",
      "distributed\n",
      "representation\n",
      "is\n",
      "exponentially\n",
      "more\n",
      "efficient\n",
      "than\n",
      "encoding\n",
      "the\n",
      "instantiation\n",
      "parameters\n",
      "8\n",
      "by\n",
      "activating\n",
      "a\n",
      "point\n",
      "on\n",
      "a\n",
      "high-dimensional\n",
      "grid\n",
      "and\n",
      "with\n",
      "the\n",
      "right\n",
      "distributed\n",
      "representation,\n",
      "capsules\n",
      "can\n",
      "then\n",
      "take\n",
      "full\n",
      "advantage\n",
      "of\n",
      "the\n",
      "fact\n",
      "that\n",
      "spatial\n",
      "relationships\n",
      "can\n",
      "be\n",
      "modelled\n",
      "by\n",
      "matrix\n",
      "multiplies.\n",
      "capsules\n",
      "use\n",
      "neural\n",
      "activities\n",
      "that\n",
      "vary\n",
      "as\n",
      "viewpoint\n",
      "varies\n",
      "rather\n",
      "than\n",
      "trying\n",
      "to\n",
      "eliminate\n",
      "viewpoint\n",
      "variation\n",
      "from\n",
      "the\n",
      "activities.\n",
      "this\n",
      "gives\n",
      "them\n",
      "an\n",
      "advantage\n",
      "over\n",
      "\"normalization\"\n",
      "methods\n",
      "like\n",
      "spatial\n",
      "transformer\n",
      "networks\n",
      "(jaderberg\n",
      "et\n",
      "al.\n",
      "[2015]):\n",
      "they\n",
      "can\n",
      "deal\n",
      "with\n",
      "multiple\n",
      "different\n",
      "affine\n",
      "transformations\n",
      "of\n",
      "different\n",
      "objects\n",
      "or\n",
      "object\n",
      "parts\n",
      "at\n",
      "the\n",
      "same\n",
      "time.\n",
      "capsules\n",
      "are\n",
      "also\n",
      "very\n",
      "good\n",
      "for\n",
      "dealing\n",
      "with\n",
      "segmentation,\n",
      "which\n",
      "is\n",
      "another\n",
      "of\n",
      "the\n",
      "toughest\n",
      "problems\n",
      "in\n",
      "vision,\n",
      "because\n",
      "the\n",
      "vector\n",
      "of\n",
      "instantiation\n",
      "parameters\n",
      "allows\n",
      "them\n",
      "to\n",
      "use\n",
      "routing-by-agreement,\n",
      "as\n",
      "we\n",
      "have\n",
      "demonstrated\n",
      "in\n",
      "this\n",
      "paper.\n",
      "research\n",
      "on\n",
      "capsules\n",
      "is\n",
      "now\n",
      "at\n",
      "a\n",
      "similar\n",
      "stage\n",
      "to\n",
      "research\n",
      "on\n",
      "recurrent\n",
      "neural\n",
      "networks\n",
      "for\n",
      "speech\n",
      "recognition\n",
      "at\n",
      "the\n",
      "beginning\n",
      "of\n",
      "this\n",
      "century.\n",
      "there\n",
      "are\n",
      "fundamental\n",
      "representational\n",
      "reasons\n",
      "for\n",
      "believing\n",
      "that\n",
      "it\n",
      "is\n",
      "a\n",
      "better\n",
      "approach\n",
      "but\n",
      "it\n",
      "probably\n",
      "requires\n",
      "a\n",
      "lot\n",
      "more\n",
      "small\n",
      "insights\n",
      "before\n",
      "it\n",
      "can\n",
      "out-perform\n",
      "a\n",
      "highly\n",
      "developed\n",
      "technology.\n",
      "the\n",
      "fact\n",
      "that\n",
      "a\n",
      "simple\n",
      "capsules\n",
      "system\n",
      "already\n",
      "gives\n",
      "unparalleled\n",
      "performance\n",
      "at\n",
      "segmenting\n",
      "overlapping\n",
      "digits\n",
      "is\n",
      "an\n",
      "early\n",
      "indication\n",
      "that\n",
      "capsules\n",
      "are\n",
      "a\n",
      "direction\n",
      "worth\n",
      "exploring.\n",
      "references\n",
      "martín\n",
      "abadi,\n",
      "ashish\n",
      "agarwal,\n",
      "paul\n",
      "barham,\n",
      "eugene\n",
      "brevdo,\n",
      "zhifeng\n",
      "chen,\n",
      "craig\n",
      "citro,\n",
      "greg\n",
      "s\n",
      "corrado,\n",
      "andy\n",
      "davis,\n",
      "jeffrey\n",
      "dean,\n",
      "matthieu\n",
      "devin,\n",
      "et\n",
      "al.\n",
      "tensorflow:\n",
      "large-scale\n",
      "machine\n",
      "learning\n",
      "on\n",
      "heterogeneous\n",
      "distributed\n",
      "systems.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1603.04467,\n",
      "2016.\n",
      "jimmy\n",
      "ba,\n",
      "volodymyr\n",
      "mnih,\n",
      "and\n",
      "koray\n",
      "kavukcuoglu.\n",
      "multiple\n",
      "object\n",
      "recognition\n",
      "with\n",
      "visual\n",
      "attention.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1412.7755,\n",
      "2014.\n",
      "jia-ren\n",
      "chang\n",
      "and\n",
      "yong-sheng\n",
      "chen.\n",
      "batch-normalized\n",
      "maxout\n",
      "network\n",
      "in\n",
      "network.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1511.02583,\n",
      "2015.\n",
      "dan\n",
      "ciregan,\n",
      "ueli\n",
      "meier,\n",
      "and\n",
      "jürgen\n",
      "schmidhuber.\n",
      "multi-column\n",
      "deep\n",
      "neural\n",
      "networks\n",
      "for\n",
      "image\n",
      "classification.\n",
      "in\n",
      "computer\n",
      "vision\n",
      "and\n",
      "pattern\n",
      "recognition\n",
      "(cvpr),\n",
      "2012\n",
      "ieee\n",
      "conference\n",
      "on,\n",
      "pages\n",
      "3642–3649.\n",
      "ieee,\n",
      "2012.\n",
      "dan\n",
      "c\n",
      "cire¸san,\n",
      "ueli\n",
      "meier,\n",
      "jonathan\n",
      "masci,\n",
      "luca\n",
      "m\n",
      "gambardella,\n",
      "and\n",
      "jürgen\n",
      "schmidhuber.\n",
      "highperformance\n",
      "neural\n",
      "networks\n",
      "for\n",
      "visual\n",
      "object\n",
      "classification.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1102.0183,\n",
      "2011.\n",
      "ian\n",
      "j\n",
      "goodfellow,\n",
      "yaroslav\n",
      "bulatov,\n",
      "julian\n",
      "ibarz,\n",
      "sacha\n",
      "arnoud,\n",
      "and\n",
      "vinay\n",
      "shet.\n",
      "multi-digit\n",
      "number\n",
      "recognition\n",
      "from\n",
      "street\n",
      "view\n",
      "imagery\n",
      "using\n",
      "deep\n",
      "convolutional\n",
      "neural\n",
      "networks.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1312.6082,\n",
      "2013.\n",
      "klaus\n",
      "greff,\n",
      "antti\n",
      "rasmus,\n",
      "mathias\n",
      "berglund,\n",
      "tele\n",
      "hao,\n",
      "harri\n",
      "valpola,\n",
      "and\n",
      "juergen\n",
      "schmidhuber.\n",
      "tagger:\n",
      "deep\n",
      "unsupervised\n",
      "perceptual\n",
      "grouping.\n",
      "in\n",
      "advances\n",
      "in\n",
      "neural\n",
      "information\n",
      "processing\n",
      "systems,\n",
      "pages\n",
      "4484–4492,\n",
      "2016.\n",
      "geoffrey\n",
      "hinton,\n",
      "alex\n",
      "krizhevsky,\n",
      "and\n",
      "sida\n",
      "wang.\n",
      "transforming\n",
      "auto-encoders.\n",
      "artificial\n",
      "neural\n",
      "networks\n",
      "and\n",
      "machine\n",
      "learning–icann\n",
      "2011,\n",
      "pages\n",
      "44–51,\n",
      "2011.\n",
      "geoffrey\n",
      "e.\n",
      "hinton.\n",
      "shape\n",
      "representation\n",
      "in\n",
      "parallel\n",
      "systems.\n",
      "in\n",
      "international\n",
      "joint\n",
      "conference\n",
      "on\n",
      "artificial\n",
      "intelligence\n",
      "vol\n",
      "2,\n",
      "1981.\n",
      "geoffrey\n",
      "e\n",
      "hinton,\n",
      "zoubin\n",
      "ghahramani,\n",
      "and\n",
      "yee\n",
      "whye\n",
      "teh.\n",
      "learning\n",
      "to\n",
      "parse\n",
      "images.\n",
      "in\n",
      "advances\n",
      "in\n",
      "neural\n",
      "information\n",
      "processing\n",
      "systems,\n",
      "pages\n",
      "463–469,\n",
      "2000a.\n",
      "geoffrey\n",
      "e\n",
      "hinton,\n",
      "zoubin\n",
      "ghahramani,\n",
      "and\n",
      "yee\n",
      "whye\n",
      "teh.\n",
      "learning\n",
      "to\n",
      "parse\n",
      "images.\n",
      "in\n",
      "advances\n",
      "in\n",
      "neural\n",
      "information\n",
      "processing\n",
      "systems,\n",
      "pages\n",
      "463–469,\n",
      "2000b.\n",
      "max\n",
      "jaderberg,\n",
      "karen\n",
      "simonyan,\n",
      "andrew\n",
      "zisserman,\n",
      "et\n",
      "al.\n",
      "spatial\n",
      "transformer\n",
      "networks.\n",
      "in\n",
      "advances\n",
      "in\n",
      "neural\n",
      "information\n",
      "processing\n",
      "systems,\n",
      "pages\n",
      "2017–2025,\n",
      "2015.\n",
      "yann\n",
      "lecun,\n",
      "corinna\n",
      "cortes,\n",
      "and\n",
      "christopher\n",
      "jc\n",
      "burges.\n",
      "the\n",
      "mnist\n",
      "database\n",
      "of\n",
      "handwritten\n",
      "digits,\n",
      "1998.\n",
      "9\n",
      "yann\n",
      "lecun,\n",
      "fu\n",
      "jie\n",
      "huang,\n",
      "and\n",
      "leon\n",
      "bottou.\n",
      "learning\n",
      "methods\n",
      "for\n",
      "generic\n",
      "object\n",
      "recognition\n",
      "with\n",
      "invariance\n",
      "to\n",
      "pose\n",
      "and\n",
      "lighting.\n",
      "in\n",
      "computer\n",
      "vision\n",
      "and\n",
      "pattern\n",
      "recognition,\n",
      "2004.\n",
      "cvpr\n",
      "2004.\n",
      "proceedings\n",
      "of\n",
      "the\n",
      "2004\n",
      "ieee\n",
      "computer\n",
      "society\n",
      "conference\n",
      "on,\n",
      "volume\n",
      "2,\n",
      "pages\n",
      "ii–104.\n",
      "ieee,\n",
      "2004.\n",
      "yuval\n",
      "netzer,\n",
      "tao\n",
      "wang,\n",
      "adam\n",
      "coates,\n",
      "alessandro\n",
      "bissacco,\n",
      "bo\n",
      "wu,\n",
      "and\n",
      "andrew\n",
      "y\n",
      "ng.\n",
      "reading\n",
      "digits\n",
      "in\n",
      "natural\n",
      "images\n",
      "with\n",
      "unsupervised\n",
      "feature\n",
      "learning.\n",
      "in\n",
      "nips\n",
      "workshop\n",
      "on\n",
      "deep\n",
      "learning\n",
      "and\n",
      "unsupervised\n",
      "feature\n",
      "learning,\n",
      "volume\n",
      "2011,\n",
      "page\n",
      "5,\n",
      "2011.\n",
      "denis\n",
      "g\n",
      "pelli,\n",
      "melanie\n",
      "palomares,\n",
      "and\n",
      "najib\n",
      "j\n",
      "majaj.\n",
      "crowding\n",
      "is\n",
      "unlike\n",
      "ordinary\n",
      "masking:\n",
      "distinguishing\n",
      "feature\n",
      "integration\n",
      "from\n",
      "detection.\n",
      "journal\n",
      "of\n",
      "vision,\n",
      "4(12):12–12,\n",
      "2004.\n",
      "ikuro\n",
      "sato,\n",
      "hiroki\n",
      "nishimura,\n",
      "and\n",
      "kensuke\n",
      "yokoi.\n",
      "apac:\n",
      "augmented\n",
      "pattern\n",
      "classification\n",
      "with\n",
      "neural\n",
      "networks.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1505.03229,\n",
      "2015.\n",
      "li\n",
      "wan,\n",
      "matthew\n",
      "zeiler,\n",
      "sixin\n",
      "zhang,\n",
      "yann\n",
      "l\n",
      "cun,\n",
      "and\n",
      "rob\n",
      "fergus.\n",
      "regularization\n",
      "of\n",
      "neural\n",
      "networks\n",
      "using\n",
      "dropconnect.\n",
      "in\n",
      "proceedings\n",
      "of\n",
      "the\n",
      "30th\n",
      "international\n",
      "conference\n",
      "on\n",
      "machine\n",
      "learning\n",
      "(icml-13),\n",
      "pages\n",
      "1058–1066,\n",
      "2013.\n",
      "matthew\n",
      "d\n",
      "zeiler\n",
      "and\n",
      "rob\n",
      "fergus.\n",
      "stochastic\n",
      "pooling\n",
      "for\n",
      "regularization\n",
      "of\n",
      "deep\n",
      "convolutional\n",
      "neural\n",
      "networks.\n",
      "arxiv\n",
      "preprint\n",
      "arxiv:1301.3557,\n",
      "2013.\n",
      "a\n",
      "how\n",
      "many\n",
      "routing\n",
      "iterations\n",
      "to\n",
      "use?\n",
      "in\n",
      "order\n",
      "to\n",
      "experimentally\n",
      "verify\n",
      "the\n",
      "convergence\n",
      "of\n",
      "the\n",
      "routing\n",
      "algorithm\n",
      "we\n",
      "plot\n",
      "the\n",
      "average\n",
      "change\n",
      "in\n",
      "the\n",
      "routing\n",
      "logits\n",
      "at\n",
      "each\n",
      "routing\n",
      "iteration.\n",
      "fig.\n",
      "a.1\n",
      "shows\n",
      "the\n",
      "average\n",
      "bij\n",
      "change\n",
      "after\n",
      "each\n",
      "routing\n",
      "iteration.\n",
      "experimentally\n",
      "we\n",
      "observe\n",
      "that\n",
      "there\n",
      "is\n",
      "negligible\n",
      "change\n",
      "in\n",
      "the\n",
      "routing\n",
      "by\n",
      "5\n",
      "iteration\n",
      "from\n",
      "the\n",
      "start\n",
      "of\n",
      "training.\n",
      "average\n",
      "change\n",
      "in\n",
      "the\n",
      "2nd\n",
      "pass\n",
      "of\n",
      "the\n",
      "routing\n",
      "settles\n",
      "down\n",
      "after\n",
      "500\n",
      "epochs\n",
      "of\n",
      "training\n",
      "to\n",
      "0.007\n",
      "while\n",
      "at\n",
      "routing\n",
      "iteration\n",
      "5\n",
      "the\n",
      "logits\n",
      "only\n",
      "change\n",
      "by\n",
      "1e\n",
      "􀀀\n",
      "5\n",
      "on\n",
      "average.\n",
      "figure\n",
      "a.1:\n",
      "average\n",
      "change\n",
      "of\n",
      "each\n",
      "routing\n",
      "logit\n",
      "(bij\n",
      ")\n",
      "by\n",
      "each\n",
      "routing\n",
      "iteration.\n",
      "after\n",
      "500\n",
      "epochs\n",
      "of\n",
      "training\n",
      "on\n",
      "mnist\n",
      "the\n",
      "average\n",
      "change\n",
      "is\n",
      "stabilized\n",
      "and\n",
      "as\n",
      "it\n",
      "shown\n",
      "in\n",
      "right\n",
      "figure\n",
      "it\n",
      "decreases\n",
      "almost\n",
      "linearly\n",
      "in\n",
      "log\n",
      "scale\n",
      "with\n",
      "more\n",
      "routing\n",
      "iterations.\n",
      "(a)\n",
      "during\n",
      "training.\n",
      "(b)\n",
      "log\n",
      "scale\n",
      "of\n",
      "final\n",
      "differences.\n",
      "we\n",
      "observed\n",
      "that\n",
      "in\n",
      "general\n",
      "more\n",
      "routing\n",
      "iterations\n",
      "increases\n",
      "the\n",
      "network\n",
      "capacity\n",
      "and\n",
      "tends\n",
      "to\n",
      "overfit\n",
      "to\n",
      "the\n",
      "training\n",
      "dataset.\n",
      "fig.\n",
      "a.2\n",
      "shows\n",
      "a\n",
      "comparison\n",
      "of\n",
      "capsule\n",
      "training\n",
      "loss\n",
      "on\n",
      "cifar10\n",
      "when\n",
      "trained\n",
      "with\n",
      "1\n",
      "iteration\n",
      "of\n",
      "routing\n",
      "vs\n",
      "3\n",
      "iteration\n",
      "of\n",
      "routing.\n",
      "motivated\n",
      "by\n",
      "fig.\n",
      "a.2\n",
      "and\n",
      "fig.\n",
      "a.1\n",
      "we\n",
      "suggest\n",
      "3\n",
      "iteration\n",
      "of\n",
      "routing\n",
      "for\n",
      "all\n",
      "experiments.\n",
      "10\n",
      "figure\n",
      "a.2:\n",
      "traning\n",
      "loss\n",
      "of\n",
      "capsulenet\n",
      "on\n",
      "cifar10\n",
      "dataset.\n",
      "each\n",
      "training\n",
      "step\n",
      "is\n",
      "on\n",
      "128\n",
      "batches\n",
      "of\n",
      "data.\n",
      "the\n",
      "capsulenet\n",
      "with\n",
      "3\n",
      "iteration\n",
      "of\n",
      "routing\n",
      "optimizes\n",
      "the\n",
      "loss\n",
      "both\n",
      "faster\n",
      "and\n",
      "converges\n",
      "to\n",
      "a\n",
      "lower\n",
      "loss\n",
      "at\n",
      "the\n",
      "end.\n",
      "convolutional 23\n",
      "mnist 20\n",
      "capsnet 20\n",
      "arxiv 15\n",
      "instantiation 13\n",
      "hinton 11\n",
      "2011 9\n",
      "bij 9\n",
      "vj 8\n",
      "ujji 8\n",
      "digitcaps 8\n",
      "multimnist 8\n",
      "parse 7\n",
      "primarycapsules 7\n",
      "affine 7\n",
      "preprint 7\n",
      "convolution 6\n",
      "2015 6\n",
      "reconstructions 6\n",
      "2013 6\n",
      "pooling 5\n",
      "cij 5\n",
      "kernels 5\n",
      "decoder 5\n",
      "segmenting 4\n",
      "softmax 4\n",
      "logits 4\n",
      "conv1 4\n",
      "tensorflow 4\n",
      "2016 4\n",
      "exponentially 4\n",
      "lecun 4\n",
      "2014 4\n",
      "cifar10 4\n",
      "instantiated 3\n",
      "2017 3\n",
      "linearity 3\n",
      "computes 3\n",
      "cnns 3\n",
      "jjsj 3\n",
      "8d 3\n",
      "regularization 3\n",
      "encode 3\n",
      "augmentation 3\n",
      "10k 3\n",
      "confuses 3\n",
      "2012 3\n",
      "skew 3\n",
      "affnist 3\n",
      "zeiler 3\n",
      "fergus 3\n",
      "schmidhuber 3\n",
      "unsupervised 3\n",
      "yann 3\n",
      "frosst 2\n",
      "google 2\n",
      "fixations 2\n",
      "2000a 2\n",
      "wholes 2\n",
      "nips 2\n",
      "cannot 2\n",
      "shrunk 2\n",
      "jj2 2\n",
      "priors 2\n",
      "logit 2\n",
      "jjvcjj 2\n",
      "relu 2\n",
      "sigmoid 2\n",
      "16d 2\n",
      "abadi 2\n",
      "optimizer 2\n",
      "ensembling 2\n",
      "ciregan 2\n",
      "sato 2\n",
      "cnn 2\n",
      "dropout 2\n",
      "perturbations 2\n",
      "robustness 2\n",
      "2000b 2\n",
      "goodfellow 2\n",
      "greff 2\n",
      "smallnorb 2\n",
      "cire 2\n",
      "32x32 2\n",
      "netzer 2\n",
      "inefficiencies 2\n",
      "pelli 2\n",
      "jaderberg 2\n",
      "ueli 2\n",
      "meier 2\n",
      "jürgen 2\n",
      "cvpr 2\n",
      "zoubin 2\n",
      "ghahramani 2\n",
      "yee 2\n",
      "whye 2\n",
      "teh 2\n",
      "epochs 2\n",
      "capsulenet 2\n",
      "sabour 1\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "file = open ('python files/first_20k_words.txt','r')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    (word, f) = line.split()\n",
    "    freq[word] = float(f)\n",
    "file.close()\n",
    "\n",
    "import re # \"regular expressions\" library\n",
    "counts = {}\n",
    "file = open ('python files/Hinton.txt','r', encoding='utf8')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    # words = line.split() - this is a problematic splitter - blank is a default\n",
    "    # there are a few splitters: \\w - alphanumeric splitter, \\W - non-alphanumeric splitter\n",
    "    # \\w+ or \\W+ meaning that there is one or more characters of this type\n",
    "    #words = re.split('\\W+', line) # <---\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print(word)\n",
    "    words = re.split('\\W+', line) \n",
    "    for word in words:\n",
    "        if word not in freq and len(word)>0: # cleanind of empty word  , in not in freq-it means they not popular words\n",
    "            counts[word] = counts.get(word,0) + 1\n",
    "file.close()\n",
    "\n",
    "count = 0\n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    count += 1\n",
    "    if count > 100: # print only 100 first words\n",
    "        break\n",
    "    print(word, counts[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "# regular expressions\n",
    "import re\n",
    "\n",
    "a = 'Hello, world!'\n",
    "match = re.search('e', a)\n",
    "if match:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "match = re.search('H.+o', a)\n",
    "if match:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, wo\n"
     ]
    }
   ],
   "source": [
    "match = re.search('H(.+)o', a) # extract string in brackets- return everything between H and o\n",
    "if match:\n",
    "    print(match.group()) #group()- Return the string matched by the RE   group(1) - not include H and o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "match = re.search('H(\\w+)o', a) # take only alphanumerics\n",
    "if match:\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world\n"
     ]
    }
   ],
   "source": [
    "a = '!Hello, world!'\n",
    "match = re.search('!(.+)!', a) # take only alphanumerics\n",
    "if match:\n",
    "    print(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "urllib.request.urlretrieve('http://allrecipes.com/recipe/18095/buttermilk-cinnamon-rolls/',\n",
    "                           'recipe.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast\n",
      "water\n",
      "buttermilk\n",
      "oil\n",
      "flour\n",
      "salt\n",
      "soda\n",
      "butter\n",
      "sugar\n",
      "cinnamon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "file=open ('recipe.html','r')\n",
    "for line in file:\n",
    "    line= line.strip()\n",
    "    match=re.search('itemprop=\"ingredients\">(.+)</span>',line)\n",
    "    if match:\n",
    "        ingredient=match.group(1)\n",
    "        ingredient= re.sub(' \\(.+\\)$','',ingredient)\n",
    "        ingredient= re.sub(',.+$','',ingredient)\n",
    "        words=re.split('\\W+', ingredient)\n",
    "        print(words[-1]) #take the last word\n",
    "        \n",
    "file.close\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast\n",
      "water\n",
      "buttermilk\n",
      "buttermilk\n",
      "oil\n",
      "yeast\n",
      "flour\n",
      "salt\n",
      "soda\n",
      "flour\n",
      "butter\n",
      "sugar\n",
      "cinnamon\n",
      "sugar\n",
      "butter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "file=open ('recipe.html','r')\n",
    "ingredients={}\n",
    "for line in file:\n",
    "    line= line.strip()\n",
    "    match=re.search('itemprop=\"ingredients\">(.+)</span>',line)\n",
    "    if match:\n",
    "        ingredient=match.group(1)\n",
    "        ingredient= re.sub(' \\(.+\\)$','',ingredient)\n",
    "        ingredient= re.sub(',.+$','',ingredient)\n",
    "        words=re.split('\\W+', ingredient)\n",
    "        ingredients[words[-1]]= True\n",
    "    match=re.search('\"recipe-directions__list--item\">(.+)</span></li>',line)\n",
    "    if match:\n",
    "        step=match.group(1)\n",
    "        step=step.lower()\n",
    "        words=re.split('\\W+', step)\n",
    "        for word in words:\n",
    "             if word in ingredients:\n",
    "                     print (word)\n",
    "        \n",
    "file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'emma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ce508ee56ed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.imdb.com/name/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# bring a page from the Internet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# open file for read\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# initialize counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'emma'"
     ]
    }
   ],
   "source": [
    "import os # bring available libraries\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "name = 'emma' # Initialization: store 3 strings in memory\n",
    "next = 'writer'\n",
    "id = 'nm0000668'\n",
    "\n",
    "urllib.request.urlretrieve('http://www.imdb.com/name/'+id, name+'.html') # bring a page from the Internet\n",
    "os.mkdir(name)\n",
    "file = open(name+'.html','r') # open file for read\n",
    "n = 1 # initialize counter\n",
    "for line in file: # go over the file, line by line\n",
    "    match = re.search(r'^\\<b\\>\\<a href\\=\\\"(\\/title\\/\\w+)\\/\\?ref_', line)\n",
    "    if match: # get inside the block only if the condition is true\n",
    "        print(n, match.group(1))\n",
    "        urllib.request.urlretrieve('http://www.imdb.com'+match.group(1), name+'/'+str(n)+'.html')\n",
    "        n += 1\n",
    "    if re.search(r'<div id=\"filmo-head-'+next+'\" class=\"head\" data-category=', line):\n",
    "        break\n",
    "file.close() # close file\n",
    "\n",
    "dir = os.listdir(name)\n",
    "counts = {}\n",
    "for filename in dir:\n",
    "    file = open(name+'/'+filename, 'r', encoding='utf8')\n",
    "    for line in file:\n",
    "        match = re.search(r'<span class=\"itemprop\" itemprop=\"genre\">(.+)</span></a>', line)\n",
    "        if match:\n",
    "            genre = match.group(1)\n",
    "            counts[genre] = counts.get(genre,0) + 1\n",
    "    file.close()\n",
    "\n",
    "out = open(name+'_genres.txt', 'w') # open file for write\n",
    "for genre in sorted(counts, key=counts.get, reverse=True):\n",
    "    out.write(genre + '\\t'+ str(counts[genre]) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lp.html', <http.client.HTTPMessage at 0x213f90897b8>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "#os.mkdir('Lecture7') # create a new folder in my computer\n",
    "urllib.request.urlretrieve('http://www.metrolyrics.com/lp-lyrics.html', 'lp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.metrolyrics.com/lost-on-you-lyrics-lp.html\n",
      "http://www.metrolyrics.com/night-like-this-lyrics-lp.html\n",
      "http://www.metrolyrics.com/tokyo-sunrise-lyrics-lp.html\n",
      "http://www.metrolyrics.com/into-the-wild-lyrics-lp.html\n",
      "http://www.metrolyrics.com/lost-on-you-lyrics-lp.html\n",
      "http://www.metrolyrics.com/night-like-this-lyrics-lp.html\n",
      "http://www.metrolyrics.com/tokyo-sunrise-lyrics-lp.html\n",
      "http://www.metrolyrics.com/into-the-wild-lyrics-lp.html\n",
      "http://www.metrolyrics.com/suburban-sprawl-alcohol-lyrics-lp.html\n",
      "http://www.metrolyrics.com/cling-to-me-lyrics-lp.html\n",
      "http://www.metrolyrics.com/wasted-love-lyrics-lp.html\n",
      "http://www.metrolyrics.com/savannah-lyrics-lp.html\n",
      "http://www.metrolyrics.com/tightrope-lyrics-lp.html\n",
      "http://www.metrolyrics.com/levitator-live-lyrics-lp.html\n",
      "http://www.metrolyrics.com/wasted-lyrics-lp.html\n",
      "http://www.metrolyrics.com/your-town-lyrics-lp.html\n",
      "http://www.metrolyrics.com/other-people-lyrics-lp.html\n",
      "http://www.metrolyrics.com/one-last-mistake-lyrics-lp.html\n",
      "http://www.metrolyrics.com/when-were-high-lyrics-lp.html\n",
      "http://www.metrolyrics.com/muddy-waters-lyrics-lp.html\n",
      "http://www.metrolyrics.com/salvation-lyrics-lp.html\n",
      "http://www.metrolyrics.com/free-to-love-lyrics-lp.html\n",
      "http://www.metrolyrics.com/no-witness-lyrics-lp.html\n",
      "http://www.metrolyrics.com/heavenly-light-lyrics-lp.html\n",
      "http://www.metrolyrics.com/cadillac-life-lyrics-lp.html\n",
      "http://www.metrolyrics.com/lost-on-you-pilarinos-karypidis-remix-lyrics-lp.html\n",
      "http://www.metrolyrics.com/you-want-it-all-lyrics-lp.html\n",
      "http://www.metrolyrics.com/all-i-have-lyrics-lp.html\n",
      "http://www.metrolyrics.com/forever-for-now-lyrics-lp.html\n",
      "http://www.metrolyrics.com/last-star-lyrics-lp.html\n",
      "http://www.metrolyrics.com/strange-lyrics-lp.html\n",
      "http://www.metrolyrics.com/up-against-me-lyrics-lp.html\n",
      "http://www.metrolyrics.com/suspicion-lyrics-lp.html\n",
      "http://www.metrolyrics.com/get-over-yourself-lyrics-lp.html\n",
      "http://www.metrolyrics.com/other-people-explicit-lyrics-lp.html\n",
      "http://www.metrolyrics.com/levitator-lyrics-lp.html\n",
      "http://www.metrolyrics.com/change-of-scenery-lyrics-lp.html\n",
      "http://www.metrolyrics.com/someday-lyrics-lp.html\n",
      "http://www.metrolyrics.com/nowhere-lyrics-lp.html\n",
      "http://www.metrolyrics.com/never-was-lyrics-lp.html\n",
      "http://www.metrolyrics.com/little-death-lyrics-lp.html\n",
      "http://www.metrolyrics.com/heartless-lyrics-lp.html\n",
      "http://www.metrolyrics.com/death-valley-lyrics-lp.html\n",
      "http://www.metrolyrics.com/other-people-dj-ross-savietto-remix-lyrics-lp.html\n",
      "http://www.metrolyrics.com/the-darkside-lyrics-lp.html\n",
      "http://www.metrolyrics.com/switchblade-lyrics-lp.html\n",
      "http://www.metrolyrics.com/lost-on-you-pilarinos-karypidis-remix-radio-version-lyrics-lp.html\n",
      "http://www.metrolyrics.com/long-way-to-go-to-die-lyrics-lp.html\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file = open('lp.html', 'r')\n",
    "n = 0\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('<a href=\"(.+lp\\.html)\" onmousedown', line)\n",
    "    if match:\n",
    "        print(match.group(1))\n",
    "        urllib.request.urlretrieve(match.group(1), 'Lecture7/'+str(n)+'.html')\n",
    "        n += 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir('Lecture7') # create list of file names in the dir\n",
    "start = False\n",
    "counts = {}\n",
    "for filename in filenames:\n",
    "    file = open('Lecture7/' + filename, 'r')\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if re.search('^<', line):\n",
    "            start = False\n",
    "        if re.search('<p class=\\'verse\\'>', line):\n",
    "            start = True\n",
    "        if start:\n",
    "            line = re.sub('<[^>]+>', ' ', line) # delete html tags\n",
    "            line = line.lower()\n",
    "            words = re.split('\\W+', line)\n",
    "            for word in words:\n",
    "                if len(word) > 0:\n",
    "                    counts[word] = counts.get(word,0) + 1\n",
    "    file.close()\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1318\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 936\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1418eef289bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.metrolyrics.com/lp-lyrics.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lp.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 544\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib\n",
    "\n",
    "urllib.request.urlretrieve('http://www.metrolyrics.com/lp-lyrics.html','lp.html')\n",
    "\n",
    "start = False \n",
    "counts = {}\n",
    "freq = {}\n",
    "folder_name = 'lplyrics'\n",
    "\n",
    "os.mkdir (folder_name)\n",
    "\n",
    "file = open ('first_20k_words.txt','r')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    (word, f) = line.split()\n",
    "    freq[word] = float(f)\n",
    "file.close()\n",
    "\n",
    "file = open('lp.html','r')\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('<a href=\"(.+lp\\.html)\" onmousedown', line) # this match is better/good\n",
    "file.close()\n",
    "\n",
    "file = open('lp.html','r')\n",
    "folder_name = 'lplyrics'\n",
    "index = 0\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('<a href=\"(.+lp\\.html)\" onmousedown', line)\n",
    "    if match:\n",
    "        urllib.request.urlretrieve(match.group(1),folder_name+'/'+str(index)+'.html') # the \"/\" should be of this type\n",
    "        index += 1\n",
    "file.close()\n",
    "\n",
    "dir = os.listdir(folder_name)\n",
    "\n",
    "for filename in dir:\n",
    "    file = open(folder_name+'/'+filename, 'r') # add \", encoding='utf8'\" if needed\n",
    "    for line in file:\n",
    "        if re.search('^<', line):\n",
    "            start = False\n",
    "        if re.search('<p class=\\'verse\\'>', line): #<p class='verse'>\n",
    "            start = True\n",
    "        if start:\n",
    "            line = re.sub('<[^>]+>', ' ', line) # delete html tags\n",
    "            line = line.lower()\n",
    "            words = re.split('\\W+', line)\n",
    "            for word in words:\n",
    "                if word not in freq and len(word) > 0:\n",
    "                    counts[word] = counts.get(word, 0) + 1\n",
    "    file.close()\n",
    "    \n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    print(word, counts[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LP_AllLyrics.html', <http.client.HTTPMessage at 0x213f90e9f60>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "urllib.request.urlretrieve('http://www.metrolyrics.com/lp-lyrics.html', 'LP_AllLyrics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('LP song links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file = open('LP_AllLyrics.html', 'r')\n",
    "n=0\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('<a href=\"(.+lp\\.html)\" onmousedown', line)\n",
    "    if match:\n",
    "        LP_url = match.group(1)\n",
    "        urllib.request.urlretrieve(match.group(1), 'LP song links/'+str(n)+'.html')\n",
    "        n+=1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 623\n",
      "i 474\n",
      "the 311\n",
      "me 283\n",
      "oh 263\n",
      "and 243\n",
      "to 242\n",
      "it 236\n",
      "a 161\n",
      "we 160\n",
      "t 160\n",
      "on 147\n",
      "s 146\n",
      "all 130\n",
      "re 126\n",
      "that 114\n",
      "in 114\n",
      "know 107\n",
      "is 104\n",
      "my 100\n",
      "don 86\n",
      "can 85\n",
      "love 84\n",
      "of 80\n",
      "never 80\n",
      "your 77\n",
      "lost 74\n",
      "just 74\n",
      "when 72\n",
      "are 72\n",
      "ll 71\n",
      "get 69\n",
      "like 67\n",
      "go 67\n",
      "up 65\n",
      "m 65\n",
      "but 64\n",
      "let 58\n",
      "yeah 58\n",
      "so 57\n",
      "time 57\n",
      "see 55\n",
      "be 54\n",
      "one 53\n",
      "no 52\n",
      "this 49\n",
      "what 46\n",
      "was 45\n",
      "baby 45\n",
      "there 44\n",
      "with 44\n",
      "for 42\n",
      "ve 42\n",
      "wasted 40\n",
      "long 40\n",
      "la 40\n",
      "down 39\n",
      "falling 36\n",
      "they 36\n",
      "take 36\n",
      "doo 36\n",
      "away 35\n",
      "again 34\n",
      "give 34\n",
      "make 34\n",
      "die 33\n",
      "want 32\n",
      "now 32\n",
      "do 32\n",
      "levitator 32\n",
      "ooooh 31\n",
      "tell 30\n",
      "way 30\n",
      "gonna 30\n",
      "not 29\n",
      "last 29\n",
      "could 28\n",
      "more 28\n",
      "light 28\n",
      "cause 27\n",
      "here 27\n",
      "too 26\n",
      "had 26\n",
      "heavenly 26\n",
      "wait 26\n",
      "life 25\n",
      "everything 24\n",
      "against 24\n",
      "will 23\n",
      "ever 23\n",
      "from 22\n",
      "how 22\n",
      "or 22\n",
      "day 22\n",
      "d 22\n",
      "believe 22\n",
      "where 22\n",
      "change 21\n",
      "high 21\n",
      "gone 21\n",
      "ah 21\n",
      "mind 20\n",
      "come 20\n",
      "out 20\n",
      "always 20\n",
      "wanna 20\n",
      "said 19\n",
      "right 19\n",
      "say 19\n",
      "suspicion 19\n",
      "ooh 18\n",
      "hold 18\n",
      "our 18\n",
      "ahoo 18\n",
      "stars 18\n",
      "world 18\n",
      "back 18\n",
      "she 18\n",
      "wanted 17\n",
      "turn 17\n",
      "heart 17\n",
      "town 17\n",
      "place 17\n",
      "won 16\n",
      "feel 16\n",
      "alone 16\n",
      "sende 16\n",
      "if 15\n",
      "got 15\n",
      "still 15\n",
      "night 15\n",
      "were 15\n",
      "every 15\n",
      "into 15\n",
      "us 15\n",
      "cling 15\n",
      "em 14\n",
      "cut 14\n",
      "only 14\n",
      "coming 14\n",
      "by 14\n",
      "same 14\n",
      "times 14\n",
      "have 14\n",
      "people 14\n",
      "heartless 14\n",
      "ya 14\n",
      "after 13\n",
      "hope 13\n",
      "been 13\n",
      "some 13\n",
      "save 13\n",
      "over 13\n",
      "mi 13\n",
      "kaybedildiler 13\n",
      "then 13\n",
      "other 13\n",
      "wrong 13\n",
      "without 13\n",
      "things 12\n",
      "ho 12\n",
      "hoooooh 12\n",
      "end 12\n",
      "alive 12\n",
      "did 12\n",
      "home 12\n",
      "thought 12\n",
      "bear 12\n",
      "witness 12\n",
      "who 12\n",
      "hearts 11\n",
      "forever 11\n",
      "something 11\n",
      "another 11\n",
      "until 11\n",
      "live 11\n",
      "anything 11\n",
      "less 11\n",
      "stay 11\n",
      "break 11\n",
      "sky 11\n",
      "hard 11\n",
      "run 11\n",
      "chorus 11\n",
      "remember 10\n",
      "going 10\n",
      "raise 10\n",
      "around 10\n",
      "touch 10\n",
      "didn 10\n",
      "why 10\n",
      "as 10\n",
      "hear 10\n",
      "tight 10\n",
      "walk 10\n",
      "near 10\n",
      "bring 10\n",
      "leave 10\n",
      "at 10\n",
      "about 10\n",
      "gotta 10\n",
      "heaven 9\n",
      "glass 9\n",
      "two 9\n",
      "loose 9\n",
      "hate 9\n",
      "ones 9\n",
      "well 9\n",
      "really 9\n",
      "keep 9\n",
      "pride 9\n",
      "wasn 9\n",
      "dark 9\n",
      "savannah 9\n",
      "good 9\n",
      "waiting 9\n",
      "head 9\n",
      "need 9\n",
      "blow 9\n",
      "muddy 9\n",
      "ohhhh 9\n",
      "tonight 9\n",
      "breaking 9\n",
      "uh 9\n",
      "yourself 9\n",
      "dawn 8\n",
      "through 8\n",
      "free 8\n",
      "knew 8\n",
      "fire 8\n",
      "low 8\n",
      "rope 8\n",
      "gold 8\n",
      "while 8\n",
      "lord 8\n",
      "harder 8\n",
      "aaaaa 8\n",
      "many 8\n",
      "days 7\n",
      "smoke 7\n",
      "knows 7\n",
      "much 7\n",
      "own 7\n",
      "cold 7\n",
      "air 7\n",
      "nowhere 7\n",
      "slow 7\n",
      "even 7\n",
      "tüm 7\n",
      "enough 7\n",
      "water 7\n",
      "starts 7\n",
      "ain 7\n",
      "woh 7\n",
      "hurt 7\n",
      "than 6\n",
      "mine 6\n",
      "whats 6\n",
      "happy 6\n",
      "stop 6\n",
      "sorry 6\n",
      "used 6\n",
      "lie 6\n",
      "true 6\n",
      "couldn 6\n",
      "look 6\n",
      "learn 6\n",
      "care 6\n",
      "afraid 6\n",
      "takes 6\n",
      "elevate 6\n",
      "mess 6\n",
      "talk 6\n",
      "maybe 6\n",
      "body 6\n",
      "mistake 6\n",
      "stuff 6\n",
      "finally 6\n",
      "behind 6\n",
      "rain 6\n",
      "find 6\n",
      "sun 6\n",
      "someday 6\n",
      "salvation 6\n",
      "lead 6\n",
      "nothing 6\n",
      "hoooooah 6\n",
      "ohhoah 6\n",
      "thing 6\n",
      "changed 6\n",
      "girl 6\n",
      "easy 6\n",
      "hey 6\n",
      "anyone 6\n",
      "strange 6\n",
      "being 6\n",
      "person 6\n",
      "switchblade 6\n",
      "came 5\n",
      "any 5\n",
      "name 5\n",
      "loved 5\n",
      "wish 5\n",
      "words 5\n",
      "along 5\n",
      "face 5\n",
      "made 5\n",
      "lose 5\n",
      "middle 5\n",
      "dance 5\n",
      "because 5\n",
      "soul 5\n",
      "seems 5\n",
      "pain 5\n",
      "part 5\n",
      "stays 5\n",
      "before 5\n",
      "them 5\n",
      "rise 5\n",
      "starting 5\n",
      "meant 5\n",
      "done 5\n",
      "far 5\n",
      "fall 5\n",
      "broke 5\n",
      "ground 5\n",
      "star 5\n",
      "barely 5\n",
      "wave 5\n",
      "bad 5\n",
      "explain 5\n",
      "unchain 5\n",
      "else 5\n",
      "yoooh 5\n",
      "ordinary 5\n",
      "darkside 5\n",
      "tender 4\n",
      "surrender 4\n",
      "years 4\n",
      "understand 4\n",
      "drink 4\n",
      "born 4\n",
      "tear 4\n",
      "apart 4\n",
      "lovers 4\n",
      "doodoo 4\n",
      "cross 4\n",
      "song 4\n",
      "taking 4\n",
      "since 4\n",
      "rough 4\n",
      "matter 4\n",
      "inside 4\n",
      "kiss 4\n",
      "calling 4\n",
      "trying 4\n",
      "chance 4\n",
      "today 4\n",
      "put 4\n",
      "lift 4\n",
      "levitate 4\n",
      "half 4\n",
      "already 4\n",
      "little 4\n",
      "lonely 4\n",
      "though 4\n",
      "wants 4\n",
      "hadi 4\n",
      "bir 4\n",
      "söyle 4\n",
      "oooooh 4\n",
      "am 4\n",
      "hit 4\n",
      "god 4\n",
      "kind 4\n",
      "wherever 4\n",
      "tokyo 4\n",
      "ooooooooooh 4\n",
      "everybody 4\n",
      "outta 4\n",
      "screaming 4\n",
      "ten 4\n",
      "dead 4\n",
      "doubt 4\n",
      "needin 4\n",
      "decisions 4\n",
      "mad 4\n",
      "choice 4\n",
      "independent 4\n",
      "essence 4\n",
      "spend 4\n",
      "huh 4\n",
      "melody 4\n",
      "till 4\n",
      "beautiful 4\n",
      "fuck 4\n",
      "once 4\n",
      "reckless 4\n",
      "rock 4\n",
      "older 3\n",
      "plainer 3\n",
      "saner 3\n",
      "danger 3\n",
      "burning 3\n",
      "embers 3\n",
      "ago 3\n",
      "toil 3\n",
      "expectations 3\n",
      "patience 3\n",
      "beyond 3\n",
      "together 3\n",
      "pretend 3\n",
      "survive 3\n",
      "whistle 3\n",
      "taste 3\n",
      "rolling 3\n",
      "sand 3\n",
      "took 3\n",
      "tried 3\n",
      "breath 3\n",
      "an 3\n",
      "loud 3\n",
      "sacred 3\n",
      "ooooo 3\n",
      "midnight 3\n",
      "eyes 3\n",
      "ask 3\n",
      "fly 3\n",
      "mid 3\n",
      "feels 3\n",
      "cry 3\n",
      "castaway 3\n",
      "went 3\n",
      "led 3\n",
      "state 3\n",
      "grace 3\n",
      "lately 3\n",
      "would 3\n",
      "winds 3\n",
      "intoxicates 3\n",
      "streets 3\n",
      "filled 3\n",
      "frozen 3\n",
      "golden 3\n",
      "çünkü 3\n",
      "iki 3\n",
      "kadeh 3\n",
      "kaldıralım 3\n",
      "kaybettiğim 3\n",
      "şeylere 3\n",
      "senin 3\n",
      "üzerinden 3\n",
      "benden 3\n",
      "kurtulabilirsin 3\n",
      "herşeyi 3\n",
      "kaybettikten 3\n",
      "sonra 3\n",
      "ahead 3\n",
      "somebody 3\n",
      "guess 3\n",
      "bed 3\n",
      "bars 3\n",
      "worst 3\n",
      "seem 3\n",
      "million 3\n",
      "fine 3\n",
      "crawling 3\n",
      "holds 3\n",
      "sold 3\n",
      "waters 3\n",
      "round 3\n",
      "saved 3\n",
      "flame 3\n",
      "think 3\n",
      "paid 3\n",
      "married 3\n",
      "youth 3\n",
      "sins 3\n",
      "pour 3\n",
      "laugh 3\n",
      "flow 3\n",
      "comin 3\n",
      "coast 3\n",
      "sparks 3\n",
      "waking 3\n",
      "sure 3\n",
      "trade 3\n",
      "chevy 3\n",
      "cadillac 3\n",
      "drive 3\n",
      "crank 3\n",
      "motor 3\n",
      "hundred 3\n",
      "goodbye 3\n",
      "promise 3\n",
      "block 3\n",
      "lock 3\n",
      "makes 3\n",
      "falls 3\n",
      "burns 3\n",
      "lights 3\n",
      "left 3\n",
      "devout 3\n",
      "shred 3\n",
      "worried 3\n",
      "bout 3\n",
      "spell 3\n",
      "thank 3\n",
      "none 3\n",
      "scenery 3\n",
      "view 3\n",
      "myself 3\n",
      "young 3\n",
      "try 3\n",
      "needed 3\n",
      "help 3\n",
      "loveless 3\n",
      "hiding 3\n",
      "cuts 3\n",
      "stab 3\n",
      "fell 3\n",
      "happiness 3\n",
      "longing 2\n",
      "wishin 2\n",
      "machinations 2\n",
      "crush 2\n",
      "summer 2\n",
      "lust 2\n",
      "dared 2\n",
      "chain 2\n",
      "bane 2\n",
      "remain 2\n",
      "ooo 2\n",
      "lucky 2\n",
      "blame 2\n",
      "pay 2\n",
      "cost 2\n",
      "summers 2\n",
      "played 2\n",
      "wasnt 2\n",
      "innocent 2\n",
      "storming 2\n",
      "morning 2\n",
      "road 2\n",
      "should 2\n",
      "comes 2\n",
      "almost 2\n",
      "found 2\n",
      "felt 2\n",
      "mistakes 2\n",
      "has 2\n",
      "brought 2\n",
      "escape 2\n",
      "scared 2\n",
      "wreckage 2\n",
      "hurricane 2\n",
      "bigger 2\n",
      "stage 2\n",
      "legend 2\n",
      "pot 2\n",
      "rainbow 2\n",
      "higher 2\n",
      "waste 2\n",
      "moment 2\n",
      "empty 2\n",
      "year 2\n",
      "okay 2\n",
      "although 2\n",
      "caught 2\n",
      "thrilled 2\n",
      "past 2\n",
      "daha 2\n",
      "gibi 2\n",
      "önce 2\n",
      "ele 2\n",
      "geçirdiğinde 2\n",
      "içine 2\n",
      "çek 2\n",
      "onları 2\n",
      "batıyor 2\n",
      "istediğim 2\n",
      "sendin 2\n",
      "asla 2\n",
      "bebeğim 2\n",
      "wishing 2\n",
      "geri 2\n",
      "bana 2\n",
      "loneliness 2\n",
      "veins 2\n",
      "heal 2\n",
      "fade 2\n",
      "ends 2\n",
      "someone 2\n",
      "swallow 2\n",
      "moon 2\n",
      "follow 2\n",
      "memories 2\n",
      "nights 2\n",
      "edge 2\n",
      "mercy 2\n",
      "blind 2\n",
      "clear 2\n",
      "choose 2\n",
      "planned 2\n",
      "its 2\n",
      "fail 2\n",
      "arms 2\n",
      "lot 2\n",
      "reigned 2\n",
      "shores 2\n",
      "beaten 2\n",
      "waves 2\n",
      "spark 2\n",
      "heartbeat 2\n",
      "scent 2\n",
      "paths 2\n",
      "mines 2\n",
      "aching 2\n",
      "debt 2\n",
      "horses 2\n",
      "broken 2\n",
      "splayed 2\n",
      "breathing 2\n",
      "blood 2\n",
      "warm 2\n",
      "fires 2\n",
      "oooooooooooooh 2\n",
      "whoot 2\n",
      "gave 2\n",
      "walls 2\n",
      "crazy 2\n",
      "highs 2\n",
      "feelin 2\n",
      "code 2\n",
      "seen 2\n",
      "bet 2\n",
      "buried 2\n",
      "table 2\n",
      "war 2\n",
      "guard 2\n",
      "yours 2\n",
      "sow 2\n",
      "seed 2\n",
      "regrets 2\n",
      "gets 2\n",
      "such 2\n",
      "shame 2\n",
      "leavin 2\n",
      "real 2\n",
      "minute 2\n",
      "bounce 2\n",
      "bouncin 2\n",
      "son 2\n",
      "holdin 2\n",
      "spot 2\n",
      "reppin 2\n",
      "girls 2\n",
      "cat 2\n",
      "nothin 2\n",
      "game 2\n",
      "hush 2\n",
      "heard 2\n",
      "lies 2\n",
      "awake 2\n",
      "sung 2\n",
      "anymore 2\n",
      "rush 2\n",
      "small 2\n",
      "ocean 2\n",
      "wanting 2\n",
      "piling 2\n",
      "watch 2\n",
      "trust 2\n",
      "universe 2\n",
      "getting 2\n",
      "worse 2\n",
      "wild 2\n",
      "catch 2\n",
      "smile 2\n",
      "government 2\n",
      "read 2\n",
      "matters 2\n",
      "turning 2\n",
      "chase 2\n",
      "fear 2\n",
      "cruel 2\n",
      "cheat 2\n",
      "great 2\n",
      "above 2\n",
      "babe 2\n",
      "darling 2\n",
      "fading 2\n",
      "astray 2\n",
      "making 2\n",
      "goin 2\n",
      "decide 2\n",
      "best 2\n",
      "somewhere 2\n",
      "happened 2\n",
      "hell 2\n",
      "move 2\n",
      "told 2\n",
      "worry 2\n",
      "different 2\n",
      "story 2\n",
      "amused 2\n",
      "missing 2\n",
      "dusk 2\n",
      "hallelujah 2\n",
      "glad 2\n",
      "suburban 2\n",
      "sprawl 2\n",
      "alcohol 2\n",
      "use 2\n",
      "isn 2\n",
      "slip 2\n",
      "hopelessly 2\n",
      "sweet 1\n",
      "stone 1\n",
      "pretty 1\n",
      "slipped 1\n",
      "fingers 1\n",
      "hand 1\n",
      "cage 1\n",
      "set 1\n",
      "single 1\n",
      "searing 1\n",
      "hoping 1\n",
      "kept 1\n",
      "mountain 1\n",
      "southern 1\n",
      "drove 1\n",
      "ashes 1\n",
      "episode 1\n",
      "house 1\n",
      "stayed 1\n",
      "miss 1\n",
      "sweetness 1\n",
      "anyway 1\n",
      "close 1\n",
      "shut 1\n",
      "feather 1\n",
      "float 1\n",
      "clouds 1\n",
      "step 1\n",
      "fearless 1\n",
      "reach 1\n",
      "dearest 1\n",
      "cast 1\n",
      "expect 1\n",
      "thew 1\n",
      "sunny 1\n",
      "california 1\n",
      "stories 1\n",
      "corner 1\n",
      "old 1\n",
      "new 1\n",
      "speed 1\n",
      "holding 1\n",
      "snapshots 1\n",
      "page 1\n",
      "wondering 1\n",
      "thinking 1\n",
      "everywhere 1\n",
      "yaşlandığında 1\n",
      "sade 1\n",
      "aklı 1\n",
      "başında 1\n",
      "olduğunda 1\n",
      "çinden 1\n",
      "geldiğimiz 1\n",
      "tehlikeleri 1\n",
      "hatırladığında 1\n",
      "köz 1\n",
      "yanan 1\n",
      "düşen 1\n",
      "hassas 1\n",
      "olan 1\n",
      "teslimiyetin 1\n",
      "olmadığı 1\n",
      "günlerden 1\n",
      "yıllar 1\n",
      "iyi 1\n",
      "bildiğin 1\n",
      "cennete 1\n",
      "gidemeyeceğim 1\n",
      "nasıl 1\n",
      "gidileceğini 1\n",
      "bilmiyorum 1\n",
      "nations 1\n",
      "halk 1\n",
      "arasında 1\n",
      "görmeyi 1\n",
      "umuyordum 1\n",
      "aklındaki 1\n",
      "beklentilerin 1\n",
      "zahmetini 1\n",
      "anlıyorum 1\n",
      "sabrını 1\n",
      "kaybetmemiş 1\n",
      "sarıl 1\n",
      "beni 1\n",
      "nefret 1\n",
      "ettiğinden 1\n",
      "çok 1\n",
      "sevdiğini 1\n",
      "hala 1\n",
      "benimsin 1\n",
      "lisk 1\n",
      "böylece 1\n",
      "içki 1\n",
      "alalım 1\n",
      "zamanların 1\n",
      "liskinden 1\n",
      "dönelim 1\n",
      "suddenly 1\n",
      "dreams 1\n",
      "shadow 1\n",
      "full 1\n",
      "spent 1\n",
      "throwing 1\n",
      "stones 1\n",
      "destined 1\n",
      "desolate 1\n",
      "building 1\n",
      "bridges 1\n",
      "whether 1\n",
      "rested 1\n",
      "dives 1\n",
      "vibes 1\n",
      "pass 1\n",
      "beat 1\n",
      "collected 1\n",
      "slowly 1\n",
      "wound 1\n",
      "bouquet 1\n",
      "favorite 1\n",
      "wallow 1\n",
      "jimmy 1\n",
      "hips 1\n",
      "insert 1\n",
      "random 1\n",
      "ass 1\n",
      "line 1\n",
      "kneeling 1\n",
      "rivers 1\n",
      "tempting 1\n",
      "steps 1\n",
      "closer 1\n",
      "gnawing 1\n",
      "corners 1\n",
      "pathway 1\n",
      "spirits 1\n",
      "gather 1\n",
      "across 1\n",
      "pull 1\n",
      "arrived 1\n",
      "door 1\n",
      "recognize 1\n",
      "shore 1\n",
      "retreat 1\n",
      "isolation 1\n",
      "locked 1\n",
      "temptation 1\n",
      "tie 1\n",
      "frustrated 1\n",
      "desperation 1\n",
      "sometimes 1\n",
      "virtual 1\n",
      "lobotomy 1\n",
      "looking 1\n",
      "their 1\n",
      "mediocrity 1\n",
      "big 1\n",
      "plague 1\n",
      "truth 1\n",
      "condone 1\n",
      "caving 1\n",
      "off 1\n",
      "hypocrisy 1\n",
      "escaped 1\n",
      "street 1\n",
      "drug 1\n",
      "doors 1\n",
      "wars 1\n",
      "cars 1\n",
      "kinda 1\n",
      "lookin 1\n",
      "breakin 1\n",
      "droppin 1\n",
      "calls 1\n",
      "halls 1\n",
      "bustin 1\n",
      "balls 1\n",
      "joke 1\n",
      "touche 1\n",
      "burn 1\n",
      "bridge 1\n",
      "sink 1\n",
      "swim 1\n",
      "crawl 1\n",
      "blue 1\n",
      "hot 1\n",
      "win 1\n",
      "honey 1\n",
      "soldiers 1\n",
      "damn 1\n",
      "expected 1\n",
      "drinkin 1\n",
      "drugs 1\n",
      "chime 1\n",
      "quit 1\n",
      "concrete 1\n",
      "jumgle 1\n",
      "tumblin 1\n",
      "gasoline 1\n",
      "roundabout 1\n",
      "swimming 1\n",
      "these 1\n",
      "fields 1\n",
      "prepare 1\n",
      "stand 1\n",
      "count 1\n",
      "died 1\n",
      "changing 1\n",
      "bitter 1\n",
      "poetry 1\n",
      "subtleties 1\n",
      "call 1\n",
      "jumped 1\n",
      "livin 1\n",
      "very 1\n",
      "special 1\n",
      "act 1\n",
      "mistreated 1\n",
      "whatever 1\n",
      "phase 1\n",
      "reminisce 1\n",
      "man 1\n",
      "waited 1\n",
      "boy 1\n",
      "promises 1\n",
      "replace 1\n",
      "nervous 1\n",
      "settlin 1\n",
      "especially 1\n",
      "creepin 1\n",
      "cocky 1\n",
      "sounds 1\n",
      "giggle 1\n",
      "better 1\n",
      "angry 1\n",
      "prove 1\n",
      "playin 1\n",
      "gamin 1\n",
      "actin 1\n",
      "wake 1\n",
      "chill 1\n",
      "homey 1\n",
      "instead 1\n",
      "beef 1\n",
      "phony 1\n",
      "console 1\n",
      "word 1\n",
      "faint 1\n",
      "cries 1\n",
      "hardly 1\n",
      "storm 1\n",
      "horizon 1\n",
      "sweep 1\n",
      "children 1\n",
      "untainted 1\n",
      "hourglass 1\n",
      "escapes 1\n",
      "carefree 1\n",
      "clock 1\n",
      "ticks 1\n",
      "watched 1\n",
      "ending 1\n",
      "ascending 1\n",
      "fragmented 1\n",
      "pictures 1\n",
      "beating 1\n",
      "needing 1\n",
      "coliding 1\n",
      "speak 1\n",
      "laid 1\n",
      "brave 1\n",
      "tame 1\n",
      "burst 1\n",
      "flames 1\n",
      "stoke 1\n",
      "coals 1\n",
      "everyday 1\n",
      "guarantee 1\n",
      "whenever 1\n",
      "knowing 1\n",
      "asked 1\n",
      "wall 1\n",
      "chips 1\n",
      "lennon 1\n",
      "sent 1\n",
      "somehow 1\n",
      "invent 1\n",
      "becomes 1\n",
      "crime 1\n",
      "crippled 1\n",
      "emotion 1\n",
      "green 1\n",
      "silliest 1\n",
      "notions 1\n",
      "romeo 1\n",
      "potion 1\n",
      "basically 1\n",
      "anywhere 1\n",
      "roads 1\n",
      "first 1\n",
      "treated 1\n",
      "king 1\n",
      "fucking 1\n",
      "spoiled 1\n",
      "rotten 1\n",
      "gotten 1\n",
      "hateable 1\n",
      "debatable 1\n",
      "gain 1\n",
      "superior 1\n",
      "everyone 1\n",
      "inferior 1\n",
      "hero 1\n",
      "zero 1\n",
      "floor 1\n",
      "encore 1\n",
      "lived 1\n",
      "conversations 1\n",
      "sad 1\n",
      "minds 1\n",
      "money 1\n",
      "constitute 1\n",
      "dictate 1\n",
      "goddamn 1\n",
      "fast 1\n",
      "wonder 1\n",
      "injustice 1\n",
      "track 1\n",
      "turns 1\n",
      "whan 1\n",
      "ou 1\n",
      "race 1\n",
      "saving 1\n",
      "fate 1\n",
      "sway 1\n",
      "might 1\n",
      "pace 1\n",
      "hopes 1\n",
      "saints 1\n",
      "collide 1\n",
      "sick 1\n",
      "sayin 1\n",
      "photograph 1\n",
      "laughter 1\n",
      "lying 1\n",
      "holdling 1\n",
      "driving 1\n",
      "nobody 1\n",
      "understood 1\n",
      "flowers 1\n",
      "snow 1\n",
      "river 1\n",
      "aware 1\n",
      "dumb 1\n",
      "bitch 1\n",
      "applies 1\n",
      "happen 1\n",
      "suffer 1\n",
      "connected 1\n",
      "mouth 1\n",
      "bell 1\n",
      "dog 1\n",
      "lottery 1\n",
      "tickets 1\n",
      "job 1\n",
      "rushing 1\n",
      "city 1\n",
      "killin 1\n",
      "kills 1\n",
      "space 1\n",
      "map 1\n",
      "pushin 1\n",
      "tryin 1\n",
      "pawnshop 1\n",
      "guy 1\n",
      "shit 1\n",
      "horoscope 1\n",
      "says 1\n",
      "western 1\n",
      "start 1\n",
      "dying 1\n",
      "roll 1\n",
      "stretching 1\n",
      "anybody 1\n",
      "leather 1\n",
      "seats 1\n",
      "gettin 1\n",
      "hotter 1\n",
      "taps 1\n",
      "her 1\n",
      "feet 1\n",
      "spits 1\n",
      "killed 1\n",
      "sang 1\n",
      "twenty 1\n",
      "songs 1\n",
      "rotation 1\n",
      "goes 1\n",
      "occasion 1\n",
      "vacation 1\n",
      "fakin 1\n",
      "ours 1\n",
      "beginning 1\n",
      "liked 1\n",
      "each 1\n",
      "hoppin 1\n",
      "summertime 1\n",
      "cared 1\n",
      "abd 1\n",
      "hill 1\n",
      "top 1\n",
      "topanga 1\n",
      "handful 1\n",
      "pills 1\n",
      "adventure 1\n",
      "late 1\n",
      "september 1\n",
      "started 1\n",
      "pretending 1\n",
      "electric 1\n",
      "accept 1\n",
      "regret 1\n",
      "moving 1\n",
      "rival 1\n",
      "tears 1\n",
      "send 1\n",
      "ring 1\n",
      "realized 1\n",
      "fears 1\n",
      "five 1\n",
      "stitch 1\n",
      "patching 1\n",
      "seams 1\n",
      "fake 1\n",
      "aw 1\n",
      "rescue 1\n",
      "tryna 1\n",
      "forget 1\n",
      "haven 1\n",
      "yet 1\n",
      "kitchen 1\n",
      "sewing 1\n",
      "room 1\n",
      "soon 1\n",
      "blacker 1\n",
      "taught 1\n",
      "belong 1\n",
      "running 1\n",
      "streetlights 1\n",
      "leading 1\n",
      "sight 1\n",
      "bleed 1\n"
     ]
    }
   ],
   "source": [
    "start = False \n",
    "counts = {}\n",
    "for file in os.listdir('LP song links'):\n",
    "    file = open('LP song links/'+file, 'r', encoding='utf8')\n",
    "    for line in file:\n",
    "        if re.search('^<', line):\n",
    "            start = False\n",
    "        if re.search('<p class=\\'verse\\'>', line): #<p class='verse'>\n",
    "            start = True\n",
    "        if start:\n",
    "            line = re.sub('<[^>]+>', ' ', line) # delete html tags\n",
    "            line = line.lower()\n",
    "            words = re.split('\\W+', line)\n",
    "            for word in words:\n",
    "                if len(word) > 0:\n",
    "                    counts[word] = counts.get(word, 0) + 1\n",
    "    file.close()\n",
    "for word in sorted(counts, key=counts.get, reverse=True):\n",
    "    print(word, counts[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trumptweets/937065834635292673.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cb6fb90d1a1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtweetsdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/status/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trumptweets/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trumptweets/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# Handle temporary file setup.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mtfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trumptweets/937065834635292673.html'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "\n",
    "#os.mkdir(\"trumptweets\")\n",
    "url = \"https://twitter.com/realDonaldTrump\"\n",
    "urllib.request.urlretrieve(url, 'trump.html')\n",
    "file = open('trump.html', encoding=\"utf-8\")\n",
    "\n",
    "tweets = []\n",
    "tweetsdict = {}\n",
    "for line in file:\n",
    "    match = re.search('<a href=\\\"/realDonaldTrump/status/(\\d+)\\\" class=\\\"tweet-timestamp', line)\n",
    "    #<a href=\"/realDonaldTrump/status/936209447747190784\" class=\"tweet-timestamp js-permalink\n",
    "    if match:\n",
    "        tweets.append(match.group(1))\n",
    "        tid = match.group(1)\n",
    "        tweetsdict[tid] = {}\n",
    "        urllib.request.urlretrieve(url+\"/status/\"+match.group(1), 'trumptweets/'+match.group(1)+'.html')\n",
    "        tfile = open('trumptweets/'+match.group(1)+'.html', encoding=\"utf-8\")\n",
    "        for tline in tfile:\n",
    "            tmatch = re.search('<meta  property=\\\"og:description\\\" content=\\\"“(.+)”\\\">', tline)\n",
    "            if tmatch:\n",
    "                tweetsdict[tid]['tweet'] = tmatch.group(1)\n",
    "            rmatch = re.search('\\<strong\\>([\\d,]+)</strong> Retweets', tline)\n",
    "            if rmatch:\n",
    "                retweets = re.sub(',', '', rmatch.group(1))\n",
    "                tweetsdict[tid]['retweets'] = retweets\n",
    "        tfile.close()\n",
    "file.close()\n",
    "tweetwords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-028cad1dc74f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweetsdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#print tweets with specific words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AMERICA GREAT|China'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweetsdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetsdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Retweets: \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtweetsdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'retweets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet'"
     ]
    }
   ],
   "source": [
    "stopwords = ['the', 'to', 'of', 'and', 'a', 'for', 'that', 'is', 'in', 'are', 'with', 'it', 'have', 'our']\n",
    "for x in tweetsdict:\n",
    "    #print tweets with specific words\n",
    "    if re.search('AMERICA GREAT|China', tweetsdict[x]['tweet']):\n",
    "        print(tweetsdict[x]['tweet']+\" Retweets: \"+tweetsdict[x]['retweets'])\n",
    "    \n",
    "    tweet = tweetsdict[x]['tweet'].lower()\n",
    "    tweet = re.sub(r'[^\\w\\s]','',tweet)\n",
    "    words = tweet.split()\n",
    "    #find common words\n",
    "    #print(tweet)\n",
    "    for word in  words:\n",
    "        if word not in stopwords:\n",
    "            tweetwords[word] = tweetwords.get(word,0) + 1\n",
    "            #print(word)\n",
    "        if word == 's:':\n",
    "            print(tweet)\n",
    "print(\"=======================\")\n",
    "for word in sorted(tweetwords, key=tweetwords.get, reverse=True):\n",
    "    print(word+\" \"+str(tweetwords[word]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('boaz_sharabi.html', <http.client.HTTPMessage at 0x213f9246e48>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve from Shironet the page of \"Boaz Sharabi\" songs list\n",
    "import urllib                             \n",
    "urllib.request.urlretrieve('http://shironet.mako.co.il/artist?type=works&lang=1&prfid=183', 'boaz_sharabi.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "רשימת השירים של היוצר בועז שרעבי\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print('רשימת השירים של היוצר בועז שרעבי')    # Header - (can be defined on a variable basis)\n",
    "boaz_sharabi = 183           #The artist code. For future use\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        song_name = match.group(1)\n",
    "        songs[song_name] = song_name\n",
    "    match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "    if match:\n",
    "        song_id = match.group(1)\n",
    "        songs[song_id] = song_id\n",
    "        print(songs[song_name])\n",
    "        print(songs[song_id])\n",
    "file.close   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "רשימת השירים של היוצר בועז שרעבי\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print('רשימת השירים של היוצר בועז שרעבי')    # Header - (can be defined on a variable basis)\n",
    "boaz_sharabi = 183           #The artist code. For future use\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "song = 'Nada'\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        songs[song_name] = match.group(1)\n",
    "    match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "    if match:\n",
    "        songs[song_id] = match.group(1)\n",
    "        print(songs[song_name], songs[song_id])\n",
    "file.close   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "רשימת השירים של היוצר בועז שרעבי\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print('רשימת השירים של היוצר בועז שרעבי')    # Header - (can be defined on a variable basis)\n",
    "boaz_sharabi = 183           #The artist code. For future use\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        song_name = match.group(1)\n",
    "        songs[song_name] = song_name\n",
    "    match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "    if match:\n",
    "        song_id = match.group(1)\n",
    "        songs[song_id] = song_id\n",
    "        print(songs[song_name], songs[song_id])\n",
    "        #       print(songs[song_name])\n",
    "        #      print(songs[song_id])\n",
    "file.close   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "רשימת השירים של היוצר בועז שרעבי\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print('רשימת השירים של היוצר בועז שרעבי')    # Header - (can be defined on a variable basis)\n",
    "boaz_sharabi = 183           #The artist code. For future use\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "acords = 'nada'\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        acords = match.group(1)\n",
    "        if acords in songs:\n",
    "            break\n",
    "        song_name = match.group(1)\n",
    "        songs[song_name] = song_name\n",
    "        match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "        if match:\n",
    "            song_id = match.group(1)\n",
    "            songs[song_id] = song_id\n",
    "            print(songs[song_name], songs[song_id])\n",
    "file.close   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "accord = {} #פה נשמור שירים עם אקורדים\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        song_name = match.group(1)\n",
    "        if song_name in songs:\n",
    "            accord[song_name] = 1\n",
    "        \n",
    "    match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "    if match:\n",
    "        song_id = match.group(1)            \n",
    "        songs[song_name] = song_id\n",
    "                        \n",
    "file.close()\n",
    "\n",
    "for song in songs:\n",
    "    if song in accord:\n",
    "        print ('שיר זה מלווה באקורדים לליווי', song, songs[song] )\n",
    "    else:\n",
    "        print (song, songs[song])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "file = open('boaz_sharabi.html', 'r', encoding = 'utf8')\n",
    "songs = {}\n",
    "accord = {} #פה נשמור שירים עם אקורדים\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    match = re.search('class=\"artist_player_songlist\">(.+)</a>', line)  # Match Song Name\n",
    "    if match:\n",
    "        song_name = match.group(1)\n",
    "        if song_name in songs:\n",
    "            accord[song_name] = 1\n",
    "        \n",
    "    match = re.search('&wrkid=(.+)\" class=', line)     # Match song code. For future use\n",
    "    if match:\n",
    "        song_id = match.group(1)            \n",
    "        songs[song_name] = song_id\n",
    "                        \n",
    "file.close()\n",
    "\n",
    "for song in songs:\n",
    "    if song in accord:\n",
    "        print ('שיר זה מלווה באקורדים לליווי', song, songs[song] )\n",
    "    else:\n",
    "        print (song, songs[song])\n",
    "\n",
    "#  retrieve song: http://shironet.mako.co.il/artist?type=lyrics&lang=1&prfid=183&wrkid=10172\n",
    "# Retrieve from Shironet the page of \"Boaz Sharabi\" songs list\n",
    "singer = '183'\n",
    "link = 'http://shironet.mako.co.il/artist?type=lyrics&lang=1&prfid=183&wrkid='\n",
    "for song in songs:\n",
    "    webpage = link + songs[song]\n",
    "    target = 'מילות השיר ' + song.strip() + '.html'\n",
    "    print(target)\n",
    "    urllib.request.urlretrieve(webpage, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
